[{"content":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下\n UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence\n 尝试1：在不断尝试了很多种网页编码问题后仍然没有解决\n之后让别的小伙伴尝试了同样的代码，别人可以运行但自己无法运行，问题关键在于解决编码和解码问题。通常这种问题是由于编译器编码出现问题\n解决方法： 针对编码问题，主要从两个方面进行出发，其一是网页编码，其二是Pycharm编码，顺利解决这个小bug。\n解决步骤如下图：\n","permalink":"https://hanson00.github.io/posts/thinking/%E7%88%AC%E8%99%AB%E8%A7%81%E9%97%BB%E5%BD%95/","summary":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下 UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 尝试1：在不断尝试了很多种网页","title":"爬虫见闻录"},{"content":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档\npandas官方文档\n以下代码是简单的为单元格赋值操作\n1 2 3 4 5 6 7 8 9 10 11 12 13  import win32com.client from win32com.client import Dispatch xlapp = win32com.client.Dispatch(\u0026#34;Excel.Application\u0026#34;) #创建一个Excel程序 xlapp.DisplayAlerts = False #取消弹窗 xlapp.Visible = True #显示Excel wb = xlapp.Workbooks.Add() #添加工作簿 wb.Worksheets.Item(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A1\u0026#34;).Value = 2000 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A2\u0026#34;).Value = 100 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).cells(1,3).value = 200 #为单元格赋值 wb.SaveAs(r\u0026#34;C:\\Users\\Lenovo\\Desktop\\jb\\123.xlsx\u0026#34;) #保存工作簿 wb.Close() #关闭工作簿 xlapp.quit() #关闭Excel程序 print(\u0026#34;正常结束\u0026#34;)   1 2 3 4 5 6 7 8 9 10 11 12  from win32com.client import Dispatch # 创建Excel程序 xlapp = Dispatch(\u0026#34;Excel.Application\u0026#34;) # 视为可见 xlapp.Visible = True # 警告信息 xlapp.DisplayAlerts = True # 添加工作簿 wb = xlapp.Workbooks.Add() ws = wb.Sheets(\u0026#34;Sheet1\u0026#34;)\t#或者ws = wb.Sheets.Add()后面这个为新添加一个Sheet # ws2 = wb.Sheets.Add() ws.Range(\u0026#34;A1\u0026#34;).Value = 100    Python\u0026ndash;Word python-docx文档\n Python\u0026ndash;PowerPoint python-pptx文档\n Python\u0026ndash;PDF ","permalink":"https://hanson00.github.io/posts/technology/python/python-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC/","summary":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档 pandas官方文档 以下代码是简单的为单元格赋值操作 1 2 3 4 5 6 7 8 9 10 11 12 13 import","title":"Python 自动化办公"},{"content":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求]\n蛋白质 优质蛋白质\n 鱼类 蛋类 精瘦肉 漆皮肌肉  劣质蛋白质\n 牛腩 较肥的肉  淀粉（锻炼开始和结束可以和蛋白质一起吃） 优质淀粉\n 黑麦面包 红薯 全谷物面包 麸皮食品  劣质淀粉\n 土豆 高糖谷类 白米饭 白面包 干果不要吃太多  饮料 优质\n 花草茶 矿物质水 水 茶  健身周期计划 想要塑造肌肉就要摄入大量的热量\n健身-减脂周期：  先进行6个月的锻炼，同时摄入更多的热量 减脂阶段：在锻炼的途中减脂，控制热量的摄入，全程减脂 在稍微增加热量，增加肌肉 在稍微减少热量，减脂  ","permalink":"https://hanson00.github.io/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%86%E4%BA%AB/%E9%94%BB%E7%82%BC%E5%88%86%E4%BA%AB/","summary":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求] 蛋白质 优质蛋白质 鱼类 蛋类 精瘦肉 漆皮肌肉 劣质蛋白质 牛腩 较肥的肉 淀粉","title":"锻炼分享"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\n==csv模块实现了 CSV 格式表单数据的读写，==提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;温铭军\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E6%A8%A1%E5%9D%97/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv模块"},{"content":"Pycharm 常用快捷键  连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ctrl+Alt+T\u0026ndash;添加Try/catch Alt+Shift+上下键\u0026ndash;上下移动 Ctrl+Shift+Enter\u0026ndash;补全代码 Ctrl+R\u0026ndash;替换 Ctrl+Shift+I\u0026ndash;快速查看方法的实现内容 Shift+F1\u0026ndash;查看API文档 Ctrl+Alt+H\u0026ndash;查看那里调用了方法  ","permalink":"https://hanson00.github.io/posts/technology/python/pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/","summary":"Pycharm 常用快捷键 连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ct","title":"Pycharm快捷键"},{"content":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解\n Servlet快速入门  浏览器使用URL访问时所发生的事情  Servlet的方法  根据请求方式的不同进行处理 ServletRequest转换成HttpServletReque才能获取是什么球球方式\n Servlet的运行过程 Servlet程序是由WEB服务器调用，web服务器收到客户端的Servlet访问请求后： ①Web服务器首先检查是否已经装载并创建了该Servlet的实例对象。如果是，则直接执行第④步，否则，执行第②步。 ②装载并创建该Servlet的一个实例对象。 ③调用Servlet实例对象的init()方法。 ④创建一个用于封装HTTP请求消息的HttpServletRequest对象和一个代表HTTP响应消息的HttpServletResponse对象，然后调用Servlet的service()方法并将请求和响应对象作为参数传递进去。 ⑤WEB应用程序被停止或重新启动之前，Servlet引擎将卸载Servlet，并在卸载之前调用Servlet的destroy()方法。\n Request(请求)和Response(响应) request对象里有很多请求数据，可以通过该对象获取请求数据\nresponse返回设置的响应数据\n1.Tomcat需要解析请求数据，封装为request对象，并创建request对象传递到service方法中。\n SqlSessionFactory的优化 1.代码重复每次都要创建一个SqlSessionFactory\n使用静态代码块进行优化\n","permalink":"https://hanson00.github.io/posts/technology/java/04servlet/","summary":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解 Servlet快速入门 浏览器使用UR","title":"04Servlet"},{"content":"MyBatis(持久层/数据访问层)持久层的优秀框架  Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;x.x.x\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   2.编写Mybatis核心配置文件（mybatis-config.xml文件的命名）\u0026ndash;\u0026gt;==替换连接信息，解决硬编码问题==（官网）\n上面的是需要修改的 上面的mappers是指定sql的映射文件的\n\u0026lt;mapper resource=\u0026ldquo;这一段字符串是要替换的sql映射文件(sql语句的文件)的地址\u0026rdquo;\n 3.编写SQL映射文件\u0026ndash;》统一管理sql语句(官网也有)，，命名规则是要操作的表名Mapper.xml\n    编码\n 定义pojo类 加载核心配置文件，获取SQL session factory对象（官网）   获取sqlsession对象，执行sql语句（下面的Mapper代理开发可以替换）   释放资源    Mapper代理开发 1.定义与SQL映射文件同名的Mapper接口，并放在同一目录下2.设置namespace为全限名3.接口的方法名是要与id的名称相同，并且返回值要与编码相同\nMabatis的具体使用(三步走:编写接口方法，编写sql，执行方法)  编写接口方法Mapper接口 生成相应的sql映射文件  2.步骤的图    执行方法\n 数据库的列名和实体类的名称对应不上的时候使用 除了主键外的起别名用\u0026lt;==result== \u0026gt;\n\u0026lt;==result== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n\u0026lt;==id== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n 主键起别名用\u0026lt;==id== \u0026gt;\n\u0026lt;==id== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n 上面图片的resultMap替换了原先的resultType\n void selectById(int id);\n占位符使用 #{}\n不同的接收参数的方式\n多条件查询(使用if实现)，\n单条件查询(类似于switch语句)\n增删改操作后需要提交事务\n添加-主键返回需要使用useGeneratedKeys和keyProperty\n批量删除接受的数组是要使用注解名的，否则传递不进去 separator分隔符\n","permalink":"https://hanson00.github.io/posts/technology/java/03mybatis/","summary":"MyBatis(持久层/数据访问层)持久层的优秀框架 Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包 1","title":"03MyBatis"},{"content":"maven  提供了一套标准化的项目结构\n提供了一套标准化的项目构建流程（编译，测试，打包，发布）\n提供了一套完整的依赖管理机制\n ","permalink":"https://hanson00.github.io/posts/technology/java/03maven/","summary":"maven 提供了一套标准化的项目结构 提供了一套标准化的项目构建流程（编译，测试，打包，发布） 提供了一套完整的依赖管理机制","title":"03maven"},{"content":"JDBC   注册驱动Class.forName()\n  获取连接DriverManager.getConnecti\n1 2 3 4  String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);     定义sql操作语句\n1  String sql=\u0026#34;update stu set math=10 where id=8\u0026#34;;     获取执行sql的对象Statement XXX = conn.createStatement();\n1  Statement stmt=conn.createStatement();     执行sql\n  处理结果\n  释放资源\n DriverManager  DriverManager(驱动管理类)：  注册驱动DriverManager.registerDriver 获取数据库连接    它下面的registerDriver()注册驱动 getCconnection()获取连接,语法jdbc:mysql://ip地址:端口号/数据库名称【jdbc:mysql://127.0.0.1:3306/DB1】\n  1 2 3 4 5  //获取连接 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);    Connection   Connection(数据库连接对象):\n 获取执行sql的对象  普通执行sql对象 Statement createStatement() 预编译SQL的执行sql,防止sql注入 PreparedStatement preareStatement(sql) 执行存储过程的对象 CallbleStatement prepareCall(sql)     int executeUpdate(sql):这个在下面(即参考回滚事务代码)\nResultSet executeQuery(sql):执行DQL语句，返回值是ResultSet结果集对象\n  管理事务  开启事务：setAutoCommit(boolean autoCommit) 提交事务commit() 回滚事务rollback()    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  try { //开启事务  conn.setAutoCommit(false); int count1=stmt.executeUpdate(sql1);//受影响的行数  //处理受影响的结果  System.out.println(count1); int count2=stmt.executeUpdate(sql2);//受影响的行数  //处理受影响的结果  System.out.println(count2); //提交事务  conn.commit(); } catch (Exception throwables) { //回滚事务  conn.rollback(); throwables.printStackTrace(); }      Statement   Statement作用:\n  执行sql语句\n  int executeUpdate(sql):执行DML、DDL语句【DDL操作数据库，DML对数据进行增删改查】 返回值:1.DML语句受影响的行数2.DDL语句执行后，执行成功也可能返回0\n  ResultSet executeQuery(sql)DQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  //定义sql  String sql = \u0026#34;select * from stu\u0026#34;; //获取执行sql对象,获取Statement对象  Statement stmt = conn.createStatement(); //5.执行sql  ResultSet rs = stmt.executeQuery(sql); //6.处理结果，遍历所有数据  while (rs.next()){ //获取数据  int id = rs.getInt(1); String name = rs.getString(2); double money = rs.getDouble(3); //输出  System.out.println(id); System.out.println(name); System.out.println(money); } //7.释放资源  rs.close(); stmt.close(); conn.close(); }         PreparedStatement作用\n 预防sql注入    ","permalink":"https://hanson00.github.io/posts/technology/java/02jdbc/","summary":"JDBC 注册驱动Class.forName() 获取连接DriverManager.getConnecti 1 2 3 4 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password); 定义sql操作语句","title":"02JDBC"},{"content":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )\n show databases; 查询本地数据库\n  create database 数据库名称; 创建数据库\n  一般采用该语句用于创建数据库\ncreate database if not exists 数据库名称; \n  drop database 数据库名称; 删除数据库\n  一般采用该语句用于删除数据库\ndrop database if exists 数据库名称;\n  use 数据库名称; 使用数据库\n  select database(); 查看当前使用的数据库\n  表操作：\n show tables; 查看当前数据库的表名称\n  desc 表名称; 查询表结构\n  1 2 3 4 5 6 7 8 9 10  --创建表createtable表名(列名1,数据类型，列名2,数据类型，列名3,数据类型，列名n,数据类型)；    drop table 表名; 删除表\n  drop table if exists 表名; 删除表并判断是否存在\n  alter table 表名 rename 新表名 修改表名\n  alter table 表名 add 新列名 数据类型; 添加一列\n  alter table 表名 modify 列名 新的数据类型; 修改数据类型\n  alter table 表名 change 列名 新的数据类型; 修改列名和数据类型\n  alter table 表名 drop 列名; 删除列\n  insert into 表名(列名1,列名2,...) values(值1,值2,值3); 给指定的列添加数据\n  insert into 表名 values(值1,值2,值3,...); 给全部列添加数据\n  insert into 表名 values(值1,值2,值3,...),(值1,值2,值3,...),(值1,值2,值3,...); 批量添加数据\n  ==查询语法DQL详细学习见后面自学==\n 条件查询 排序查询 聚合函数 分组查询 分页查询 约束实现 多表查询中的内连接，外连接 多表查询的子查询 事务   ","permalink":"https://hanson00.github.io/posts/technology/java/01mysql/","summary":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )","title":"01mysql"},{"content":"sodjsondonsdad ","permalink":"https://hanson00.github.io/posts/linux/ts/","summary":"sodjsondonsdad","title":"Ts"},{"content":"1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-selenium/","summary":"1 2 3 4 5 6 7 8 from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发","title":"Python Selenium"},{"content":"Python并发与爬虫 进程与线程 每个主进程中至少都会有一个线程，当你创建一个新的线程时，就相当于有两个员工(线程)在工作具体的工作顺序由CPU决定，主线程会继续往下面执行，子线程也会自己执行自己的任务\n在Python中一般不创建多进程，因为进程消耗的内存资源较大\n线程执行案例 t.start()，当前线程准备就绪，等待CPU调度 t.join()等待当前任务执行完成，当前任务执行完成之后才会往后面执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # import threading # # # 线程如果想传参一定要通过arg=(元组的形式进行传递) # def func(name): # for i in range(1000): # print(name, i) # # if __name__ == \u0026#34;__main__\u0026#34;: # t1 = threading.Thread(target=func, args=(\u0026#34;Rose\u0026#34;,) ) # t1.start() # t2 = threading.Thread(target=func, args=(\u0026#34;你太牛\u0026#34;,) ) # t2.start() # for i in range(1000): # print(\u0026#34;main\u0026#34;, i) from threading import Thread class Mythread(Thread): def run(self): #固定写法 for i in range(1000): print(self.name, i) if __name__ == \u0026#34;__main__\u0026#34;: t1 = Mythread() t1.start() t2 = Mythread() t2.start() for i in range(1000): print(\u0026#34;main\u0026#34;, i) #Thread-2 main984  #Thread-2867  #985main  #Thread-2 868 #main986 #Thread-2 869 #main 870 #main987 #Thread-2 988 871 #Thread-2  #main989  #872 #Thread-2 main 873990 #Thread-2 #main 991 #Thread-2 992 #874Thread-2 # main 875993 #Thread-2 main994 #Thread-2 995 876   进程执行案例 1 2 3 4 5 6 7 8 9 10 11  from multiprocessing import Process def func(): for i in range(100000): print(\u0026#34;函数里\u0026#34;, i) if __name__ == \u0026#39;__main__\u0026#39;: t = Process(target=func) t.start() for i in range(100000): print(\u0026#34;main\u0026#34;, i)    线程池：一次性创建一批线程，然后我们用户把任务分配给线程池，然后线程池分配任务给线程池里面的线程\n1 2 3 4 5 6 7 8 9 10 11  from concurrent.futures import ThreadPoolExecutor def func(name): for i in range(100): print(name, i) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(50) as t: for i in range(10): t.submit(func, name=f\u0026#34;线程{i}\u0026#34;) print(\u0026#34;OVER\u0026#34;)    北京新发地爬虫与多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  # coding:utf-8 # @Time : 2022/5/17 14:41 # @Author : 软件1194温铭军 # @file : 北京新发地多线程.py # $software : PyCharm import requests from concurrent.futures import ThreadPoolExecutor import csv url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def getdata(data): url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; response = requests.post(url, headers=header, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;, \u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(20) as t: for i in range(1, 60): data = { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: f\u0026#39;{i}\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } t.submit(getdata, data) print(\u0026#34;OVER\u0026#34;)   协程(多任务异步协程) 协程是在单线程的基础上操作（多线程协作可以充分地利用CPU）\n 切换条件：当程序遇到IO操作时可以选择切换到其他任务(计算类型的任务)上\n在微观上一个任务一个任务进行切换，切换条件是遇到IO操作(单线程的多个任务来回切换运行)\n在宏观上就是多个任务共同运行\n 会使程序陷入阻塞状态的有：IO操作和requests.get()在网络请求返回前都会处于阻塞状态\n 协程分配的任务是由人来调度，线程池是由系统来调度的\n 异步任务和同步任务看是否需要等待某一操作\n在编写协程代码时要注意：await(挂起)要写到async的函数里面，放在协程对象里面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #协程的编写 import time import asyncio async def func1(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱我们\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作,time.sleep()是串形同步操作，使得异步中断了 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱你\u0026#34;) async def func2(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱22\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱22\u0026#34;) async def func3(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱33\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(4) #改成这个异步操作代码就可以实现协程的异步处理 print(\u0026#34;这是第二次我33\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: #input()也会让CPU处于阻塞状态 #requests.get()也会使程序处于阻塞状态 #当程序处于IO操作时程序会陷入阻塞状态 #协程：当程序遇到IO操作时，可以选择性的切换到其他任务 #如果执行函数就会得到一个协程对象 #协程函数的运行需要asyncio模块的支持 t1 = time.time() f1 = func1() f2 = func2() f3 = func3() task = [f1, f2, f3] #一次性启动多个任务（协程） #多任务同时启动就需要asyncio.wait,把多个任务交给asyncio.wait(),而启动就需要asyncio.run() asyncio.run(asyncio.wait(task)) t2 = time.time() print(t2-t1)   上面代码是同步操作的协程\n下面代码是加入异步的协程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #协程的编写 import time import asyncio async def func1(): print(\u0026#34;我爱我们\u0026#34;) await asyncio.sleep(3) print(\u0026#34;这是第二次我爱你\u0026#34;) async def func2(): print(\u0026#34;我爱22\u0026#34;) await asyncio.sleep(3) print(\u0026#34;这是第二次我爱22\u0026#34;) async def func3(): print(\u0026#34;我爱33\u0026#34;) await asyncio.sleep(4) print(\u0026#34;这是第二次我33\u0026#34;) #Python官方推荐的写法 def main(): pass if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() f1 = func1() f2 = func2() f3 = func3() task = [f1, f2, f3] #一次性启动多个任务（协程） #多任务同时启动就需要asyncio.wait,把多个任务交给asyncio.wait(),而启动就需要asyncio.run() asyncio.run(asyncio.wait(task)) t2 = time.time() print(t2-t1)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import asyncio import time async def func1(): print(\u0026#34;我是第一\u0026#34;) await asyncio.sleep(3) print(\u0026#34;我是第一\u0026#34;) async def func2(): print(\u0026#34;我是第2\u0026#34;) await asyncio.sleep(2) print(\u0026#34;我是第2\u0026#34;) async def func3(): print(\u0026#34;我是第3\u0026#34;) await asyncio.sleep(4) print(\u0026#34;我是第3\u0026#34;) async def main(): task = [] task = [asyncio.create_task(func1()), asyncio.create_task(func2()), asyncio.create_task(func3())] await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1) #output:我是第一 # 我是第2 # 我是第3 # 我是第2 # 我是第一 # 我是第3 # 4.009669780731201   使用异步协程爬取图片\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # coding:utf-8 # @Time : 2022/5/18 15:41 # @Author : 软件1194温铭军 # @file : aiohttp_umei.py # $software : PyCharm #aiohttp异步爬取优美图库 # requests.get()该操作是同步操作，如果想要 # 使用异步操作的话，就得使用aiohttp import asyncio import aiohttp import time urls = [ \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/zkc0inje5x0.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/ufmo0xczdbg.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/15dujfems5v.jpg\u0026#34; ] async def downsave(url): name = url.split(\u0026#34;/\u0026#34;)[-1] async with aiohttp.ClientSession() as sessions: # 这部的操作就相当于 \u0026lt;==\u0026gt; requests async with sessions.get(url) as s: with open(f\u0026#34;{name}.jpg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(await s.content.read()) print(\u0026#34;OVER\u0026#34;) async def main(): task = [] for url in urls: task.append(downsave(url)) await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1)    个人对于协程的理解 asyncio.run()是用来运行最高层级的入口点\n","permalink":"https://hanson00.github.io/posts/technology/python/python%E5%B9%B6%E5%8F%91%E4%B8%8E%E7%88%AC%E8%99%AB/","summary":"Python并发与爬虫 进程与线程 每个主进程中至少都会有一个线程，当你创建一个新的线程时，就相当于有两个员工(线程)在工作具体的工作顺序由CP","title":"Python并发与爬虫"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\n==csv模块实现了 CSV 格式表单数据的读写，==提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;君不愁\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E5%AD%A6%E4%B9%A0/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv学习"},{"content":"爬虫路飞进阶 视频网站工作原理   用户上传视频到视频网站\n  视频网站对视频进行转码(4K,2K,1080P,标清等)\n  对视频进行切片处理（个人理解不知道对错，分布式存储视频片段） ==================被切分的视频片段，当用户想拖到后面的片段播放时就直接播放后面被切分的一小段片段，节省流量资源\n  切分后的视频的排序问题，需要一个文件1.记录视频播放顺序2.视频的存放路径基本上会直接存放在M3U文件(固定格式的文本)如json，txt，M3U文件通过utf-8编码后的名字是M3U8\n 怎么爬取？\n 找到m3u8文件（通过各种手段） 通过m3u8下载视频文件 将下载到的视频文件合并成一个视频文件     selenium的视频学习 tip：\n 所打开的浏览器的左上角的浏览器受到控制提示可能后面会遇到反爬 如果遇到ajax的页面(局部刷新)所要操作时需要等待否则大概率报错 当代码只爬取到了部分数据就报错了一般是爬取太快所导致 有些视频网站会有iframe，必须先要拿到iframe(iframe = find_element()找到iframe)然后把视角切换到iframe(web.switch_to.window(iframe窗口))才能拿到数据，切换回原页面使用web.switch_to.default_content()  先粗细定位整体在精细定位局部\n如果你的程序chrome被检测到了是爬虫程序，一般分为两种情况解决chrome版本号小于88和大于88（浏览器控制台输入window.navigator.webdriver查看）  关于selenium的一些常见用法 最好能记住下面的案例所要导入的模块，多看\n1 2 3 4 5 6 7 8 9 10 11 12  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.page_source browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER)   selenium的微信读书小案例 1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from selenium import webdriver import re #使用无界模式访问,并获取网页源代码 sin = input(\u0026#34;请输入你要查找的新闻:\u0026#34;) url = f\u0026#34;https://search.sina.com.cn/news?q={sin}\u0026amp;c=news\u0026amp;from=index\u0026#34; # #无界模式 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) browser.get(url) # data = browser.page_source browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[1]\u0026#39;).send_keys(\u0026#34;阿里巴巴\u0026#34;) browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[4]\u0026#39;).click() retitle = re.compile(r\u0026#39;\u0026lt;div class=\u0026#34;box-result clearfix\u0026#34; data-sudaclick=\u0026#34;.*?\u0026#34;\u0026gt;.*?\u0026lt;a href=\u0026#34;(?P\u0026lt;url\u0026gt;.*?)\u0026#34;.*?\u0026gt;(?P\u0026lt;title\u0026gt;.*?)\u0026lt;/a\u0026gt;\u0026#39;, re.S) result = retitle.finditer(browser.page_source) for i in result: title = i.group(\u0026#34;title\u0026#34;).replace(\u0026#39;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026#39;,\u0026#34;\u0026#34;).replace(\u0026#39;\u0026lt;/font\u0026gt;\u0026#39;, \u0026#34;\u0026#34;) url = i.group(\u0026#34;url\u0026#34;)   拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) #上面//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1] for li in all_list: # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a jobname = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).text salary = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[2]/span\u0026#39;).text print(jobname, salary)   验证码  自己写图像识别 选择互联网上成熟的验证码破解工具  下拉框案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # coding:utf-8 # @Time : 2022/5/21 10:46 # @Author : 软件1194温铭军 # @file : 无头浏览器.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.select import Select #无头浏览器代码 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--headless\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://www.endata.com.cn/BoxOffice/BO/Year/index.html\u0026#39; browser.get(url) sel_el = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;OptionDate\u0026#34;]\u0026#39;) #选择下拉框的提取 sel = Select(sel_el) #将下拉框包装成select对象 #让浏览器调整选项 for i in range(len(sel.options)): sel.select_by_index(i) #拿到每一个下拉框的选项 time.sleep(1) table = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;TableList\u0026#34;]\u0026#39;) print(table.text) time.sleep(1)   窗口切换 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # coding:utf-8 # @Time : 2022/5/20 9:32 # @Author : 软件1194温铭军 # @file : lagou.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   12306 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/21 11:41 # @Author : 软件1194温铭军 # @file : 12306.py # $software : PyCharm #12306账号：a954952920 #密码：hansonwmj2000 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time from selenium.webdriver.common.action_chains import ActionChains #鼠标操作导包,事件链 #防止被认出是爬虫程序，chrome版本88以上适用 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--disable-blink-features=AutomationControlled\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://kyfw.12306.cn/otn/resources/login.html\u0026#39; browser.get(url) time.sleep(1) #输入账号密码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-userName\u0026#34;]\u0026#39;).send_keys(\u0026#39;你的账号\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-password\u0026#34;]\u0026#39;).send_keys(\u0026#39;密码\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-login\u0026#34;]\u0026#39;).click() time.sleep(2) #拖拽目标按钮 btn = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1_n1z\u0026#34;]\u0026#39;) #找到目标拖拽验证码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1__scale_text\u0026#34;]/span\u0026#39;) #拖拽移动 ActionChains(browser).drag_and_drop_by_offset(btn, 300, 0).perform() #横推拽   ","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%E7%88%AC%E8%99%AB/","summary":"爬虫路飞进阶 视频网站工作原理 用户上传视频到视频网站 视频网站对视频进行转码(4K,2K,1080P,标清等) 对视频进行切片处理（个人理解不知道","title":"路飞爬虫进阶爬虫"},{"content":"爬虫基础 爬虫基本流程\n1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到本地，爬取完的URL放到已爬取队列 3.分析网页内容，找出网页里面关心的URL链接和内容，继续执行第二步\n 获取网页 提取信息 保存数据 自动化程序  爬虫如果需要模拟则把该网站下面的所有请求信息封装(例如UA)然后发送\nHTTP请求方法 请求，是由客户端向服务器发出一般分为4部分内容：请求方法（request method）、请求的网址（request url）、请求头（request Headers）、请求体（request Body）\n   1 GET 请求指定的页面信息，并返回实体主体。     2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头   3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。   4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。   5 DELETE 请求服务器删除指定的页面。   6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。   7 OPTIONS 允许客户端查看服务器的性能。   8 TRACE 回显服务器收到的请求，主要用于测试或诊断。   9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。    请求头 请求头，用来说明服务器要使用的附加信息，比较重要的信息有，cookie、Refer、User-Agent\n响应 响应，由服务器端返回给客户端，分为响应状态码（Response Status Code）、响应头（Response headers）、响应体（Response Body）\n响应头 包含了服务器对请求的应答信息\n 小知识  Host:域名。表示请求的服务器网址   爬虫分类 通用爬虫 聚焦爬虫  爬虫学习 urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)打开一个网页并把获取该URL的网页对象 data 必须是一个对象，用于给出要发送到服务器的附加数据，若不需要发送数据则为 None，data支持的对象类型包括字节串、类文件对象和可遍历的类字节串对象\nurllib.request.urlopen()  一个简单的get请求爬取\n 1 2 3 4 5 6 7 8 9 10 11  import urllib.request import urllib.parse #解析器 import urllib.error #Get请求 response = urllib.request.urlopen(\u0026#34;http://www.baidu.com\u0026#34;) print(response.read().decode(\u0026#34;UTF-8\u0026#34;))\t# response是\u0026lt;http.client.HTTPResponse object at 0x000002DC28A404C0\u0026gt; url = \u0026#34;http://httpbin.org/get\u0026#34; response2 = urllib.request.urlopen(url=url) print(response2.read().decode(\u0026#34;UTF-8\u0026#34;))    一个简单的post请求(post请求需要提交个表单信息)，需要提供一个封装的数据\n 1 2 3 4 5  #post请求 需要封装数据 http://httpbin.org/post url = \u0026#34;http://httpbin.org/post\u0026#34; data = bytes(urllib.parse.urlencode({\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;}), encoding = \u0026#34;utf-8\u0026#34;)\t#把数据变成二进制格式 response = urllib.request.urlopen(url=url, data=data) print(response.read().decode(\u0026#34;utf-8\u0026#34;))   urllib.request.Request() urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) 上面的urlopen过于简单直接把赋值的url直接打开，包含不了太多伪装信息，所以使用==urllib.request.Request()==\nheaders：告诉要访问的服务器，我们是什么类型的机器（浏览器），本质上是告诉浏览器我们可以接受什么水平的文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import urllib.request import urllib.parse #解析器 import urllib.error # http://httpbin.org/post https://www.douban.com url = \u0026#34;http://httpbin.org/post\u0026#34; headers = {\u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} data = bytes(urllib.parse.urlencode({\u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;}),encoding = \u0026#34;UTF-8\u0026#34;) #封装了一个请求对象 req = urllib.request.Request(url=url, data=data, headers=headers) response = urllib.request.urlopen(req)\t#响应对象 print(response.read().decode(\u0026#34;UTF-8\u0026#34;))   简单的爬取网页 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  import urllib.request import urllib.error def main(): #基础的URL # 1.爬取网页 baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xlsx\u0026#34; # datalist = getData(baseurl) getData(baseurl) #2.解析数据 #3.保存数据 # saveData(savepath) def getData(baseurl): datalist = [] for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) return datalist # 3.保存数据,在本列子中暂时不用 def saveData(savepath): pass # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main()   BeautifulSoup库的基本使用(爬虫的解析) BeautifulSoup4将HTML文档转换成树形结构，每个节点都是Python对象，归类成四种：\n Tag\t可以获取标签及其标签内容 NavigableString BeautifulSoup comment  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) # Tag 获取标签及内容 print(bs.title) #output: \u0026lt;title\u0026gt;百度一下，你就知道 \u0026lt;/title\u0026gt; print(bs.a) #output: \u0026lt;a class=\u0026#34;mnav\u0026#34; href=\u0026#34;http://news.baidu.com\u0026#34; name=\u0026#34;tj_trnews\u0026#34;\u0026gt;\u0026lt;!--新闻--\u0026gt;\u0026lt;/a\u0026gt; # 获取标签里面的内容 print(bs.title.string) #output: 百度一下，你就知道 print(bs.a.string) #output: 新闻 # NavigableString 获取标签里的所有属性，并返回一个字典 print(bs.a.attrs) #attrs是attribute属性的缩写 # print(bs) 打印整个html文件 # 文件的遍历 contents print(\u0026#34;-----文档的遍历-----\u0026#34;) print(bs.head.contents) #返回一个列表   BeautifulSoup2.0文档搜索 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  import re import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) #文档搜索 # 1. find_all()方法:字符串过滤会查找与字符串完全匹配的内容 list = bs.find_all(\u0026#34;a\u0026#34;) # list = bs.find_all(\u0026#34;a\u0026#34;, limit = 3) limit限制多少 print(list) #正则表达式搜索 print(\u0026#34;---正则表达式搜索---\u0026#34;) re_list = bs.find_all(re.compile(\u0026#34;a\u0026#34;)) print(re_list) print(len(re_list)) #传入一个函数作为参数进行搜索,按照函数进行搜索 print(\u0026#34;---函数搜索---\u0026#34;) def name_is_exists(tag): return tag.has_attr(\u0026#34;name\u0026#34;) funcres = bs.find_all(name_is_exists) print(funcres) #kwargs关键字搜索 print(\u0026#34;----kwargs关键字搜索----\u0026#34;) kwlist = bs.find_all(id = \u0026#34;head\u0026#34;) kwlist2 = bs.find_all(href = \u0026#34;https://www.hao123.com\u0026#34;) kwlist3 = bs.find_all(class_ = True) #这里class加下划线是因为class是Python的关键字 print(kwlist3) #text文本参数,查找标签里的字符串 print(\u0026#34;----text文本参数-----\u0026#34;) ts_list = bs.find_all(text=\u0026#34;hao123\u0026#34;) ts_list2 = bs.find_all(text=[\u0026#34;hao123\u0026#34;, \u0026#34;地图\u0026#34;]) print(ts_list2) #css选择器,和css选择器的语法一样 print(\u0026#34;------css选择器-------\u0026#34;) css_list = bs.select(\u0026#34;title\u0026#34;) css_list2 = bs.select(\u0026#34;.mnav\u0026#34;) css_list3 = bs.select(\u0026#34;#u1\u0026#34;) css_list4 = bs.select(\u0026#34;a[class = \u0026#39;bri\u0026#39;]\u0026#34;) print(css_list3)   liwei爬虫学习的最终 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  # coding:utf-8 # @Time : 2022/5/7 9:57 # @Author : 软件1194温铭军 # @file : ts.py # $software : PyCharm import bs4 import re import xlwt import urllib.request import urllib.error def main(): baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; #待爬取的URL savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xls\u0026#34; #保存位置 datalist = getData(baseurl) #爬取到的数据列表 # getData(baseurl) # print(datalist) #3.保存数据 saveData(datalist,savepath) # 提取影片的提取规则 recom_link = re.compile(r\u0026#39;\u0026lt;a href=\u0026#34;(.*?)\u0026#34;\u0026gt;\u0026#39;) recom_img = re.compile(r\u0026#39;\u0026lt;img alt=\u0026#34;.*? src=\u0026#34;(.*?)\u0026#34;.*?/\u0026gt;\u0026#39;, re.S) #re.S让换行符也包含在内 recom_title = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_rating = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_judge = re.compile(r\u0026#39;\u0026lt;span\u0026gt;(\\d*)人评价\u0026lt;/span\u0026gt;\u0026#39;) recom_inq = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;inq\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_db = re.compile(r\u0026#39;\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;(.*?)\u0026lt;/p\u0026gt;\u0026#39;,re.S) def getData(baseurl): datalist = [] #生成要爬取的网址 for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) #进行数据解析 soup = bs4.BeautifulSoup(html, \u0026#34;html.parser\u0026#34;) for item in soup.find_all(\u0026#34;div\u0026#34;, class_ = \u0026#34;item\u0026#34;):\t#在\u0026lt;class \u0026#39;bs4.BeautifulSoup\u0026#39;\u0026gt;中查找是div并且class=\u0026#34;item\u0026#34;的内容 # print(item) data = [] #保存每一部电影的信息 item = str(item) link = re.findall(recom_link, item)[0] data.append(link) #添加链接 img = re.findall(recom_img, item)[0] data.append(img) title = re.findall(recom_title, item) if (len(title) ==2): ctitle = title[0] data.append(ctitle) otitle = title[1].replace(\u0026#34;/\u0026#34;,\u0026#34;\u0026#34;) #去掉无关符号 data.append(otitle) else: data.append(title[0]) data.append(\u0026#34; \u0026#34;) #留空 rating = re.findall(recom_rating, item)[0] data.append(rating) judge = re.findall(recom_judge, item)[0] data.append(judge) inq = re.findall(recom_inq, item) if len(inq) !=0: inq = inq[0].replace(\u0026#34;。\u0026#34;, \u0026#34; \u0026#34;) data.append(inq) else: data.append(\u0026#34; \u0026#34;) db = re.findall(recom_db, item)[0] db = re.sub(\u0026#39;\u0026lt;br(\\s+)?/\u0026gt;(\\s+)?\u0026#39;, \u0026#34;\u0026#34;, db) db = re.sub(\u0026#39;/\u0026#39;, \u0026#34; \u0026#34;, db) data.append(db.strip()) datalist.append(data) # print(datalist) return datalist # 3.保存数据 def saveData(datalist,savepath): wb = xlwt.Workbook(encoding=\u0026#34;UTF-8\u0026#34;) ws = wb.add_sheet(\u0026#34;douban_dataTOP250\u0026#34;) col = (\u0026#34;电影详情链接\u0026#34;, \u0026#34;图片链接\u0026#34;, \u0026#34;影片中文名\u0026#34;, \u0026#34;影片外国名\u0026#34;, \u0026#34;评分\u0026#34;, \u0026#34;评价数\u0026#34;, \u0026#34;概况\u0026#34;, \u0026#34;相关信息\u0026#34;) for i in range(0,8): ws.write(0,i,col[i]) #列名 for i in range(0,250): print(\u0026#34;第%d条\u0026#34; %(i+1)) data = datalist[i] for j in range(0,8): ws.write(i+1,j,data[j]) #写入数据 wb.save(savepath) #保存 # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) # print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main() print(\u0026#34;爬取完毕\u0026#34;)   爬虫所用到的库 bs4(beautifulsoup)\nurllib\nre正则表达式  re.L 表示特殊字符集 \\w, \\W, \\b, \\B, \\s, \\S 依赖于当前环境 re.M 多行模式 re.S 即为' . \u0026lsquo;并且包括换行符在内的任意字符（\u0026rsquo; . \u0026lsquo;不包括换行符） re.U 表示特殊字符集 \\w, \\W, \\b, \\B, \\d, \\D, \\s, \\S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和\u0026rsquo; # \u0026lsquo;后面的注释   ==注意==:\n 除了txt纯文本文件使用r来读取，其他类型的文件都是二进制文件用rb读取 乱码问题：Python内存层面使用的是Unicode编码，而Unicode不能存储和传输的，必须把Unicode编码进行转码(写文件时要编码，读文件时要解码)，转换成UTF-8或者GBK 得到的b\u0026rsquo;字符串\u0026rsquo;是==字节==，需要转码 在已经爬取得到网页后进行数据解析时，如果测试正则表达式没有问题但仍无数据时，可以加上re.S之类的re.compile(pattern[, flags])的flag参数 当使用BeautifulSoup的find(参数)参数中出现Python关键字时可以在关键字后面加__(PS: class_ = \u0026quot;值\u0026quot;)  1 2 3  #如下面两种写法一样 find(classs_=\u0026#34;table\u0026#34;) find(attrs={\u0026#34;class\u0026#34;:\u0026#34;table\u0026#34;})   BeautifulSoup中如果想拿到某个属性的值，可以使用get(\u0026ldquo;要获取的属性名\u0026rdquo;)方法  ","permalink":"https://hanson00.github.io/posts/technology/python/%E6%9D%8E%E5%B7%8D%E7%88%AC%E8%99%AB/","summary":"爬虫基础 爬虫基本流程 1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到","title":"李巍爬虫"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Person struct { age int name string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板文件 \troute.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) //配置路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到首页\u0026#34;) }) route.POST(\u0026#34;/post\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到post页面\u0026#34;) }) route.GET(\u0026#34;json1\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, map[string]interface{}{ \u0026#34;json1\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第一个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json2\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;json2\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第2个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json3\u0026#34;, func(c *gin.Context) { tom := \u0026amp;Person{ 18, \u0026#34;tom\u0026#34;, } c.JSON(http.StatusOK, tom) }) route.GET(\u0026#34;xml\u0026#34;, func(c *gin.Context) { c.XML(http.StatusOK, map[string]interface{}{ \u0026#34;xml\u0026#34;: \u0026#34;6666\u0026#34;, }) }) route.GET(\u0026#34;index\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;index.html\u0026#34;, map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;后台的标题\u0026#34;, }) }) route.Run(\u0026#34;:8821\u0026#34;) }   模板渲染 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Aticle struct { Title string Content string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板路径 \t//route.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) \t//如果有多层模板目录/**代表一层目录 \troute.LoadHTMLGlob(\u0026#34;templates/**/*\u0026#34;) //前台路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { //这里的html名称要和渲染的htmldefine后面的名称一致 \tc.HTML(http.StatusOK, \u0026#34;templates/default/beindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/new\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/default/benew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) //后台路由 \troute.GET(\u0026#34;/admin\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;templates/admin/adindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/adminnew\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/admin/adnew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) route.Run(\u0026#34;:8848\u0026#34;) }   ","permalink":"https://hanson00.github.io/posts/technology/golang/01gin/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34;","title":"01gin"},{"content":"Git Learning HEAD是指当前所在分支\ngit branch 显示所在分支\ngit check cat 切换到cat分支\n  git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表示文件已经被安置到暂存区，稍后会与其他文件一起放到存储库) git add \u0026ndash;all全部文件添加到暂存区 git commit -m**\u0026ldquo;init commit\u0026rdquo;** commit只会把暂存区里面的文件提交到存储库，加粗字体段为提交文件的说明描述一定要添加否则会打开vim编辑器进行再次补充 额外点：二段式：git commit -a -m \u0026ldquo;描述补充信息\u0026rdquo;（一次直接add 和commit） git log 查看日志信息   使用git查询历史记录时的常见问题  git log git log welcome.html 访问特定文件的日志信息 git log \u0026ndash;oneline 显示一行日志 git reflog 可以移动指针次数有所记录方便回调版本 想要查找某个人或者某些人的commit \u0026ndash;git log \u0026ndash;oneline \u0026ndash;author=\u0026ldquo;作者名称\u0026rdquo; 使用\u0026quot;\\|\u0026ldquo;表示或者 git log \u0026ndash;oneline \u0026ndash;grep=\u0026ldquo;关键字信息\u0026rdquo; 在关键字的内容中进行查询 git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; 找到早上九点到十二点的所有commit git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; \u0026ndash;after=\u0026ldquo;2022-01\u0026rdquo; 从2022-01之后的每天的早上九点到十二点的所有commit   在git中删除或者修改文件  直接删除，使用系统直接删除或者使用rm  删除后还要使用git add \u0026ldquo;要删除的文件名称\u0026rdquo; git add 1.txt 移入暂存区 add后还要进行commit操作 git commit -m \u0026ldquo;描述信息\u0026rdquo; 提交到存储库   使用git进行删除 git rm 2.txt 直接完成add操作后面只需使用commit操作即可完成   如果不想git在对文件进行控制可以使用git rm \u0026ldquo;2.txt文件名\u0026rdquo; \u0026ndash;cached 变更文件名  使用系统直接改变名称后 使用git add \u0026ndash;all git commit -m \u0026ldquo;描述信息\u0026rdquo;   或者直接使用git进行改名 git mv filename1 filename2 修改commit描述记录 git commit \u0026ndash;amend -m \u0026ldquo;描述信息\u0026rdquo; 使用amend只能修改最后一条记录 git blame 找出哪段代码谁是修改的 git checkout welcome.html 挽救误删的文件   版本前进后退  git reflog git reset \u0026ndash;hard hash值 前进后退到某一个文档  git diff 文件名 将工作区文件和暂存区文件作比较\ngit diff HEAD 文件名 与本地库做对比\ngit diff 全部文件做比较\n 分支  git branch 获取当前分支 git branch cat 创建cat分支 git branch -d cat 删除cat分支 -D强制删除 git checkout cat 切换分支 git branch -v 查看有什么分支  分支合并 1.先切换到要合并的分支 git merge     git与GitHub   如果是全新的项目创建的话git init\n  git add 1.txt\n  git commit -m \u0026ldquo;git one file\u0026rdquo;\n  接下来要把内容推送到远端的git服务器上\n git remote add origin GitHub服务器地址 origin只是默认的代名词，代指后面的那个服务器地址，可以改名 （起别名，以后可以用这个别名代替远程库地址） git push -u origin master push指令： 例如 远端节点为wmj，选择cat分支推送上去 git push wmj cat  把master分支的内容推送到origin位置 如果远端服务器不存在master分支，会自动创建 如果服务器上存在master分支，则会把master分支的位置指向最新的状态 -u其实就是upstream（上游），其实就是另一个分支的名称而已，在git里，每个分支都可以设置一个上游，它会自动跟踪某个分支 git push -u origin master 会把origin/master设置为本地master的分支的upstream，当下次执行git push时不添加任何指令，git会猜测推送给远端节点origin，并把master分支推上去 git push origin master=git push origin master:master 如果想把分支推上去后进行改名则git push origin master:cat      把推送的内容拉回来更新  git fetch git merge origin/master    推送命令 git pull =git fetch+git merge\n  git pull -rebase 可以使得产生的额外commit取消\n  有时候push推送失败是因为在线版本的内容比本地内容新此时需要 先拉再推 git pull \u0026ndash;rebase\n  看到有趣的项目时单击clone or download 或者使用 git clone git@github.com:\u0026hellip;. 复制下来地址后面还可以加目录名\n与其他开发者互动pull request（待续） ","permalink":"https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/git%E5%AD%A6%E4%B9%A0/","summary":"Git Learning HEAD是指当前所在分支 git branch 显示所在分支 git check cat 切换到cat分支 git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表","title":"Git学习"},{"content":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看UA(User-Agent)\npost请求注意发送数据\n网页的渲染方式： 使用浏览器开发者工具和查看网页源代码可以看出为什么渲染方式\n 服务器渲染：在服务器那边就直接把数据和html结合在一起，统一返回给客户端(浏览器) 客户端渲染：客户端浏览器第一次请求时只要一个html骨架，待浏览器检查html时再第二次向服务器请求数据，并进行渲染【在网页源代码中看不到数据】|如果遇到客户端渲染的网页则要在抓包工具中(浏览器开发者工具)捕获需要的数据源在对该数据源的URL进行请求抓取  关于网页编码与解码的一些总结   在一般网页有给出自己给出编码字符集时可以观察编码后自行赋值或者自己编写正则表达式自己提取\n电影天堂项目体现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests #1.从电影天堂的首页爬取相关首页的内容 # 2.从相关内容中提取相关下载信息 url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } # response = requests.get(url, headers=header, verify=False) response = requests.get(url, headers=header) #本次实验不需verify也可以获得信息 # 获取网页的编码方式,自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) # print(code) #下面两行代码即可解决编码问题 # code = response.encoding # print(response.text.encode(code).decode(\u0026#34;gb2312\u0026#34;)) #或者在爬取乱码网页后进行观察后自行赋值（直接自己赋值） response.encoding = code html = response.text # print(html) #提取首页中所需内容 redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(html) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) #在提取的首页内容中进一步提取子页所需内容 child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_response = requests.get(child_url, headers=header) child_response.encoding = code # print(child_response.text) child_html = child_response.text child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_html) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) response.close() print(\u0026#34;爬取结束\u0026#34;)     案例2\n1 2 3 4 5 6 7 8 9  import requests heads = {\u0026#34;User_Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} url1 = \u0026#34;https://www.baidu.com\u0026#34; request_data = requests.get(url=url1, headers=heads) code = request_data.encoding print(code) response = request_data.text.encode(code).decode(\u0026#34;utf-8\u0026#34;) print(response) #response = request_data.text 有时可以直接无需转码或者有时候可以使用decode(\u0026#34;gbk\u0026#34;)   requests get方式请求：\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#34;https://www.baidu.com/s?ie=UTF-8\u0026amp;wd=%E5%91%A8%E6%9D%B0%E4%BC%A6\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(url, headers=headers) print(response.text)   post方式请求： post请求时需要查看网页的From data所需要的参数然后在填充后传入\n1 2 3 4 5 6 7 8 9 10 11  # post请求：百度翻译 import requests url = \u0026#34;https://fanyi.baidu.com/sug\u0026#34; ins = input(\u0026#34;请输入你要翻译的词\u0026#34;) data = { \u0026#34;kw\u0026#34;: ins } response = requests.post(url, data=data) print(response.json())   数据解析 三种解析方式：\n re解析(最快) bs4解析(便捷)在李巍爬虫学习中 xpath解析  re正则表达式 1 2 3 4 5 6 7 8 9 10 11 12  import re str = \u0026#39;\u0026#39;\u0026#39; \u0026lt;div class=\u0026#34;jay\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;1\u0026#34;\u0026gt;周杰伦\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay2\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;2\u0026#34;\u0026gt;周杰伦2\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay3\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;3\u0026#34;\u0026gt;周杰伦3\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026#39;\u0026#39;\u0026#39; # 使用 (?P\u0026lt;分组名\u0026gt;正则) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;(?P\u0026lt;id\u0026gt;.*?)\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;.*\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;) result = res.finditer(str) for i in result: print(i.group(\u0026#39;id\u0026#39;))   xPath解析   首先导入from xlml import etree etree才包括了xPath解析的功能\nxPath解析XML\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  from lxml import etree xml = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; tree = etree.XML(xml) #生成一个xPath解析的XML对象 # result = tree.xpath(\u0026#34;/book\u0026#34;) #output:[\u0026lt;Element book at 0x183918b2f00\u0026gt;] result = tree.xpath(\u0026#34;/book/name\u0026#34;) #output:[\u0026lt;Element name at 0x21065d142c0\u0026gt;] result = tree.xpath(\u0026#34;/book/name/text()\u0026#34;) #output:[\u0026#39;野花遍地香\u0026#39;] # 把后代(子孙节点的内容拿出来) result = tree.xpath(\u0026#34;/book/author//nick/text()\u0026#34;) #output:[\u0026#39;周大枪\u0026#39;, \u0026#39;周芷若\u0026#39;, \u0026#39;周杰伦\u0026#39;, \u0026#39;蔡依林\u0026#39;, \u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] #*通配符 result = tree.xpath(\u0026#34;/book/author/*/nick/text()\u0026#34;) #output:[\u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] print(result)     xPath解析html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  from lxml import etree tree = etree.parse(\u0026#34;ts.html\u0026#34;)\t# result = tree.xpath(\u0026#34;/html\u0026#34;) #output:[\u0026lt;Element html at 0x1e6e3568880\u0026gt;] result = tree.xpath(\u0026#34;/html/body/ul/li/a\u0026#34;) #output:[\u0026lt;Element a at 0x1f354c57300\u0026gt;, \u0026lt;Element a at 0x1f354c57400\u0026gt;, \u0026lt;Element a at 0x1f354c573c0\u0026gt;] #xPath的索引是从第一个开始 result = tree.xpath(\u0026#34;/html/body/ul/li[1]/a/text()\u0026#34;) #output:[\u0026#39;百度\u0026#39;] #两种提取大炮的方法 result = tree.xpath(\u0026#34;/html/body/ol/li[2]/a/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] #@表示a里面的属性href是dapao的 #[@XXX=XXX] : 属性的筛选 result = tree.xpath(\u0026#34;/html/body/ol/li/a[@href=\u0026#39;dapao\u0026#39;]/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] result = tree.xpath(\u0026#34;/html/body/ol/li\u0026#34;) #output: [\u0026#39;飞机\u0026#39;] #[\u0026#39;大炮\u0026#39;] #[\u0026#39;火车\u0026#39;] for i in result: a = i.xpath(\u0026#34;./a/text()\u0026#34;) print(a) #拿到a里面的属性的值 result = tree.xpath(\u0026#34;/html/body/ol/li/a/@href\u0026#34;) print(result) \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34;\u0026gt;百度\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.sogou.com\u0026#34;\u0026gt;搜狗\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;feiji\u0026#34;\u0026gt;飞机\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;dapao\u0026#34;\u0026gt;大炮\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;huoche\u0026#34;\u0026gt;火车\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;div class=\u0026#34;job\u0026#34;\u0026gt;李嘉诚\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;common\u0026#34;\u0026gt;胡辣汤\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34;   xPath解析猪八戒网 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests from lxml import etree import csv baseurl = \u0026#34;https://zhanjiang.zbj.com/search/f/?kw=saas\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(baseurl, headers=headers) # print(response.text) html = etree.HTML(response.text) #相对解析 #拿到整个大的div的列表 divs = html.xpath(\u0026#34;/html/body/div[6]/div/div/div[2]/div[5]/div[1]/div\u0026#34;) namelist = [] datalist = [] #在整个大的div里面选取要提取的数据 for i in divs: price = i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[1]/span[1]/text()\u0026#34;)[0].strip(\u0026#34;¥\u0026#34;) title = \u0026#34;saas\u0026#34;.join(i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[2]/p/text()\u0026#34;)) com_name = i.xpath(\u0026#34;./div/div/a[1]/div[1]/p/text()\u0026#34;)[1].strip() place = i.xpath(\u0026#34;./div/div/a[1]/div[1]/div/span/text()\u0026#34;)[0] datalist = [title, price, com_name, place] # print(com_name) with open(\u0026#34;猪八戒.csv\u0026#34;, \u0026#34;a+\u0026#34;) as csvfile: zwriter = csv.writer(csvfile, ) zwriter.writerow(datalist) response.close() print(\u0026#34;爬取结束\u0026#34;)   防盗链-梨视频 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  import requests #原始页面的链接，有需要替换的信息 url = \u0026#34;https://www.pearvideo.com/video_1761797\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34;, #防盗链：溯源1-》2-》3一样 \u0026#39;Referer\u0026#39;: \u0026#39;https://www.pearvideo.com/video_1761797\u0026#39; } #获取后面要用于替换的信息 conid = url.split(\u0026#34;_\u0026#34;)[1] #从抓包工具中获取的url里面获取json，目的是要获得假的URL visit_url = f\u0026#34;https://www.pearvideo.com/videoStatus.jsp?contId={conid}\u0026amp;mrd=0.561999819571843\u0026#34; response = requests.get(url=visit_url, headers=header) # print(response.json()) #在抓包工具中的链接里面获得假视频URL和假视频URL的信息用于替换 #\u0026#39;https://video.pearvideo.com/mp4/adshort/20220511/1652607901500-15877599_adpkg-ad_hd.mp4\u0026#39; flase_url = response.json()[\u0026#34;videoInfo\u0026#34;][\u0026#34;videos\u0026#34;][\u0026#34;srcUrl\u0026#34;] systemtime = response.json()[\u0026#39;systemTime\u0026#39;] # print(systemtime) # print(flase_url) realurl = flase_url.replace(systemtime, f\u0026#34;cont-{conid}\u0026#34;) print(realurl) #下载视频 with open(\u0026#34;梨视频.mp4\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(realurl).content) response.close() print(\u0026#34;爬取结束\u0026#34;)   豆瓣小抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests import re import csv url = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } req = requests.get(url, headers=header) response = req.text # print(response) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;hd\u0026#34;\u0026gt;.*?\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(?P\u0026lt;name\u0026gt;.*?)\u0026#39; R\u0026#39;\u0026lt;/span\u0026gt;.*?\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;.*?\u0026lt;br\u0026gt;(?P\u0026lt;year\u0026gt;.*?)\u0026amp;nbsp.*?\u0026#39; R\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(?P\u0026lt;scroe\u0026gt;.*?)\u0026lt;/span\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;span\u0026gt;(?P\u0026lt;people\u0026gt;\\d+)人评价\u0026lt;/span\u0026gt;\u0026#39;, re.S) result = res.finditer(response) with open(\u0026#34;lufeidouban.csv\u0026#34;, \u0026#34;a\u0026#34;) as f: file_writer = csv.writer(f) for i in result: # print(i.group(\u0026#34;name\u0026#34;)) # print(i.group(\u0026#34;year\u0026#34;).strip()) # print(i.group(\u0026#34;scroe\u0026#34;)) # print(i.group(\u0026#34;people\u0026#34;)) #将数据保存成csv文件 dic = i.groupdict() dic[\u0026#34;year\u0026#34;] = dic[\u0026#34;year\u0026#34;].strip() file_writer.writerow(dic.values()) req.close() print(\u0026#34;爬取结束\u0026#34;)    北京新发地抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/13 15:42 # @Author : 软件1194温铭军 # @file : beijing_xinfadi.py # $software : PyCharm import requests import csv #使用post请求时所要添加data请求参数,可以在浏览器开发者工具里面的From Data里面看到 url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } data= { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } response = requests.post(url, headers=headers, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;w\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;,\u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) response.close() print(\u0026#34;爬取结束\u0026#34;)   电影天堂重复代码处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # coding:utf-8 # @Time : 2022/5/13 9:09 # @Author : 软件1194温铭军 # @file : dytt.py # $software : PyCharm # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def htmldata(url): \u0026#34;\u0026#34;\u0026#34; 代替提取网页信息 :param url: :return: \u0026#34;\u0026#34;\u0026#34; response = requests.get(url, headers=header) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) response.encoding = code html = response.text response.close() return html # 或者自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(htmldata(url)) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_data = htmldata(child_url) child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_data) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) print(\u0026#34;爬取结束\u0026#34;)   代理 代理：通过第三方的一个机器去发送请求（去网上找免费代理，或者找）\n综合训练-网易云音乐评论抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132  # coding:utf-8 # @Time : 2022/5/16 10:07 # @file : 网易云音乐.py # $software : PyCharm #爬取网易云音乐评论 # 步骤： # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密 # 3.请求到网易，拿到评论信息 # 加密参数 # params:就是encText # encSecKey:就是encSecKey from Crypto.Cipher import AES import requests import re import json from base64 import b64encode url = \u0026#34;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\u0026#34; #POST请求 headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } #找到了真正的参数，接下来还需要解密过程 data = { \u0026#39;csrf_token\u0026#39;: \u0026#34;\u0026#34;, \u0026#39;cursor\u0026#39;: \u0026#34;-1\u0026#34;, \u0026#39;offset\u0026#39;: \u0026#34;0\u0026#34;, \u0026#39;orderType\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageNo\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageSize\u0026#39;: \u0026#34;20\u0026#34;, \u0026#39;rid\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34;, \u0026#39;threadId\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34; } #处理加密过程 #服务于windows.arXXX的加密过程 e = \u0026#39;010001\u0026#39; f = \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; g = \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; i = \u0026#39;LvjKmGgecBJv6kkF\u0026#39; #手动固定 #如果把i固定那么下面这个函数一定是固定的，由下面的c函数解出 def get_encSecKey(): return \u0026#34;7c435414bc686e49dc26a57ffedbb80b320fd755755c5da8e72f416a61370039aa2d3fa333e54a6e7c9abe7f26faffa69d1721db76cb2e6f17d0393d4cfbc6176590909d027022c4e458aee2123c329b60e1e422beef0f8e39efad5014cbeab022199bc7e6c47a5d0bca5528f6c7946305ae019674309d562c69b39dde9ea429\u0026#34; #字典不能加密，所以下面这个函数默认收到的是字符串 def get_params(data): \u0026#34;\u0026#34;\u0026#34; 就是去还原下面的b的两次加密 h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i) :param data: :return: \u0026#34;\u0026#34;\u0026#34; first = enc_params(data, g) second = enc_params(first, i) return second #返回的就是params #转换成16的长度为加密算法服务 def to_16(data): pad = 16 - len(data) % 16 data += chr(pad) *pad return data #第二个参数的加密算法 def enc_params(data, key): iv = \u0026#39;0102030405060708\u0026#39; data = to_16(data) aes = AES.new(key=key.encode(\u0026#34;utf-8\u0026#34;), IV=iv, mode=AES.MODE_CBC) #创建了一个加密工具 bs = aes.encrypt(data.encode(\u0026#34;utf-8\u0026#34;)) #如果想要返回字符串还需要base64转码 return str(b64encode(bs), \u0026#34;utf-8\u0026#34;) pass \u0026#34;\u0026#34;\u0026#34; function a(a) { var d, e, b = \u0026#34;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\u0026#34;, c = \u0026#34;\u0026#34;; for (d = 0; a \u0026gt; d; d += 1) #循环16次 e = Math.random() * b.length, #产生随机数 e = Math.floor(e), #取整 c += b.charAt(e); return c #返回一个16个字母 } function b(a, b) { a是要加密的内容 var c = CryptoJS.enc.Utf8.parse(b) c和b是一回事 , d = CryptoJS.enc.Utf8.parse(\u0026#34;0102030405060708\u0026#34;) 偏移量 , e = CryptoJS.enc.Utf8.parse(a) e和a相同都是数据 , f = CryptoJS.AES.encrypt(e, c, { e是数据，c是 AES是加密算法自己查的话可以知道下面的东西 iv: d, 偏移量 mode: CryptoJS.mode.CBC 加密模式CBC 看完之后发现少了个密钥所以c(b)是密钥 }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,\u0026#34;\u0026#34;,c), #RSA加密 e = encryptedString(d, a) } function d(d, e, f, g) { d：就是真实数据data,除了数据d是变的之外，其他都是不变的 var h = {}产生一个空对象 e：固定值 \u0026#39;010001\u0026#39; , i = a(16); i是一个16位数的随机字符串 f:超长定值 \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i), i是密钥 #返回的就是加密数据param #如果i固定那么encSeckey就固定 h.encSecKey = c(i, e, f), i是一个16位数的随机字符串，ef是固定的值 #返回的就是加密参数encSecKey return h }windows.a = d加密参数的入口是d encText是通过两次加密得出第二个加密参数 \u0026#34;\u0026#34;\u0026#34; response = requests.post(url, headers=headers, data={ \u0026#34;params\u0026#34;:get_params(json.dumps(data)), \u0026#34;encSecKey\u0026#34;:get_encSecKey() }) print(response.text) # list = response.json()[\u0026#39;data\u0026#39;][\u0026#39;hotComments\u0026#39;] # res = re.compile(r\u0026#34;\u0026#39;content\u0026#39;: \u0026#39;(.*?)\u0026#39;,\u0026#34;) # # print(list[0]) # print(res.findall(str(list))) response.close() print(\u0026#34;OVER!\u0026#34;)   [后续练习] 熟练使用浏览器开发者工具\n [附录] 使用.*?代替变化的内容，使用(.*?)代表需要提取的内容\n记得关闭请求\numeitu.com 暂且不行\n","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB/","summary":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看UA(User-Agent) post请求注意发送数据 网页的渲染方式： 使用浏览器开发者工具和查看","title":"路飞爬虫"},{"content":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪Love and share 🧐本人使用过的语言 💪 正在学习:\n 🧠 计划学习:\n ","permalink":"https://hanson00.github.io/about/","summary":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪Love and share 🧐本人使用过的语言 💪 正在学习:\n 🧠 计划学习:\n ","title":""}]
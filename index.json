[{"content":"为什么要使用并发编程 **例如：**使用一个网络爬虫，在没有使用并发编程的时候，花费了1小时，采用并发下载可以大幅度减少下载时间\nCPU密集型计算（CPU-bound）：计算密集型，CPU需要进行大量的计算，CPU占用特别高。解压和压缩，加密解密，正则表达式搜索\nIO密集型：大部分的等待时间是等待（硬盘和内存的读写操作）文件处理程序，网络爬虫，读写数据库程序。（read，write，recv，send，etc）\n  多进程\n 优点：可以利用多核CPU并行运算 缺点：  启动数目比线程少 占用资源多    适用于CPU密集型计算\n   多线程\n 优点：相对于进程占用资源少，更轻量 缺点：  相对于进程，多线程只支持并发执行，不能利用多CPU（GIL锁） 相对于协程，启动数目受限制，占用内存资源，有线程切换开销    适用于IO密集型计算，同时对运行的任务数量要求不多\n   协程\n 优点，内存开销较少，可以启动协程的数目最多 缺点：  支持的库受限，代码复杂    适用于IO密集型计算，需要超多的任务执行\n  使用多线程爬虫与单线程爬虫对比 1 2 3 4 5 6 7 8 9 10 11 12  # @file : ts.py import requests urls = [f\u0026#34;https://q.cnblogs.com/list/unsolved?page={page}\u0026#34; for page in range(1, 51)] # print(urls) def craw(url): response = requests.get(url=url) print(url, \u0026#34; :\u0026#34;, len(response.text)) if __name__ == \u0026#39;__main__\u0026#39;: craw(urls[0])   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # @file : ts2.py import ts import time from threading import Thread def single_thread(): print(\u0026#34;single_thread being...\u0026#34;) for url in ts.urls: ts.craw(url) print(\u0026#34;single_thread ending...\u0026#34;) def multi_thread(): print(\u0026#34;single_thread being...\u0026#34;) thread_list = [] for url in ts.urls: thread_list.append(Thread(target=ts.craw, args=(url,))) for thread in thread_list: thread.start() for thread in thread_list: # 等待线程结束 thread.join() print(\u0026#34;single_thread ending...\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: start_time = time.time() single_thread() end_time = time.time() print(f\u0026#34;single_thread：{end_time - start_time}\u0026#34;) start_time = time.time() multi_thread() end_time = time.time() print(f\u0026#34;multi_thread：{end_time - start_time}\u0026#34;)   上面两段代码的运行结果如下：大约相差二十倍的时差\n多线程 就算创建了多个线程但是在同一时刻只能有一个线程运行（GIL），切换并发运行\n多线程数据通信的queue.Queue queue.Queue可以用于多线程之间的线程安全的数据通信\n多线程并发的访问数据时不会出现数据冲突安全\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # @file : ts.py import requests from lxml import etree from urllib.parse import urljoin old_url = \u0026#34;https://q.cnblogs.com/list/unsolved?page=1\u0026#34; urls = [f\u0026#34;https://www.cnblogs.com/#p{page}\u0026#34; for page in range(1, 50)] # print(urls) def craw(url): response = requests.get(url=url) return response.text def parse(data): html = etree.HTML(data) li_s = html.xpath(\u0026#34;//div[@class=\u0026#39;post-item-text\u0026#39;]\u0026#34;) res_li = [] for li in li_s: title = li.xpath(\u0026#34;./a/text()\u0026#34;)[0] href = li.xpath(\u0026#34;./a/@href\u0026#34;)[0] title_href = urljoin(old_url, href) # print(title_href, title) res_li.append([title_href, title]) return res_li if __name__ == \u0026#39;__main__\u0026#39;: a = parse(craw(urls[0])) print(a)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  #!/usr/bin/env python # coding:utf-8 # @Time : 2022/11/8 8:34 # @Author : 糕阳 # @file : ts4.py # $software : PyCharm import queue import random import threading import time import ts # 生产者，负责生产出数据供消费者消费 def do_craw(url_queue, res_html_queue): while url_queue: url = url_queue.get() html = ts.craw(url) print(threading.current_thread().name, f\u0026#34;craw {url}\u0026#34;) res_html_queue.put(html) time.sleep(random.randint(1,2)) url_queue.task_done() # 消费者，负责消费生产者生产的html页面（res_html_queue）做解析 def do_parse(res_html_queue, file): while True: html = res_html_queue.get() result_data = ts.parse(html) for i in result_data: file.write(str(i) + \u0026#34;\\n\u0026#34;) print(\u0026#34;parse... \u0026#34; ,threading.current_thread().name) time.sleep(random.randint(2, 3)) res_html_queue.task_done() if __name__ == \u0026#39;__main__\u0026#39;: url_queue = queue.Queue() res_html_queue = queue.Queue() file = open(\u0026#34;./blog_yuan.txt\u0026#34;, \u0026#34;a\u0026#34;, encoding=\u0026#34;utf8\u0026#34;) for url in ts.urls: url_queue.put(url) for number in range(3): t = threading.Thread(target=do_craw, args=(url_queue, res_html_queue), name=f\u0026#34;craw{number}\u0026#34;) t.setDaemon(True) t.start() for number in range(2): t = threading.Thread(target=do_parse, args=(res_html_queue, file), name=f\u0026#34;parse{number}\u0026#34;) t.setDaemon(True) t.start() url_queue.join() res_html_queue.join() file.close()   关于上述代码：\nq.join()计数不为0时，会让主线程阻塞等待，队列计数为0后才会继续往后执行\nput的时候计数+1，get不会-1，get需要和task_done 一起使用才会-1\nLock用于解决线程安全 **线程安全：**某个函数在被多线程执行时，能够正确地处理多个线程之间共享变量，使程序能正确的完成\n使用Lock锁住该要锁的\n线程池 线程会有资源竞争的问题\n线程池＋队列，好像无法守护主线程，需要使用手动创建线程，然后守护线程\n线程池的map和submit的区别：\n map的第二个参数是转递一个可迭代对象，也就是一次性就把要提交的任务的参数提交完 submit是一个任务一个任务的提交（更为强大）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import ts from concurrent.futures import ThreadPoolExecutor # craw爬取，使用map方式 with ThreadPoolExecutor(10) as t: # map方式需要把要爬取的内容一次性交给线程池 html = t.map(ts.craw, ts.urls) htmls = list(zip(ts.urls, html)) for url, html in htmls: print(url, len(html)) print(\u0026#34;craw over...\u0026#34;) with ThreadPoolExecutor(4) as t: futures = {} # submit 逐个提交参数 for url, html in htmls: res = t.submit(ts.parse, html) futures[res] = url for res, url in futures.items(): print(url, res.result())   多进程 孤儿线程/孤儿进程 孤儿进程：主进程主线程结束了，但子进程子线程还在工作\n为了能够让主线程回收子线程，可以将子线程设置为守护线程，即该线程不重要，主线程结束，子线程也结束\n1 2 3  t1 = threading.Thread(target=func, args=(,)) t1.setDaemon(True) t1.start()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from multiprocessing import Process def add_num(): res = 0 for i in range(10000): res += i print(res) if __name__ == \u0026#39;__main__\u0026#39;: # 创建进程 p1 = Process(target=add_num) p1.daemon = True p1.start() p1.join() # time.sleep(3) print(\u0026#34;主线程：66\u0026#34;)   进程间不同享全局变量\nProcess之间有时需要通信，操作系统提供了很多机制来实现进程间的通信，Queue。from multiprocessing import Queue\n多进程与多线程的语法：\n多进程中队列的使用 多进程中使用普通的队列模块会发生阻塞，对应的需要使用multiprocessing提供的JoinableQueue模块，其使用过程和在线程中使用的queue方法相同\nQueue的使用 可以使用multiprocessing模块的Queue实现多进程之间的数据传递。\nQueue本身是一个消息列队程序，首先用一个小实例来演示一下Queue的工作原理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from multiprocessing import Queue q = Queue(3) # 初始化一个Queue对象，最多可接收三条put消息 q.put(\u0026#34;消息1\u0026#34;) q.put(\u0026#34;消息2\u0026#34;) print(q.full()) # 判断当前队列是否已满: False q.put(\u0026#34;消息3\u0026#34;) print(q.full()) # True # 如果队列已满put_nowait会立即抛出异常，put等待两秒会抛出异常 q.put(\u0026#34;消息4\u0026#34;, True, 2) q.put_nowait(\u0026#34;消息4\u0026#34;) # 推荐的方式，先判断消息列队是否已满，再写入 if not q.full(): q.put_nowait(\u0026#34;消息4\u0026#34;) # 读取消息时，先判断消息列队是否为空，再读取 if not q.empty(): for i in range(q.qsize()): print(q.get_nowait())   说明\n初始化Queue()对象时（例如：q=Queue()），若括号中没有指定最大可接收的消息数量，或数量为负值，那么就代表可接受的消息数量没有上限（直到内存的尽头）；\n Queue.qsize()：返回当前队列包含的消息数量； Queue.empty()：如果队列为空，返回True，反之False ； Queue.full()：如果队列满了，返回True,反之False； Queue.get([block[, timeout]])：获取队列中的一条消息，然后将其从列队中移除，block默认值为True；  1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出\u0026quot;Queue.Empty\u0026quot;异常；\n2）如果block值为False，消息列队如果为空，则会立刻抛出\u0026quot;Queue.Empty\u0026quot;异常；\n Queue.get_nowait()：相当Queue.get(False)； Queue.put(item,[block[, timeout]])：将item消息写入队列，block默认值为True；  1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出\u0026quot;Queue.Full\u0026quot;异常；\n2）如果block值为False，消息列队如果没有空间可写入，则会立刻抛出\u0026quot;Queue.Full\u0026quot;异常；\n Queue.put_nowait(item)：相当Queue.put(item, False)  Queue 实例 我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  from multiprocessing import Process, Queue import time, random # 写数据进程执行的代码: def write(q): for value in [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;]: print(\u0026#39;Put %sto queue...\u0026#39; % value) q.put(value) time.sleep(random.random()) # 读数据进程执行的代码: def read(q): while True: if not q.empty(): value = q.get(True) print(\u0026#39;Get %sfrom queue.\u0026#39; % value) time.sleep(random.random()) else: break if __name__ == \u0026#39;__main__\u0026#39;: # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 等待pw结束: pw.join() # 启动子进程pr，读取: pr.start() pr.join() # pr进程里是死循环，无法等待其结束，只能强行终止: print(\u0026#39;\u0026#39;) print(\u0026#39;所有数据都写入并且读完\u0026#39;)   协程 在爬虫中requests不支持异步，需要使用aiohttp\n协程函数的返回值是一个协程对象\nDeprecationWarning:显式地将协程对象传递给asyncio.wait()自Python 3.8以来已弃用，并计划在Python 3.11中删除。\nawait关键字的含义是：当遇到IO阻塞的时候就让至尊循环（loop = asyncio.get_event_loop()）执行下一个绑定的任务\n多线程和协程 在限制信号量的情况下，爬取效率和多线程相差无几，因为都是在单线程（Python由于GIL锁的机制，多线程其实是单线程）的使用情况下，异步协程只是减少了线程切换所消耗的资源。\n下面图片是在限制信号量为十的情况下的爬取耗时\n下面的图是开启了容量为10的线程池所爬取的耗时\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import asyncio import aiohttp import ts async def craw(url): async with aiohttp.ClientSession() as session: async with session.get(url=url) as response: result = await response.text() print(f\u0026#34;craw url:{url}\u0026#34;, len(result)) async def main(): tasks = [asyncio.create_task(craw(url=url)) for url in ts.urls] await asyncio.wait(tasks) if __name__ == \u0026#39;__main__\u0026#39;: loop = asyncio.get_event_loop() loop.run_until_complete(main())   直接调用异步对象会阻塞 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import asyncio import time import aiohttp async def requests_data(session, i): res = await session.get(\u0026#34;https://www.baidu.com\u0026#34;) print(f\u0026#34;第{i}次请求\u0026#34;) return res async def main(): async with aiohttp.ClientSession() as session: task_list = [] for i in range(30): # 获取协程对象 res = requests_data(session, i) # 创建成task对象，让事件循环可以判断当前是否为可执行对象（判断当前协程的状态：可以运行，待运行，已执行） task = asyncio.create_task(res) task_list.append(task) await asyncio.wait(task_list) # 直接调用异步对象任务会阻塞 # await requests_data(session, i) if __name__ == \u0026#39;__main__\u0026#39;: start_time = time.time() loop = asyncio.get_event_loop() loop.run_until_complete(main()) end_time = time.time() print(f\u0026#34;运行时间：{end_time - start_time}\u0026#34;)   上述代码的运行结果比对：\n真正的非阻塞异步\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # @file : ts.py import requests from lxml import etree from urllib.parse import urljoin old_url = \u0026#34;https://q.cnblogs.com/list/unsolved?page=1\u0026#34; urls = [f\u0026#34;https://www.cnblogs.com/#p{page}\u0026#34; for page in range(1, 20)] # print(urls) def craw(url): response = requests.get(url=url) return response.text def parse(data): html = etree.HTML(data) li_s = html.xpath(\u0026#34;//div[@class=\u0026#39;post-item-text\u0026#39;]\u0026#34;) res_li = [] for li in li_s: title = li.xpath(\u0026#34;./a/text()\u0026#34;)[0] href = li.xpath(\u0026#34;./a/@href\u0026#34;)[0] title_href = urljoin(old_url, href) # print(title_href, title) res_li.append([title_href, title]) return res_li if __name__ == \u0026#39;__main__\u0026#39;: a = parse(craw(urls[0])) print(a)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # @file : ts5.py import asyncio import aiohttp import ts async def craw(url): async with aiohttp.ClientSession() as session: async with session.get(url=url) as response: result = await response.text() print(f\u0026#34;craw url:{url}\u0026#34;, len(result)) async def main(): # task = [(craw(url))for url in ts.urls] # print(len(task)) task = [asyncio.create_task(craw(url)) for url in ts.urls] await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: # asyncio.run(main()) loop = asyncio.get_event_loop() loop.run_until_complete(main()) print(\u0026#34;OVER\u0026#34;)   上述代码的爬取结果：\n第二个直接调用异步对象产生阻塞的案例\n使用信号量限制并发度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import asyncio import aiohttp import ts MAX = 10 SEMAPHORE = asyncio.Semaphore(MAX) async def craw(url): async with SEMAPHORE: async with aiohttp.ClientSession() as session: async with session.get(url=url) as response: result = await response.text() print(f\u0026#34;craw url:{url}\u0026#34;, len(result)) async def main(): tasks = [asyncio.create_task(craw(url=url)) for url in ts.urls] await asyncio.wait(tasks) if __name__ == \u0026#39;__main__\u0026#39;: loop = asyncio.get_event_loop() loop.run_until_complete(main())   协程用法总结 async放在那个东西前面就说明该操作是异步的，生成一个异步任务\nawait是要用在协程函数里面的等待操作前面\n协程的用法：\n 用一个变量获取协程对象req_img = self.get_img(session, hero_name, hero_id) 将该（变量）协程对象创建成任务的形式（task）task = asyncio.create_task(req_img) 将该任务（创建好的task任务形式）添加到列表中tasks.append(task) 协程对象待运行await asyncio.wait(tasks)，多任务交给asyncio.wait()  案例 使用多线程和队列爬取数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119  # @file : aiqiyiduoxiancheng.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 使用多线程和队列的方式对爱奇艺的视频信息进行爬取，并保存到mongodb中 \u0026#34;\u0026#34;\u0026#34; import queue import threading import time import requests import pymongo from urllib.parse import urlencode class Aiqiyi: # 统计插入数据数目 num = 0 def __init__(self): # 1.初始化信息：爬取网页，保存数据所需要的数据 self.db = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) self.collection = self.db[\u0026#34;aiqiyi\u0026#34;][\u0026#34;taiqiyi\u0026#34;] self.url = \u0026#34;https://pcw-api.iqiyi.com/search/recommend/list?\u0026#34; self.headers = { \u0026#34;User-Agent\u0026#34;:\u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\u0026#34; } # 降低耦合度：使用三个队列进行通信 # 获取url的队列 self.url_queue = queue.Queue() # 获取网页待解析的数据 的 队列 self.json_queue = queue.Queue() # 完成解析要存入数据库的数据 的 队列 self.content_save_list = queue.Queue() def get_url(self): # 该方法是要将目标url放入队列 for page in range(1, 5): self.params = { \u0026#39;channel_id\u0026#39;:2, \u0026#39;data_type\u0026#39;:1, \u0026#39;mode\u0026#39;:11, \u0026#39;page_id\u0026#39;:page, \u0026#39;ret_num\u0026#39;:48, \u0026#39;session\u0026#39;:\u0026#39;c6cd981989c038184221a8a95a2a558f\u0026#39;, \u0026#39;three_category_id\u0026#39;:\u0026#39;15;must\u0026#39;, } url = self.url + urlencode(self.params) # 放入操作 self.url_queue.put(url) def get_data(self): # 1.取出存在url队列中的url进行请求， while True: url = self.url_queue.get() print(f\u0026#34;开始请求{url}\u0026#34;) self.response = requests.get(url=url, headers=self.headers) # 2.并将请求的数据放入待解析数据的url队列 self.json_queue.put(self.response.json()) self.url_queue.task_done() def parse_data(self): # 获取数据并解析 while True: print(\u0026#34;开始解析数据\u0026#34;) movie_data = self.json_queue.get()[\u0026#34;data\u0026#34;][\u0026#34;list\u0026#34;] for movie in movie_data: item = {} item[\u0026#34;name\u0026#34;] = movie[\u0026#34;name\u0026#34;] item[\u0026#34;playUrl\u0026#34;] = movie[\u0026#34;playUrl\u0026#34;] item[\u0026#34;score\u0026#34;] = movie[\u0026#34;score\u0026#34;] item[\u0026#34;description\u0026#34;] = movie[\u0026#34;description\u0026#34;] # 将解析好的数据放入队列 self.content_save_list.put(item) self.json_queue.task_done() def save_data(self): while True: data = self.content_save_list.get() print(\u0026#34;准备插入数据\u0026#34;) result = self.collection.insert_one(data) print(\u0026#34;保存成功：\u0026#34;, result) self.num += 1 self.content_save_list.task_done() def main(self): thread_list = [] # 创建一个放入url的线程，并加入到线程列表中 t_url = threading.Thread(target=self.get_url) thread_list.append(t_url) # 创建请求数据的线程 for t in range(3): t_data = threading.Thread(target=self.get_data) thread_list.append(t_data) # 创建解析数据的线程 for i in range(2): t_parse_data = threading.Thread(target=self.parse_data) thread_list.append(t_parse_data) # 存数据的线程 t_save = threading.Thread(target=self.save_data) thread_list.append(t_save) for thread in thread_list: # 守护主线程，在主进程主线程结束后子线程也结束 thread.setDaemon(True) # 开启全部子线程 thread.start() # 下面的语句的作用是：当以下队列不为空时会阻塞主线程等待队列为空 for qu_list in [self.url_queue, self.json_queue, self.content_save_list]: qu_list.join() if __name__ == \u0026#39;__main__\u0026#39;: aiqiyi = Aiqiyi() aiqiyi.main() time.sleep(2) print(f\u0026#34;已插入{aiqiyi.num}条数据\u0026#34;) print(\u0026#34;OVER\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%8D%8F%E7%A8%8B%E5%86%8D%E8%AE%A4%E8%AF%86/","summary":"为什么要使用并发编程 **例如：**使用一个网络爬虫，在没有使用并发编程的时候，花费了1小时，采用并发下载可以大幅度减少下载时间 CPU密集型计","title":"多线程多进程协程再认识"},{"content":"爬虫的概念 为什么要学爬虫 如今我们所处的时代就是一个大数据时代，很多有用的信息都藏在巨大的数据中，并且人工智能，大数据中有一个至关重要的东西，那就是数据，但是我们的数据从哪里来呢？\u0026mdash;爬虫\n数据的来源  去第三方公司购买数据（企查查） 去免费的数据网站下载数据（国家统计局） 爬虫爬取数据 人工收集数据（问卷调查）  爬取的数据的用途  展示到网站上面（今日热榜等网站） 数据分析 抢票软件 投票软件 短信轰炸（通过不断把手机号注册网站）  爬虫的分类  通用爬虫 聚焦爬虫 增量爬虫  ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%AB%E5%86%8D%E8%AE%A4%E8%AF%86-tuling/","summary":"爬虫的概念 为什么要学爬虫 如今我们所处的时代就是一个大数据时代，很多有用的信息都藏在巨大的数据中，并且人工智能，大数据中有一个至关重要的东西，","title":"爬虫再认识 Tuling"},{"content":"哈希hash hash是一类算法，该算法接受传入的内容，经过运算后得到一串hash值\nhash值的特点：\n 只要传入的内容一样得到的hash值必然是一样的 不能由hash反推内容 只要hash算法一样，无论校验的内容有多大得到的hash长度都是固定的  1 2 3 4 5 6 7 8 9 10 11 12 13  import hashlib # 创建了一个hash工厂 hash_factory = hashlib.md5() # 给工厂运送原材料 hash_factory.update(\u0026#34;hello\u0026#34;.encode(\u0026#39;utf-8\u0026#39;)) hash_factory.update(\u0026#34;world\u0026#34;.encode(\u0026#39;utf-8\u0026#39;)) res = hash_factory.hexdigest() #helloworld print(res) hash_factory2 = hashlib.md5(\u0026#34;helloworld\u0026#34;.encode(\u0026#34;utf-8\u0026#34;)) print(hash_factory2.hexdigest())   ","permalink":"https://hanson00.github.io/posts/technology/python/python-hashlib/","summary":"哈希hash hash是一类算法，该算法接受传入的内容，经过运算后得到一串hash值 hash值的特点： 只要传入的内容一样得到的hash值必然是","title":"Python Hashlib"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  import random \u0026#34;\u0026#34;\u0026#34; random模块的一些方法记录 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;random.random():\u0026#34;,random.random()) # 返回(0-1)的float小数 print(\u0026#34;random.randint():\u0026#34;,random.randint(1, 3)) # 返回[1, 3] 间的整数 print(\u0026#34;random.randrange():\u0026#34;,random.randrange(1, 3)) # 返回[1, 3) 间的整数 print(\u0026#34;random.choice():\u0026#34;,random.choice([1, \u0026#34;55\u0026#34;, [14, 22]])) # 随机取其中的某个值\\ # 从列表中任取给定数字的值的组合 print(\u0026#34;random.sample():\u0026#34;,random.sample([1, \u0026#34;55\u0026#34;, [14, 22]], 1)) print(\u0026#34;random.sample():\u0026#34;,random.sample([1, \u0026#34;55\u0026#34;, [14, 22]], 2)) print(\u0026#34;random.uniform():\u0026#34;,random.uniform(1, 3)) # 返回(1, 3)的小数 # 洗牌，随机乱序 item = [4, 5, 6, 7] random.shuffle(item) print(item) # 验证码 res = \u0026#34;\u0026#34; for i in range(6): s1 = chr(random.randint(65, 90)) s2 = str(random.randint(0, 9)) res += random.choice([s1, s2]) print(res)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-random/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import random \u0026#34;\u0026#34;\u0026#34; random模块的一些方法记录 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;random.random():\u0026#34;,random.random()) # 返回(0-1)的float小数 print(\u0026#34;random.randint():\u0026#34;,random.randint(1, 3)) # 返回[1","title":"Python Random"},{"content":"目标网站 https://ggzyfw.fj.gov.cn/business/list/\n首先观察到源代码中没有要包含的数据，所以采用抓包观察\n逆向思路和步骤  打开抓包工具后进行关键词抓包定位  发现找不到关键词，发现数据是密文数据\n如果是没有混淆的js有三种调式方式：\n 直接搜索关键字decrypt ajax渲染，搜索关键字JSON.parse 跟着堆栈，进行调试直到明文数据  找到加密js位置  首先使用搜索关键字decrypt，发现有多个js，接着改用路径搜索\n在找到该js后打开至源面板搜索decrypt\n然后打上断点，并在函数返回值处也打上断点，并点击下一页让该加密逻辑重新执行，我们重新观察\n然后把return返回的数据在控制台中打印观察是否为明文数据，结果为明文数据\n接着往前推，把b函数（前推的函数）复制下来，并把b函数的参数扣出来\n运行后根据报错一步步添加、修改\n安装算法库为npm install crypto-js,然后对比与浏览器中的加密包\n对比发现是无误后直接进行代码替换接着不断调试\n调试后遇到错误后不断去浏览器中观察报错位置\n不断调试后解密出数据\n第二重逆向 逆向解密portal-sign签名、由于该参数的存在会导致爬取失败\n进入加密portal-sign的js后不断扣出js加密代码\n不断调试该加密代码步骤和前面相同\n最终的代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  # coding:utf-8 # @Time : 2022/10/26 15:53 # @file : ts2.py # $software : PyCharm import requests import time import execjs json_data = { \u0026#39;pageNo\u0026#39;: 4, \u0026#39;pageSize\u0026#39;: 20, \u0026#39;total\u0026#39;: 6744, \u0026#39;AREACODE\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;M_PROJECT_TYPE\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;KIND\u0026#39;: \u0026#39;GCJS\u0026#39;, \u0026#39;GGTYPE\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;PROTYPE\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;timeType\u0026#39;: \u0026#39;6\u0026#39;, \u0026#39;BeginTime\u0026#39;: \u0026#39;2022-04-26 00:00:00\u0026#39;, \u0026#39;EndTime\u0026#39;: \u0026#39;2022-10-26 23:59:59\u0026#39;, \u0026#39;createTime\u0026#39;: [], \u0026#39;ts\u0026#39;: round(time.time() * 1000), } portal_sign = execjs.compile(open(\u0026#39;./头部参数逆向.js\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read()).call(\u0026#39;d\u0026#39;, json_data) headers = { \u0026#39;Accept\u0026#39;: \u0026#39;application/json, text/plain, */*\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;zh-CN,zh;q=0.9\u0026#39;, \u0026#39;Cache-Control\u0026#39;: \u0026#39;no-cache\u0026#39;, \u0026#39;Connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=UTF-8\u0026#39;, \u0026#39;Origin\u0026#39;: \u0026#39;https://ggzyfw.fj.gov.cn\u0026#39;, \u0026#39;Pragma\u0026#39;: \u0026#39;no-cache\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;https://ggzyfw.fj.gov.cn/business/list/\u0026#39;, \u0026#39;Sec-Fetch-Dest\u0026#39;: \u0026#39;empty\u0026#39;, \u0026#39;Sec-Fetch-Mode\u0026#39;: \u0026#39;cors\u0026#39;, \u0026#39;Sec-Fetch-Site\u0026#39;: \u0026#39;same-origin\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\u0026#39;, \u0026#39;portal-sign\u0026#39;: portal_sign, \u0026#39;sec-ch-ua\u0026#39;: \u0026#39;\u0026#34;Google Chrome\u0026#34;;v=\u0026#34;107\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;107\u0026#34;, \u0026#34;Not=A?Brand\u0026#34;;v=\u0026#34;24\u0026#34;\u0026#39;, \u0026#39;sec-ch-ua-mobile\u0026#39;: \u0026#39;?0\u0026#39;, \u0026#39;sec-ch-ua-platform\u0026#39;: \u0026#39;\u0026#34;Windows\u0026#34;\u0026#39;, } print(portal_sign) response = requests.post(\u0026#39;https://ggzyfw.fj.gov.cn/FwPortalApi/Trade/TradeInfo\u0026#39;, headers=headers, json=json_data).json() print(response) Data = response[\u0026#39;Data\u0026#39;] json_datas = execjs.compile(open(\u0026#39;./js数据逆向.js\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read()).call(\u0026#39;b\u0026#39;, Data) print(json_datas)   两个js：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  const crypto = require(\u0026#39;crypto\u0026#39;) function d(t) { for (var e in t) \u0026#34;\u0026#34; !== t[e] \u0026amp;\u0026amp; void 0 !== t[e] || delete t[e]; var n = \u0026#34;3637CB36B2E54A72A7002978D0506CDF\u0026#34; + l(t); return MD5Encrypt(n).toLocaleLowerCase() } function MD5Encrypt(text){ return crypto.createHash(\u0026#39;md5\u0026#39;).update(text).digest(\u0026#39;hex\u0026#39;); } function l(t) { for (var e = Object.keys(t).sort(u), n = \u0026#34;\u0026#34;, a = 0; a \u0026lt; e.length; a++) if (void 0 !== t[e[a]]) if (t[e[a]] \u0026amp;\u0026amp; t[e[a]]instanceof Object || t[e[a]]instanceof Array) { var i = JSON.stringify(t[e[a]]); n += e[a] + i } else n += e[a] + t[e[a]]; return n } function u(t, e) { return t.toString().toUpperCase() \u0026gt; e.toString().toUpperCase() ? 1 : t.toString().toUpperCase() == e.toString().toUpperCase() ? 0 : -1 } params = { \u0026#34;ts\u0026#34;: (new Date).getTime(), \u0026#34;pageNo\u0026#34;: 7, \u0026#34;pageSize\u0026#34;: 20, \u0026#34;total\u0026#34;: 6742, \u0026#34;AREACODE\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;M_PROJECT_TYPE\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KIND\u0026#34;: \u0026#34;GCJS\u0026#34;, \u0026#34;GGTYPE\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;PROTYPE\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;timeType\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;BeginTime\u0026#34;: \u0026#34;2022-04-26 00:00:00\u0026#34;, \u0026#34;EndTime\u0026#34;: \u0026#34;2022-10-26 23:59:59\u0026#34;, \u0026#34;createTime\u0026#34;: [] } console.log(d(params))   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  const Crypto = require(\u0026#39;crypto-js\u0026#39;) function b(t) { var e = Crypto.enc.Utf8.parse(\u0026#34;BE45D593014E4A4EB4449737660876CE\u0026#34;) , n = Crypto.enc.Utf8.parse(\u0026#34;A8909931867B0425\u0026#34;) , a = Crypto.AES.decrypt(t, e, { iv: n, mode: Crypto.mode.CBC, padding: Crypto.pad.Pkcs7 }); return a.toString(Crypto.enc.Utf8) } // data = \u0026#39;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\u0026#39; // console.log(b(data))   ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%A6%8F%E5%BB%BA%E7%9C%81%E5%85%AC%E5%85%B1%E8%B5%84%E6%BA%90%E4%BA%A4%E6%98%93%E7%94%B5%E5%AD%90%E5%85%AC%E5%85%B1%E6%9C%8D%E5%8A%A1%E5%B9%B3%E5%8F%B0%E9%80%86%E5%90%91/","summary":"目标网站 https://ggzyfw.fj.gov.cn/business/list/ 首先观察到源代码中没有要包含的数据，所以采用抓包观察 逆向思路和步骤 打开抓包工具后进行关键词抓包定位 发现找不到关键词，发现数据是密文","title":"福建省公共资源交易电子公共服务平台逆向"},{"content":"高等数学小记录 高等数学的核心思想：以直代曲，线性近似\n求导的常用结论 极值的意义与运用举例 洛必达法则的例子 微分 可以将复杂的问题简单化\n微分是对变化量的一种逼近，在某一小段的范围内的曲线用该点的切线来近似代替\ndy表示y的变化量的线性近似\ndx表示x的变化量\n误差：\n目前用一次函数加上一阶导数信息对曲线近似（近似区域窄）\n二次及高次的信息被当作误差，是否能用二阶或者多阶导数来对曲线进行多项式近似\n多项式次数越高，是否近似程度越高\n由此引出泰勒公式\n微分的结论公式 泰勒公式(在一个领域内) 猜想：能否通过一个点上函数值和该点上的各阶导数值，利用多项式去逼近一个函数\n用二次逼近推导 一般求出x0，fx，fx的一二阶导数就能大致推出后续的公式\n对上面解出的未知数套入方程中\n多项式逼近 下图是手动推算泰勒公式的展式规律 以下是泰勒公式的结论和存在条件，f(x)只是逼近p(x)中间还存在误差Rn为误差\n阶数越高，越逼近\n不足之处：误差项无法具体估计，领域范围无法确定【泰勒中值定理2来解决该问题】\n泰勒展式的收敛域 观察是否任意函数在任意点展开是否都随着阶数增大而有效逼近范围增大\n在上面两个例子中发现如下结论：\n上述y=2/x在x=0处有间断点，多项式逼近无法越过间断点，收敛半径等于展开点到间断点的距离\n除了判断原函数是否有间断点，还要判断导数还有虚数范围内有无间断点\n如：y=9/(x^2+9)处的图像如下图\n牛顿迭代法求高次方程的根 原理：任意选取一点，然后求出该点的切线，然后得到该切线与x轴的交点，然后再作该点的垂线与高次函数图像的交点然后继续求出该点的切线然后重复\n迭代公式如下：\n代码的算法流程实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # 使用牛顿迭代法求方程的根：x^4 - 2x^3 -x +2 = 0 # 导函数：4x^3 -6x^2 -1 from sympy import symbols from sympy import solve x0 = 5 err = 1 n = 0 while err \u0026gt; 0.000001: xn = x0 - (x0**4 - 2*x0**3 -x0 + 2)/(4*x0**3 - 6*x0**2 -1) err = abs(xn-x0) x0 = xn n += 1 print(f\u0026#34;根为{x0}，迭代的次数为{n}\u0026#34;) # shi\u0026#39;yong x = symbols(\u0026#34;x\u0026#34;) print(solve(x**4 - 2*x**3 -x + 2))   ","permalink":"https://hanson00.github.io/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%86%E4%BA%AB/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E5%B0%8F%E8%AE%B0/","summary":"高等数学小记录 高等数学的核心思想：以直代曲，线性近似 求导的常用结论 极值的意义与运用举例 洛必达法则的例子 微分 可以将复杂的问题简单化 微分是对变化","title":"高等数学小记"},{"content":"网易云音乐思路(含逆向) 步骤： 1 2 3 4  #爬取网易云音乐评论 # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密|params和encSecKey # 3.请求到网易，拿到评论信息   具体细化思路：  查看网页源代码(判断网页的渲染方式)，看需要爬取的数据是否在网页源代码中 发现其中有查看网页源代码和查看框架源代码（原因是：在网页里面嵌套了网页），要爬取的内容并不在这两个源代码里  打开浏览器开发者工具进行抓包，利用关键词搜索定位爬取数据所在的包  接着在该数据包里发现一堆乱码(加密)参数，也就是被加密的参数，我们需要找到未加密的数据  查看调用栈(查看发送请求时，经过了哪些JS脚本的执行)从下到上  点最近的一个调用栈的JS脚本后，格式化代码然后打上断点  然后重新刷新页面，然后观察到URL是否为我们需要的URL，不是的话就放掉一直刷到我们需要的URL  找到我们需要的URL后，发现参数已经被加密了，所以我们接下来观察在进入该函数之前数据有没有被加密  往回找通过Call Stack，找到参数未被加密前所在的位置  找到加密逻辑  接下来只要设置断点然后重新刷新，然后一步步的调试代码观察代码加密逻辑，然后发现代码的加密逻辑是发生在这段代码  搜索该加密逻辑在代码中的位置得到真正的加密\n将加密逻辑的参数放到控制台中输出进行观察\n观察后发现都是固定的参数，接下来只需要继续根据加密逻辑一一还原即可。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121  #爬取网易云音乐评论 # 步骤： # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密 # 3.请求到网易，拿到评论信息 # 加密参数 # params:就是encText # encSecKey:就是encSecKey from Crypto.Cipher import AES import requests import json from base64 import b64encode url = \u0026#34;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\u0026#34; #POST请求 headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } #找到了真正的参数，接下来还需要解密过程 data = { \u0026#39;csrf_token\u0026#39;: \u0026#34;\u0026#34;, \u0026#39;cursor\u0026#39;: \u0026#34;-1\u0026#34;, \u0026#39;offset\u0026#39;: \u0026#34;0\u0026#34;, \u0026#39;orderType\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageNo\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageSize\u0026#39;: \u0026#34;20\u0026#34;, \u0026#39;rid\u0026#39;: \u0026#34;R_SO_4_1982706733\u0026#34;, \u0026#39;threadId\u0026#39;: \u0026#34;R_SO_4_1982706733\u0026#34; } #处理加密过程 #服务于windows.arXXX的加密过程 e = \u0026#39;010001\u0026#39; f = \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; g = \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; i = \u0026#39;LvjKmGgecBJv6kkF\u0026#39; #手动固定 #如果把i固定那么下面这个函数一定是固定的，由下面的c函数解出 def get_encSecKey(): return \u0026#34;7c435414bc686e49dc26a57ffedbb80b320fd755755c5da8e72f416a61370039aa2d3fa333e54a6e7c9abe7f26faffa69d1721db76cb2e6f17d0393d4cfbc6176590909d027022c4e458aee2123c329b60e1e422beef0f8e39efad5014cbeab022199bc7e6c47a5d0bca5528f6c7946305ae019674309d562c69b39dde9ea429\u0026#34; #字典不能加密，所以下面这个函数默认收到的是字符串 def get_params(data): \u0026#34;\u0026#34;\u0026#34; 就是去还原下面的b的两次加密 h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i) :param data: :return: \u0026#34;\u0026#34;\u0026#34; first = enc_params(data, g) second = enc_params(first, i) return second #返回的就是params #转换成16的长度为加密算法服务 def to_16(data): pad = 16 - len(data) % 16 data += chr(pad) *pad return data #第二个参数的加密算法 def enc_params(data, key): iv = \u0026#39;0102030405060708\u0026#39; data = to_16(data) aes = AES.new(key=key.encode(\u0026#34;utf-8\u0026#34;), IV=iv, mode=AES.MODE_CBC) #创建了一个加密工具 bs = aes.encrypt(data.encode(\u0026#34;utf-8\u0026#34;)) #如果想要返回字符串还需要base64转码 return str(b64encode(bs), \u0026#34;utf-8\u0026#34;) pass \u0026#34;\u0026#34;\u0026#34; function a(a) { var d, e, b = \u0026#34;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\u0026#34;, c = \u0026#34;\u0026#34;; for (d = 0; a \u0026gt; d; d += 1) #循环16次 e = Math.random() * b.length, #产生随机数 e = Math.floor(e), #取整 c += b.charAt(e); return c #返回一个16个字母 } function b(a, b) { a是要加密的内容 var c = CryptoJS.enc.Utf8.parse(b) c和b是一回事 , d = CryptoJS.enc.Utf8.parse(\u0026#34;0102030405060708\u0026#34;) 偏移量 , e = CryptoJS.enc.Utf8.parse(a) e和a相同都是数据 , f = CryptoJS.AES.encrypt(e, c, { e是数据，c是 AES是加密算法自己查的话可以知道下面的东西 iv: d, 偏移量 mode: CryptoJS.mode.CBC 加密模式CBC 看完之后发现少了个密钥所以c(b)是密钥 }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,\u0026#34;\u0026#34;,c), #RSA加密 e = encryptedString(d, a) } function d(d, e, f, g) { d：就是真实数据data,除了数据d是变的之外，其他都是不变的 var h = {}产生一个空对象 e：固定值 \u0026#39;010001\u0026#39; , i = a(16); i是一个16位数的随机字符串 f:超长定值 \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i), i是密钥 #返回的就是加密数据param #如果i固定那么encSeckey就固定 h.encSecKey = c(i, e, f), i是一个16位数的随机字符串，ef是固定的值 #返回的就是加密参数encSecKey return h }windows.a = d加密参数的入口是d encText是通过两次加密得出第二个加密参数 \u0026#34;\u0026#34;\u0026#34; response = requests.post(url, headers=headers, data={ \u0026#34;params\u0026#34;:get_params(json.dumps(data)), \u0026#34;encSecKey\u0026#34;:get_encSecKey() }) print(response.text) response.close() print(\u0026#34;OVER!\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E8%AF%84%E8%AE%BA%E9%80%86%E5%90%91/","summary":"网易云音乐思路(含逆向) 步骤： 1 2 3 4 #爬取网易云音乐评论 # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密|params","title":"网易云音乐评论逆向"},{"content":"爬虫基础介绍 对称密钥加密 即客户端公开自身密钥，同时用公开的密钥加密数据后发送给服务器\n非对称密钥加密 服务器首先告诉客户端自己的加密规则，然后客户端用自己的公开密钥加密好数据后，再用服务器端的数据加密规则加密好后，发送给服务器。\n（服务器把一把锁给客户端，客户端自己用公开密钥加密好数据后，用服务器的锁锁好数据后，发送给服务器）\n证书密钥加密 在非对称密钥加密的方式里增加了一个证书加密机构\n","permalink":"https://hanson00.github.io/posts/technology/python/old_lufei_bobo/","summary":"爬虫基础介绍 对称密钥加密 即客户端公开自身密钥，同时用公开的密钥加密数据后发送给服务器 非对称密钥加密 服务器首先告诉客户端自己的加密规则，然后客","title":"Old_lufei_bobo"},{"content":"Python-yield 对于Python的yield的一些用法的示例\n示例一： yield遇到时会产生一个迭代器（生成器）对象，并会有记忆的记录当前位置以及变量值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def dog(name): print(f\u0026#34;狗哥{name}准备开始吃东西\u0026#34;) while True: x = yield print(f\u0026#34;狗哥{name}开始吃东西{x}\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: g = dog(\u0026#39;alex\u0026#39;) next(g) g.send(\u0026#39;骨头\u0026#39;) g.send(\u0026#39;香肠\u0026#39;) # 输出如下： # 狗哥alex准备开始吃东西 # 狗哥alex开始吃东西骨头 # 狗哥alex开始吃东西香肠   next(g)相当于《====》g.send(None)，可以理解为负责初始化一个yield对象。\n示例二： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  def dog(name): food_list = [] print(f\u0026#34;狗哥{name}准备开始吃东西\u0026#34;) while True: # x是拿到yield接收到的值，返回的是food_list x = yield food_list print(f\u0026#34;狗哥{name}开始吃东西{x}\u0026#34;) food_list.append(x) if __name__ == \u0026#39;__main__\u0026#39;: g = dog(\u0026#39;alex\u0026#39;) next(g) res = g.send(\u0026#39;骨头\u0026#39;) print(res) res = g.send(\u0026#39;香肠\u0026#39;) print(res) # 输出如下： # 狗哥alex准备开始吃东西 # 狗哥alex开始吃东西骨头 # [\u0026#39;骨头\u0026#39;] # 狗哥alex开始吃东西香肠 # [\u0026#39;骨头\u0026#39;, \u0026#39;香肠\u0026#39;]   ","permalink":"https://hanson00.github.io/posts/thinking/python-yield/","summary":"Python-yield 对于Python的yield的一些用法的示例 示例一： yield遇到时会产生一个迭代器（生成器）对象，并会有记忆的记录当前位置以及变量值 1 2","title":"Python Yield"},{"content":"Python类的封装继承和多态的记录 对象可以理解为是“容器”，一个盛放数据和功能的一个容器||整合功能\n类也是“容器”可以存放同类对象共有的数据和功能\ndef __init__(self):\n 会在调用类时自动触发执行，用来初始化对象自己独有的数据 该方法本意是用来初始化对象的属性，但是如果穿插其他任意代码也会执行 该方法的返回值必须是None  封装（隐藏属性和方法） 将封装的属性进行隐藏操作，但Python并没有真正的私有（隐藏）如果想要访问其实是可以访问到的通过_类名__隐藏名可以访问。并且这种隐藏对外不对内，类里面可以访问，且只在类体的隐藏才会发生变形\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class Person: __place = \u0026#34;上海\u0026#34; year = 2022 def __f1(self): print(\u0026#34;hello shanghai\u0026#34;) def get_place(self): print(self.__place) def __init__(self, name, age): self.__name = name self.age = age tom = Person(\u0026#34;tom\u0026#34;, 22) print(tom._Person__place) tom.get_place() print(\u0026#34;----\u0026#34;) Person.__y = 2 print(Person.__dict__)   输出如下：\n1 2 3 4  上海 上海 ---- {\u0026#39;__module__\u0026#39;: \u0026#39;__main__\u0026#39;, \u0026#39;_Person__place\u0026#39;: \u0026#39;上海\u0026#39;, \u0026#39;year\u0026#39;: 2022, \u0026#39;_Person__f1\u0026#39;: \u0026lt;function Person.__f1 at 0x000002404B61E0D0\u0026gt;, \u0026#39;get_place\u0026#39;: \u0026lt;function Person.get_place at 0x000002404E05A3A0\u0026gt;, \u0026#39;__init__\u0026#39;: \u0026lt;function Person.__init__ at 0x000002404E96AE50\u0026gt;, \u0026#39;__dict__\u0026#39;: \u0026lt;attribute \u0026#39;__dict__\u0026#39; of \u0026#39;Person\u0026#39; objects\u0026gt;, \u0026#39;__weakref__\u0026#39;: \u0026lt;attribute \u0026#39;__weakref__\u0026#39; of \u0026#39;Person\u0026#39; objects\u0026gt;, \u0026#39;__doc__\u0026#39;: None, \u0026#39;__y\u0026#39;: 2}   会发现隐藏的属性其实只是发生了变形，如果仍然想访问的话还是可以访问到的，并且在类外想添加隐藏的属性是没有效果的\n隐藏的好处：可以暴露给客户端我们想要给他的功能接口（开接口），并且可以定义相应的业务逻辑\n@property 将类的方法绑定成一个像数据属性\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class Person: __place = \u0026#34;上海\u0026#34; year = 2022 @property def tell(self): print(f\u0026#34;I am {self.__name}is {self.age}olds\u0026#34;) def __init__(self, name, age): self.__name = name self.age = age tom = Person(\u0026#34;tom\u0026#34;, 22) tom.tell   继承 继承表达的是一种什么是什么的关系\n通过继承可以解决类与类的代码冗余问题 继承可以丰富子类的属性和功能\n Python3的类如果没有继承别的类，那么会默认继承Object类 Python支持多继承但不太建议使用多继承，可能会随着继承父类的增多会使可读性变差。 可以使用Mixins的规范使用多继承  多继承 一条线的从下到上继承，在找属性和方法时，使用深度优先搜索找到（一条路走到黑）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class A: pass class B(A): pass class C(B): pass class D(C): pass # mro可以查看搜索的命名空间顺序，存放简单的基类列表 print(D.mro()) # [\u0026lt;class \u0026#39;__main__.D\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;]    下面穿插一个继承方法的调用\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  class Person: def __init__(self, name, age): self.name = name self.age = age def f1(self): print(\u0026#34;Person f1\u0026#34;) def f2(self): print(\u0026#34;Person f2\u0026#34;) # 如果想访问Person对象下面的f1可以使用Person.f1() self.f1() class Students(Person): def __init__(self, name, age, sid): super().__init__(name, age) self.sid = sid def f1(self): print(\u0026#34;Students f1\u0026#34;) student1 = Students(\u0026#34;jhon\u0026#34;, 12, \u0026#34;202201\u0026#34;) student1.f2()   多父类继承 最终的父类没有共同的继承同一个类，一个分支一个分支的找（深度优先搜索）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class A: pass class B(A): pass class C(): pass class D(B,C): pass # mro可以查看搜索的命名空间顺序 print(D.mro()) # [\u0026lt;class \u0026#39;__main__.D\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;]   最终父类继承同一个类（菱形继承）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class A: pass class B(A): pass class C(A): pass class D(B,C): pass # mro可以查看搜索的命名空间顺序 print(D.mro()) # [\u0026lt;class \u0026#39;__main__.D\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;]   有小菱形，有两个最终父类继承同一个类，但也有最终类没有继承该类\n菱形在后面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  class G: pass class A(G): pass class B(): pass class J(G): pass class C(J): pass class D(B,A,C): pass # mro可以查看搜索的命名空间顺序 print(D.mro()) # [\u0026lt;class \u0026#39;__main__.D\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.J\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.G\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;]   菱形在前面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  class G: pass class A(G): pass class B(): pass class J(G): pass class C(J): pass class D(A,C,B): pass # mro可以查看搜索的命名空间顺序 print(D.mro()) # [\u0026lt;class \u0026#39;__main__.D\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.J\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.G\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;]   【拓展】Mixin规范 在类的继承中尽可能的使用单个父类（“什么”是“什么”）然后如果要提成一个功能可以再定义一个功能父类\n例如：顶级父类交通工具类，下面有喷气式飞机类和直升飞机类、汽车类等三个子类，两个飞机有一个共同的方法飞可以提取到一个class FlyMixin:类中，用类名+Mixin来更加直接明了的表明是功能类\n组合 一个对象里面包含另一个对象\n","permalink":"https://hanson00.github.io/posts/thinking/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","summary":"Python类的封装继承和多态的记录 对象可以理解为是“容器”，一个盛放数据和功能的一个容器||整合功能 类也是“容器”可以存放同类对象共有的数","title":"Python面向对象"},{"content":"函数的闭包和装饰器(闭包的语法糖)的使用和基本概念 1.闭包 闭包函数：闭包本质上就是一个函数，并且闭包函数的参数和返回值都是函数。 闭包函数的返回值函数是对传入函数进行增强后的结果\n  用于增强函数的功能\n  函数内的属性都是有生命周期的，生命周期是函数的执行期间\n  闭包内的闭包函数引用了外部函数的变量，使其存活下来\n  能够绑定外部函数里的变量，使其活下来(与内部函数绑定一起存活)\n  在函数内部定义的函数，可以不被外界使用或修改\n  闭包函数的参数和返回值都是函数：传入的函数(参数)，返回的函数是传入函数的增强函数(返回值参数是对传入函数的增强[功能拓展])\n 首先看下一个例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  \u0026#39;\u0026#39;\u0026#39; 进行装饰器的渐入学习 1.将一个函数赋予给另一个变量 \u0026#39;\u0026#39;\u0026#39; def ShowSomething(): print(\u0026#34;你好，请看这里\u0026#34;) #接着我们来运行下这个函数 ShowSomething() print(\u0026#34;__________分割线_________\u0026#34;) #接着我们来把这个函数赋予给另一个变量 receive=ShowSomething print(\u0026#34;____开始使用刚刚赋值的变量_____\u0026#34;) #接着我们使用刚刚被赋值的变量 receive() del ShowSomething # print(ShowSomething()) 如果运行该语句则会发生Nameerror的错误 print(\u0026#34;在删除后原函数后，再次调用被赋值的变量\u0026#34;) receive() #仍然可以运行成功   自己可以试着放到运行环境里调试运行\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  #闭包函数的实例 # outer是外部函数 a和b都是外函数的临时变量 def outer( a ): b = 10 # inner是内函数 def inner(): #在内函数中 用到了外函数的临时变量 print(a+b) # 外函数的返回值是内函数的引用 return inner #这条语句的返回值是属于外部函数的 if __name__ == \u0026#39;__main__\u0026#39;: #此时外函数两个临时变量 a是5 b是10 ，并创建了内函数，然后把内函数的引用返回存给了demo # 外函数结束的时候发现内部函数将会用到自己的临时变量，这两个临时变量就不会释放，会绑定给这个内部函数 demo = outer(5)#这里的demo就是内函数的引用返回，即demo==inner # 我们调用内部函数，看一看内部函数是不是能使用外部函数的临时变量 # demo存了外函数的返回值，也就是inner函数的引用，这里相当于执行inner函数 demo() # 15 demo2 = outer(7) demo2()#17   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  import time \u0026#34;\u0026#34;\u0026#34; 通过闭包增强主要功能函数print_odds,给它增加一个统计时间功能 缺点: 需要显式进行闭包增强 \u0026#34;\u0026#34;\u0026#34; def print_odds(): \u0026#34;\u0026#34;\u0026#34; 输出0~100之间所有奇数,并统计函数执行时间 \u0026#34;\u0026#34;\u0026#34; for i in range(100): if i % 2 == 1: print(i) # 闭包本质上是一个函数 # 闭包函数的传入参数和返回值也都是函数 # 闭包函数的返回值函数是对传入函数进行增强后的结果 def count_time_wrapper(func): \u0026#34;\u0026#34;\u0026#34; 闭包,用于增强函数func: 给函数func增加统计时间的功能 \u0026#34;\u0026#34;\u0026#34; def improved_func(): start_time = time.time_ns() # 起始时间 func() # 执行函数 end_time = time.time_ns() # 结束时间 print(\u0026#34;it takes {}s to find all the olds\u0026#34;.format(end_time - start_time)) return improved_func if __name__ == \u0026#39;__main__\u0026#39;: # 调用count_time_wrapper增强函数 print_odds = count_time_wrapper(print_odds) print_odds()# improved    注意如果对被增强的函数有返回值有参数的话，闭包函数(增强函数)也要有参数和返回值\n1 2 3 4 5 6 7 8 9  def general_wrapper(func): def improved_func(*args, **kwargs): # 接收函数参数 # 增强功能 ret = func(*args, **kwargs) # 传入参数并记录返回值 # 增强功能 return ret # 返回未增强函数的返回值 return improved_func   被增强的函数有返回值和参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import time def count_time_wrapper(func): \u0026#34;\u0026#34;\u0026#34; 闭包,用于增强函数func: 给函数func增加统计时间的功能 \u0026#34;\u0026#34;\u0026#34; def improved_func(*args, **kwargs): # 增强函数应该把就饿收到的所有参数传给原函数 start_time = time.time_ns() # 起始时间 ret = func(*args, **kwargs) # 执行函数 end_time = time.time_ns() # 结束时间 print(\u0026#34;it takes {}s to find all the olds\u0026#34;.format(end_time - start_time)) return ret # 增强函数的返回值应该是原函数的返回值 return improved_func def count_odds(lim=100): \u0026#34;\u0026#34;\u0026#34; 输出0~100之间所有奇数,并统计函数执行时间 \u0026#34;\u0026#34;\u0026#34; cnt = 0 for i in range(lim): if i % 2 == 1: cnt += 1 return cnt if __name__ == \u0026#39;__main__\u0026#39;: print(count_odds(lim=100000)) # 装饰前函数能正常返回,能接收参数 print(\u0026#39;-----------\u0026#39;) count_odds = count_time_wrapper(count_odds) print(count_odds(lim=100000)) # 装饰后函数不能正常返回,不能接收参数   2.装饰器(语法糖) 装饰器(语法糖)，简单来说就是使得闭包函数的使用更简单、便捷。\n引用上面的例子，装饰器其实就是隐藏了闭包增强这条语句print_odds = count_time_wrapper(print_odds)\n1 2 3 4  if __name__ == \u0026#39;__main__\u0026#39;: # 调用count_time_wrapper增强函数 print_odds = count_time_wrapper(print_odds) print_odds()# improved    装饰器的使用@+闭包函数名 装饰器写在需要被增强的函数的前面\n@count_time_wrapper def print_odds(): 这个函数是要被增强的函数(在执行这个函数前先执行装饰器@count_time_wrapper)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import time \u0026#34;\u0026#34;\u0026#34; 通过装饰器进行函数增强,只是一种语法糖,本质上跟上个程序完全一致. \u0026#34;\u0026#34;\u0026#34; def count_time_wrapper(func): \u0026#34;\u0026#34;\u0026#34; 闭包,用于增强函数func: 给函数func增加统计时间的功能 \u0026#34;\u0026#34;\u0026#34; def improved_func(): start_time = time.time_ns() # 起始时间 func() # 执行函数 end_time = time.time_ns() # 结束时间 print( \u0026#34;it takes {}s to find all the olds\u0026#34;.format(end_time - start_time)) return improved_func @count_time_wrapper def print_odds(): \u0026#34;\u0026#34;\u0026#34; 输出0~100之间所有奇数,并统计函数执行时间 \u0026#34;\u0026#34;\u0026#34; for i in range(100): if i % 2 == 1: print(i) if __name__ == \u0026#39;__main__\u0026#39;: # 装饰器等价于在第一次调用函数时执行以下语句: # print_odds = count_time_wrapper(print_odds) print_odds()   3.多个装饰器的执行顺序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def wrapper1(func1): print(\u0026#39;set func1\u0026#39;) def improved_func1(): print(\u0026#39;call func1\u0026#39;) func1() # 封装了improve_func2(original_func) return improved_func1 def wrapper2(func2): print(\u0026#39;set func2\u0026#39;) def improved_func2(): print(\u0026#39;call func2\u0026#39;) func2() # 封装了original_func return improved_func2 @wrapper1 @wrapper2 def original_func(): pass if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;准备开始\u0026#34;) original_func() print(\u0026#39;-----\u0026#39;) original_func()   上面使用多个装饰器运行的代码打印输出如下：\n1 2 3 4 5 6 7 8  set func2 set func1 准备开始 call func1 call func2 ----- call func1 call func2    下面将装饰器拆分成闭包的方式等同执行\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  def wrapper1(func1): print(\u0026#39;set func1\u0026#39;) def improved_func1(): print(\u0026#39;call func1\u0026#39;) func1() # 封装了improve_func2(original_func) return improved_func1 def wrapper2(func2): print(\u0026#39;set func2\u0026#39;) def improved_func2(): print(\u0026#39;call func2\u0026#39;) func2() # 封装了original_func return improved_func2 # @wrapper1 # @wrapper2 def original_func(): pass if __name__ == \u0026#39;__main__\u0026#39;: original_func = wrapper1(wrapper2(original_func)) print(\u0026#34;准备开始\u0026#34;) # 想要运行wrapper1就要把wrapper1的参数运行明白 # original_func = wrapper1(wrapper2(original_func)) # 下面这两条语句等同于上面那一条语句 # original_func = wrapper2(original_func) # original_func = wrapper1(original_func) # original_func封装了improved_func1(improve_func2(original_func)) original_func() print(\u0026#39;-----\u0026#39;) original_func()   多个函数装饰器执行顺序就是从上到下的执行\n1 2 3 4 5 6 7 8 9 10 11 12  @wrapper1 @wrapper2 def original_func(): pass if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;准备开始\u0026#34;) original_func = wrapper1(wrapper2(original_func)) # 下面这两条语句等同于上面那一条语句 # original_func = wrapper2(original_func) # original_func = wrapper1(original_func) # original_func封装了improved_func1(improve_func2(original_func))   简单的说可以理解为：预加载（先加载）的顺序是自下而上，运行的顺序是自上而下\n为什么要这么使用呢？ 没有使用之前：在一个函数里，如果有函数有两个实现功能，当我们后期要修改其中的一个功能的时候就可能对另一个功能产生影响，耦合度过高\n使用后：这一个函数的两个功能分别互不产生干扰，各自运行，修改后也不会对另一个功能产生影响，实现高内聚，低耦合\n装饰器就是函数闭包的一种简单写法 ","permalink":"https://hanson00.github.io/posts/technology/python/%E9%97%AD%E5%8C%85%E5%87%BD%E6%95%B0%E4%B8%8E%E8%A3%85%E9%A5%B0%E5%99%A82.0/","summary":"函数的闭包和装饰器(闭包的语法糖)的使用和基本概念 1.闭包 闭包函数：闭包本质上就是一个函数，并且闭包函数的参数和返回值都是函数。 闭包函数的返","title":"闭包函数与装饰器2.0"},{"content":"Python高级函数 ocp原则：开放对函数的拓展，关闭对函数的修改原则\n高级函数 特点：\n 接收一个函数或多个函数作为参数 将函数作为返回值  特点1接收一个函数或多个函数作为参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  l = [1, 3, 5, 4, 10, 9, -2] def fn2(i): if i % 2 == 0: return True else: return False def fn3(i): if i % 3 == 0: return True else: return False def fn5(i): if i % 5 == 0: return True else: return False def fn(func, lst):#将一个函数作为参数传入 newlist = [] for i in lst: if func(i): newlist.append(i) print(newlist) fn(fn3 ,l) #filter()过滤器可以从序列中过滤出符合条件的元素，保存到一个新的序列中 #参数1：函数，根据函数来过滤序列（可迭代结构） #参数2：需要过滤的序列（可迭代结构） #返回值：过滤后的新的序列（可迭代结构） fiobj = filter(fn2, l) print(list(fiobj))   特点2将函数作为返回值(这种函数我们也称为闭包)  闭包函数的参数和返回值都是函数\n闭包函数的返回值函数是对传入函数的增强\n 1 2 3 4 5 6 7 8 9 10 11 12  def outer(): a = 1024 #闭包函数就是在函数的里面形成一个封闭的空间，外部看不到只有里面可以看到，就如这里的a只有内部函数inner可以访问外部却不可以 def inner(): print(\u0026#34;i am inner\u0026#34;, a) return inner infunc = outer() print(infunc) a = infunc() #闭包的好处，可以将一些私有的数据藏到闭包里外部不可以对其操作，防止对一些私密数据进行修改    形成闭包的条件： 1.函数嵌套 2.将内部函数作为返回值返回 3.内部函数对外部函数的变量进行使用\n 装饰器的引入  通过装饰器可以在不修改原来函数的情况下对函数进行拓展(功能升级)\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 装饰器的引入 # 如果有一个原有的函数add,希望在不修改原有函数的基础上添加 # 两句话开始计算前的数值，结束计算 def add(a, b): print(\u0026#34;a + b的结果是\u0026#34;, a + b) def decorators(a1, b1): print(\u0026#34;开始计算前的数值：\u0026#34;, a1, b1) add(a1, b1) print(\u0026#34;计算结束\u0026#34;) decorators(4, 3) #遗留问题，虽然已经对已有函数在没有修改的情况下完成了功能添加，但如果同类型的需要修改的函数(计算乘法除法)很多修改就显得麻烦   ","permalink":"https://hanson00.github.io/posts/technology/python/%E9%97%AD%E5%8C%85%E5%87%BD%E6%95%B0%E4%B8%8E%E8%A3%85%E9%A5%B0%E5%99%A8/","summary":"Python高级函数 ocp原则：开放对函数的拓展，关闭对函数的修改原则 高级函数 特点： 接收一个函数或多个函数作为参数 将函数作为返回值 特点1接收","title":"闭包函数与装饰器"},{"content":"浅拷贝和深拷贝 浅拷贝：只拷贝对象的内容，只是拷贝子对象的引用\n深拷贝：会连子对象的内存也全部拷贝一份，对子对象的修改不会影响源对象\n变量赋值操作：形成两个变量\n 浅拷贝：就像是要克隆多一个我一样，只是克隆了我但没有克隆我的儿子和孙子 深拷贝：不仅克隆我，还把我的后代也克隆出来一份\n 下面是浅拷贝代码\n 1 2 3 4 5 6 7 8 9 10 11 12 13  import copy a = [1, 5, [7, 9]] b = copy.copy(a) print(\u0026#34;修改前。。。\u0026#34;) print(\u0026#34;a:\u0026#34;, a) print(\u0026#34;b\u0026#34;, b) b.append(22) b[2].append(99) # 对b的子对象进行添加 print(\u0026#34;浅拷贝修改后\u0026#34;) print(\u0026#34;a:\u0026#34;, a) print(\u0026#34;b\u0026#34;, b)    下面是深拷贝代码\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import copy def tsDeepCopy(): a = [1, 5, [7, 9]] b = copy.deepcopy(a) print(\u0026#34;修改前。。。\u0026#34;) print(\u0026#34;a:\u0026#34;, a) print(\u0026#34;b\u0026#34;, b) b.append(22) b[2].append(99) # 对b的子对象进行添加 print(\u0026#34;深拷贝修改后\u0026#34;) print(\u0026#34;a:\u0026#34;, a) print(\u0026#34;b\u0026#34;, b) tsDeepCopy()   深拷贝：完全拷贝一份全新的，不会对源对象产生影响\n","permalink":"https://hanson00.github.io/posts/technology/python/%E6%B5%85%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B7%B1%E6%8B%B7%E8%B4%9D/","summary":"浅拷贝和深拷贝 浅拷贝：只拷贝对象的内容，只是拷贝子对象的引用 深拷贝：会连子对象的内存也全部拷贝一份，对子对象的修改不会影响源对象 变量赋值操作","title":"浅拷贝和深拷贝"},{"content":"Python异常处理和traceback的使用说明  直接在运行结果中打印异常\n 1 2 3 4 5 6 7 8 9 10 11  import traceback try: print(\u0026#34;hello\u0026#34;) result = 1 / 0 except: traceback.print_exc() else: print(\u0026#34;无异常的时候执行的语句\u0026#34;) finally: print(\u0026#34;无论如何都执行的语句\u0026#34;)    将traceback报出的异常输出到文本文件中\n 1 2 3 4 5 6 7 8 9 10 11 12  import traceback try: print(\u0026#34;hello\u0026#34;) result = 1 / 0 except: with open(\u0026#34;except.txt\u0026#34;, \u0026#34;a\u0026#34;) as f: traceback.print_exc(file=f) else: print(\u0026#34;无异常的时候执行的语句\u0026#34;) finally: print(\u0026#34;无论如何都执行的语句\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/thinking/python%E5%BC%82%E5%B8%B8%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/","summary":"Python异常处理和traceback的使用说明 直接在运行结果中打印异常 1 2 3 4 5 6 7 8 9 10 11 import traceback try: print(\u0026#34;hello\u0026#34;) result = 1 / 0 except: traceback.print_exc() else: print(\u0026#34","title":"Python异常知识记录"},{"content":"关于设计模式 越是大的项目，则越是需要使用设计模式\n单例模式的实现 目的：确保某一个类只有一个实例存在。 如果在程序运行期间，有很多地方都要创建一个类的实例对象的话就会导致系统中存在很多该类对象的实例，会严重浪费内存资源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import threading class Singleton: instance = None Lock = threading.RLock() def __init__(self, name): self.name = name def __new__(cls, *args, **kwargs): if cls.instance: return cls.instance # 在锁上面加入的两条语句可以稍微减少点系统资源的使用 with cls.Lock: if cls.instance: return cls.instance cls.instance = object.__new__(cls) return cls.instance def task(): obj = Singleton(\u0026#34;tom\u0026#34;) print(obj) for i in range(10): t = threading.Thread(target=task) t.start()   工厂模式的实现 目的：工厂模式实现了创建者和调用者的分离，使用专门的工厂类选择实现类、创建对象进行统一的管理和控制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  class CarFactory: def say_brand(self): print(\u0026#34;告诉你我是什么车\u0026#34;) def create_car(self, name): if name == \u0026#34;奔驰\u0026#34;: return BenZ() elif name == \u0026#34;比亚迪\u0026#34;: return BYD() elif name == \u0026#34;特斯拉\u0026#34;: return TSLA() else: return \u0026#34;该车不在该工厂生产\u0026#34; class BenZ(CarFactory): def say_brand(self): print(\u0026#34;我是梅赛德斯奔驰\u0026#34;) class BYD(CarFactory): def say_brand(self): print(\u0026#34;我是比亚迪\u0026#34;) class TSLA(CarFactory): def say_brand(self): print(\u0026#34;我是特斯拉\u0026#34;) # 创建工厂类，用于统一创建对象 factory = CarFactory() car1 = factory.create_car(\u0026#34;比亚迪\u0026#34;) car2 = factory.create_car(\u0026#34;特斯拉\u0026#34;) print(car2) print(car1) car2.say_brand()   ","permalink":"https://hanson00.github.io/posts/thinking/python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","summary":"关于设计模式 越是大的项目，则越是需要使用设计模式 单例模式的实现 目的：确保某一个类只有一个实例存在。 如果在程序运行期间，有很多地方都要创建一个","title":"Python设计模式"},{"content":"Docker的了解 问题的出现：开发能运行，运维不能运行，不同开发人员的开发环境，环境配置有所不同，可能依赖于某些配置文件，所以出现了Docker——系统平滑移植，容器虚拟化技术\n把开发的环境全部移植，系统的平滑移植，功能复现\nUbuntu系统镜像文件相当于Docker打包后的镜像文件 Docker就相当于VMware，Docker管理容器，VMware管理虚拟机\n Docker是什么？\n Docker是为了解决运行环境和配置问题的软件容器，方便做持续性集成和软件发布的容器虚拟化技术。\n Docker能干嘛？\n ","permalink":"https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/docker%E5%9F%BA%E7%A1%80/","summary":"Docker的了解 问题的出现：开发能运行，运维不能运行，不同开发人员的开发环境，环境配置有所不同，可能依赖于某些配置文件，所以出现了Dock","title":"Docker基础"},{"content":"JavaScript Hook的使用 在JavaScript逆向的时候，经常需要跟踪某些方法的堆栈调用情况，在很多情况下，一些JavaScript变量或者方法名经过混淆之后非常难跟踪\nHook技术 Hook技术又叫钩子技术，钩子技术是在程序运行时，对其中的某个方法进行重写，即在该方法的前后加入我们自定义的代码。相当于在系统没有调用该函数之前，钩子程序就先捕获消息，得到控制权，这时钩子函数可以加工处理（改变）该函数的执行行为，也可也强制结束消息的传递。\n要对JavaScript代码进行Hook操作，可以使用油猴插件（Tampermonkey）只要我们想要实现的功能使用JavaScript实现的都可以用油猴插件帮我们实现\n 油猴脚本的使用\n  新建一个油猴脚本内容如下  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // ==UserScript== // @name New Userscript // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match http://*/* // @grant none // ==/UserScript==  (function() { \u0026#39;use strict\u0026#39;; // Your code here... })();   代码最上面的是一些注释，非常有用，这部分的内容叫做UserScript Header下面简单介绍一些UserScript Header的内容\n  name：脚本名称，用来显示在控制面板的脚本\n  namespace：脚本的命名空间\n  @homepage、@homepageURL、@website、@source：作者主页，用在油猴的选项页面上，从脚本名称跳转。注意@namespace标记以http://开头\n  @icon、@icoURL：低分辨率图标\n  @icon64、@icon64URL：高分辨率图标\n  @downloadURL：更新脚本下载网址\n  @include：生效页面，可以配置多个但不支持URL Hash\n  @match：约等于@include标签，可以配置多个\n  @supportURL：报告问题的网址\n  exclude：不生效页面\n  require：附加脚本网址，相当于引入外部的脚本，这些脚本会在自定义脚本执行前\n  resource：预加载资源，通过GM_xmlhttpRequest和GM_getResourceText读取\n  run-at：脚本注入的时刻，如页面刚加载或者某个事件发生后\n document-start：尽早执行脚本 document-body：DOM的body出现时  等等\n  noframes：此标记脚本在主页面上运行，但不会在iframe上运行\n  nocompat：标记可以运行的浏览器\n  ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8111-03/","summary":"JavaScript Hook的使用 在JavaScript逆向的时候，经常需要跟踪某些方法的堆栈调用情况，在很多情况下，一些JavaScript变量或者方法名经","title":"崔庆才爬虫书阅读摘要11 03"},{"content":"JavaScript的混淆案例 控制流平坦化 打乱函数原有代码的执行流程及函数调用关系，使代码逻辑变得混乱\n下面的代码定义了两个变量，一个是要被混淆的代码，另一个是混淆选项options，接下来引入了javascript-obfuscator这个库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  const code = ` let x = \u0026#39;1\u0026#39; + 1 console.log(\u0026#39;x\u0026#39;, x) ` const options = { compact: false, controlFlowFlattening: true } const obfuscator = require(\u0026#39;javascript-obfuscator\u0026#39;) function obfuscate(code, options) { return obfuscator.obfuscate(code, options).getObfuscatedCode() } console.log(obfuscate(code, options))   代码混淆后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  function _0x84d8(_0x232b63, _0x7c8409) { const _0xae8ca9 = _0xae8c(); return _0x84d8 = function (_0x84d803, _0x468060) { _0x84d803 = _0x84d803 - 0x119; let _0x2a41f0 = _0xae8ca9[_0x84d803]; return _0x2a41f0; }, _0x84d8(_0x232b63, _0x7c8409); } const _0x338b67 = _0x84d8; (function (_0x34f352, _0x496f80) { const _0x6ae8 = _0x84d8, _0x508e1f = _0x34f352(); while (!![]) { try { const _0x13338a = parseInt(_0x6ae8(0x11f)) / 0x1 + parseInt(_0x6ae8(0x120)) / 0x2 + -parseInt(_0x6ae8(0x119)) / 0x3 + parseInt(_0x6ae8(0x11e)) / 0x4 + -parseInt(_0x6ae8(0x11b)) / 0x5 * (-parseInt(_0x6ae8(0x11d)) / 0x6) + -parseInt(_0x6ae8(0x122)) / 0x7 + parseInt(_0x6ae8(0x121)) / 0x8 * (parseInt(_0x6ae8(0x11a)) / 0x9); if (_0x13338a === _0x496f80) break; else _0x508e1f[\u0026#39;push\u0026#39;](_0x508e1f[\u0026#39;shift\u0026#39;]()); } catch (_0x5501e1) { _0x508e1f[\u0026#39;push\u0026#39;](_0x508e1f[\u0026#39;shift\u0026#39;]()); } } }(_0xae8c, 0x3bafe)); let x = \u0026#39;1\u0026#39; + 0x1; function _0xae8c() { const _0x313245 = [ \u0026#39;1086765ZUQWlZ\u0026#39;, \u0026#39;2745xICdrf\u0026#39;, \u0026#39;15RqRpej\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;87216zPpbQZ\u0026#39;, \u0026#39;1269180jrZhoJ\u0026#39;, \u0026#39;171687rKBNSE\u0026#39;, \u0026#39;268350KSFeZM\u0026#39;, \u0026#39;912HWTQLX\u0026#39;, \u0026#39;663614txjhyP\u0026#39; ]; _0xae8c = function () { return _0x313245; }; return _0xae8c(); } console[_0x338b67(0x11c)](\u0026#39;x\u0026#39;, x);   等等几种不同的代码混淆，上面只列举了其中一种\n代码压缩 javascript-obfuscator也提供了代码压缩功能，使用参数compact即可完成代码压缩，compact代码的默认值是true，如果定义为false，则混淆后的代码会分行显示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  const code = ` let x = \u0026#39;1\u0026#39; + 1 console.log(\u0026#39;x\u0026#39;, x) ` const options = { compact: false, } const obfuscator = require(\u0026#39;javascript-obfuscator\u0026#39;) function obfuscate(code, options) { return obfuscator.obfuscate(code, options).getObfuscatedCode() } console.log(obfuscate(code, options))   代码混淆压缩后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  const _0x2ab95d = _0x1f84; function _0x1a23() { const _0x578d08 = [ \u0026#39;96eRWDpg\u0026#39;, \u0026#39;8984928gWKMJA\u0026#39;, \u0026#39;9002630KfHyUm\u0026#39;, \u0026#39;22966670jhIRCB\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;9ZRbZcX\u0026#39;, \u0026#39;732322MIgNFb\u0026#39;, \u0026#39;8644oLTwYz\u0026#39;, \u0026#39;8735136AsKbiE\u0026#39;, \u0026#39;115yehIcV\u0026#39;, \u0026#39;165476VyvkLP\u0026#39; ]; _0x1a23 = function () { return _0x578d08; }; return _0x1a23(); } function _0x1f84(_0x5ab312, _0x4dd400) { const _0x1a2343 = _0x1a23(); return _0x1f84 = function (_0x1f84f0, _0x4618ff) { _0x1f84f0 = _0x1f84f0 - 0x106; let _0x4f0728 = _0x1a2343[_0x1f84f0]; return _0x4f0728; }, _0x1f84(_0x5ab312, _0x4dd400); } (function (_0x41caab, _0x244416) { const _0x41158a = _0x1f84, _0x1bcd5f = _0x41caab(); while (!![]) { try { const _0x307e14 = -parseInt(_0x41158a(0x10e)) / 0x1 + -parseInt(_0x41158a(0x10f)) / 0x2 * (parseInt(_0x41158a(0x108)) / 0x3) + -parseInt(_0x41158a(0x107)) / 0x4 * (-parseInt(_0x41158a(0x106)) / 0x5) + -parseInt(_0x41158a(0x110)) / 0x6 + -parseInt(_0x41158a(0x10a)) / 0x7 + parseInt(_0x41158a(0x109)) / 0x8 + parseInt(_0x41158a(0x10d)) / 0x9 * (parseInt(_0x41158a(0x10b)) / 0xa); if (_0x307e14 === _0x244416) break; else _0x1bcd5f[\u0026#39;push\u0026#39;](_0x1bcd5f[\u0026#39;shift\u0026#39;]()); } catch (_0x11dbd9) { _0x1bcd5f[\u0026#39;push\u0026#39;](_0x1bcd5f[\u0026#39;shift\u0026#39;]()); } } }(_0x1a23, 0xb93aa)); let x = \u0026#39;1\u0026#39; + 0x1; console[_0x2ab95d(0x10c)](\u0026#39;x\u0026#39;, x);   使用参数的true后：\n1  const _0x5a4df1=_0x7697;(function(_0x55b341,_0x15ac40){const _0x414762=_0x7697,_0x3d9d03=_0x55b341();while(!![]){try{const _0x5cc75a=parseInt(_0x414762(0x19c))/0x1*(parseInt(_0x414762(0x19e))/0x2)+parseInt(_0x414762(0x194))/0x3*(-parseInt(_0x414762(0x19b))/0x4)+-parseInt(_0x414762(0x19f))/0x5*(parseInt(_0x414762(0x197))/0x6)+-parseInt(_0x414762(0x198))/0x7*(-parseInt(_0x414762(0x19a))/0x8)+parseInt(_0x414762(0x193))/0x9*(-parseInt(_0x414762(0x19d))/0xa)+parseInt(_0x414762(0x192))/0xb*(-parseInt(_0x414762(0x199))/0xc)+parseInt(_0x414762(0x196))/0xd;if(_0x5cc75a===_0x15ac40)break;else _0x3d9d03[\u0026#39;push\u0026#39;](_0x3d9d03[\u0026#39;shift\u0026#39;]());}catch(_0x16786b){_0x3d9d03[\u0026#39;push\u0026#39;](_0x3d9d03[\u0026#39;shift\u0026#39;]());}}}(_0x21e4,0xa579a));function _0x21e4(){const _0x38d601=[\u0026#39;270DYTzMt\u0026#39;,\u0026#39;7lcowcx\u0026#39;,\u0026#39;3322776MrAtBz\u0026#39;,\u0026#39;6726536GxTMrv\u0026#39;,\u0026#39;39660vmtnkf\u0026#39;,\u0026#39;11069YwvBsu\u0026#39;,\u0026#39;10hhpWog\u0026#39;,\u0026#39;244bVHKjm\u0026#39;,\u0026#39;98915yOeVVc\u0026#39;,\u0026#39;11MZcRZk\u0026#39;,\u0026#39;3926520biJLHV\u0026#39;,\u0026#39;93MRIvnp\u0026#39;,\u0026#39;log\u0026#39;,\u0026#39;5165277CxRoGq\u0026#39;];_0x21e4=function(){return _0x38d601;};return _0x21e4();}let x=\u0026#39;1\u0026#39;+0x1;function _0x7697(_0x5dac0d,_0xf2496b){const _0x21e494=_0x21e4();return _0x7697=function(_0x7697b1,_0x4e7c9e){_0x7697b1=_0x7697b1-0x192;let _0x460700=_0x21e494[_0x7697b1];return _0x460700;},_0x7697(_0x5dac0d,_0xf2496b);}console[_0x5a4df1(0x195)](\u0026#39;x\u0026#39;,x);   变量名混淆 变量名混淆在javascript-obfuscator中配置identifierNamesGenerator参数来实现，如果将其值设置为hexadecimal，则会将变量名替换为十六进制形式的字符串 hexadecimal：将变量名替换为十六进制形式的字符串，如0xabc123 mangled：将变量名替换成普通的简写字符，如a，b等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  const code = ` let x = \u0026#39;1\u0026#39; + 1 console.log(\u0026#39;x\u0026#39;, x) ` const options = { compact: true, identifierNamesGenerator: \u0026#39;mangled\u0026#39;, } const obfuscator = require(\u0026#39;javascript-obfuscator\u0026#39;) function obfuscate(code, options) { return obfuscator.obfuscate(code, options).getObfuscatedCode() } console.log(obfuscate(code, options))   混淆后：\n1  function a(){const j=[\u0026#39;258231xUxfBs\u0026#39;,\u0026#39;7819904LvqlCs\u0026#39;,\u0026#39;979982uYIPDx\u0026#39;,\u0026#39;453426qgzSUB\u0026#39;,\u0026#39;7290FZDoxL\u0026#39;,\u0026#39;5712655GSQcsE\u0026#39;,\u0026#39;7kogAFv\u0026#39;,\u0026#39;36SmeNkk\u0026#39;,\u0026#39;1804860OZVDEi\u0026#39;,\u0026#39;log\u0026#39;,\u0026#39;2763EqCXTz\u0026#39;];a=function(){return j;};return a();}function b(c,d){const e=a();return b=function(f,g){f=f-0x1a9;let h=e[f];return h;},b(c,d);}const i=b;(function(c,d){const h=b,e=c();while(!![]){try{const f=-parseInt(h(0x1ad))/0x1+-parseInt(h(0x1b2))/0x2*(-parseInt(h(0x1ab))/0x3)+parseInt(h(0x1b3))/0x4+parseInt(h(0x1b0))/0x5+-parseInt(h(0x1ae))/0x6*(parseInt(h(0x1b1))/0x7)+-parseInt(h(0x1ac))/0x8+parseInt(h(0x1aa))/0x9*(-parseInt(h(0x1af))/0xa);if(f===d)break;else e[\u0026#39;push\u0026#39;](e[\u0026#39;shift\u0026#39;]());}catch(g){e[\u0026#39;push\u0026#39;](e[\u0026#39;shift\u0026#39;]());}}}(a,0xd8610));let x=\u0026#39;1\u0026#39;+0x1;console[i(0x1a9)](\u0026#39;x\u0026#39;,x);   另外，renameGlobals这个参数还可以指定是否混淆全局变量和函数名称，默认值是false\n字符串混淆 将一个字符串声明放到一个数组里面，使之无法被直接搜索到。这个参数通过stringArray来控制，默认是true\n1 2 3 4 5 6 7 8 9 10  const code = ` var a = \u0026#39;hello world\u0026#39; ` const options = { stringArray: true, rotateStringArray: true, stringArrayEncoding: true, // \u0026#39;base64\u0026#39; or \u0026#39;rc4\u0026#39; or false  stringArrayThreshold: 1, }   代码自我保护 可以通过selfDefending参数来开启代码自我保护功能。开启后，混淆的JavaScript代码会强制以一行的形式显示。如果我们将混淆后的代码进行格式化或重命名，该段代码将无法执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13  const code = ` console.log(\u0026#39;hello world\u0026#39;) ` const options = { selfDefending: true } const obfuscator = require(\u0026#39;javascript-obfuscator\u0026#39;) function obfuscate(code, options) { return obfuscator.obfuscate(code, options).getObfuscatedCode() } console.log(obfuscate(code, options))   混淆后的代码：\n1  var _0x45b49a=_0x43da;(function(_0x59aaf2,_0x4b59ef){var _0x1b8d82=_0x43da,_0x3968c7=_0x59aaf2();while(!![]){try{var _0x3af556=-parseInt(_0x1b8d82(0x9b))/0x1*(-parseInt(_0x1b8d82(0xa0))/0x2)+parseInt(_0x1b8d82(0xaa))/0x3*(parseInt(_0x1b8d82(0x98))/0x4)+-parseInt(_0x1b8d82(0x9d))/0x5*(-parseInt(_0x1b8d82(0x99))/0x6)+parseInt(_0x1b8d82(0xa9))/0x7+parseInt(_0x1b8d82(0xa3))/0x8*(parseInt(_0x1b8d82(0xa4))/0x9)+-parseInt(_0x1b8d82(0x9a))/0xa*(parseInt(_0x1b8d82(0xa8))/0xb)+-parseInt(_0x1b8d82(0xa6))/0xc;if(_0x3af556===_0x4b59ef)break;else _0x3968c7[\u0026#39;push\u0026#39;](_0x3968c7[\u0026#39;shift\u0026#39;]());}catch(_0x34506f){_0x3968c7[\u0026#39;push\u0026#39;](_0x3968c7[\u0026#39;shift\u0026#39;]());}}}(_0x626d,0xc0b82));function _0x626d(){var _0x1ad3d3=[\u0026#39;constructor\u0026#39;,\u0026#39;5YFEyCe\u0026#39;,\u0026#39;toString\u0026#39;,\u0026#39;(((.+)+)+)+$\u0026#39;,\u0026#39;2zmmKZw\u0026#39;,\u0026#39;apply\u0026#39;,\u0026#39;search\u0026#39;,\u0026#39;376VGKKAg\u0026#39;,\u0026#39;50247ICTFpG\u0026#39;,\u0026#39;hello\\x20world\u0026#39;,\u0026#39;33992208CZwJly\u0026#39;,\u0026#39;log\u0026#39;,\u0026#39;173822NaLfqC\u0026#39;,\u0026#39;8996029QHyncx\u0026#39;,\u0026#39;5517PHPoSl\u0026#39;,\u0026#39;948gpqWlh\u0026#39;,\u0026#39;5564292VwLISd\u0026#39;,\u0026#39;260UHMpCM\u0026#39;,\u0026#39;1122141cIoqoQ\u0026#39;];_0x626d=function(){return _0x1ad3d3;};return _0x626d();}var _0x407526=(function(){var _0x116741=!![];return function(_0x3a0166,_0x2acdf4){var _0x3e664e=_0x116741?function(){var _0x1c18b1=_0x43da;if(_0x2acdf4){var _0x36e5a7=_0x2acdf4[_0x1c18b1(0xa1)](_0x3a0166,arguments);return _0x2acdf4=null,_0x36e5a7;}}:function(){};return _0x116741=![],_0x3e664e;};}()),_0x191b2a=_0x407526(this,function(){var _0x367b78=_0x43da;return _0x191b2a[\u0026#39;toString\u0026#39;]()[_0x367b78(0xa2)](_0x367b78(0x9f))[_0x367b78(0x9e)]()[_0x367b78(0x9c)](_0x191b2a)[_0x367b78(0xa2)](\u0026#39;(((.+)+)+)+$\u0026#39;);});function _0x43da(_0x5ea48d,_0x4aec27){var _0x20ebf3=_0x626d();return _0x43da=function(_0x191b2a,_0x407526){_0x191b2a=_0x191b2a-0x98;var _0x626d2f=_0x20ebf3[_0x191b2a];return _0x626d2f;},_0x43da(_0x5ea48d,_0x4aec27);}_0x191b2a(),console[_0x45b49a(0xa7)](_0x45b49a(0xa5));   控制流平坦化 控制流平坦化就是将代码执行逻辑混淆，基本思想是将一些逻辑处理块都统一加上一个前驱逻辑块，每个逻辑块都有前驱逻辑块进行条件判断触发，构成一个个闭环逻辑\n无用代码注入 无用代码就是不会被执行党的代码或者对上下文没有任何影响的代码，注入之后对现有的JavaScript代码的阅读造成干扰使用deadCodeInjection参数来开启，默认值是false\n对象键名替换 如果是一个对象可以使用transformObjectKeys来对对象的键值进行替换\n禁用控制台输出 使用disableConsoleOutput来禁掉console.log的输出功能增大调试难度\n域名锁定 通过使用domainLock来控制代码只能在特定的域名下运行，这样可以降低代码被模拟和盗用的风险\n特殊编码 还有一些特殊的工具包可以对代码进行混淆，工具有（aaencode、jjencode、jsfick）这些工具都会把代码转换成完全不可读的代码，但是虽然看起来另类但可以找到规律从而还原\nWebAssembly 随着技术的发展，WebAssembly流行起来，不同于JavaScript混淆技术，WebAssembly的基本思路是将一些核心逻辑使用其他语言（C/C++）来编写，并编译成类似字节码的文件，并通过JavaScript调用执行，从而起到二进制级别的防护作用。\nWebAssemly是一种使用非JavaScript编程语言编写代码并且能在浏览器上运行的技术方案。比如能将C/C++文件利用Emscripten编译工具转换成wasm格式的文件，JavaScript可以直接调用该文件中执行的方法。\nWebAssemly是经过编译器编译后的字节码，可以从C/C++编译而来，得到的字节码具有和JavaScript相同的功能，运行速度更快，体积更小，而且语法上完全脱离JavaScript，具有沙盒化的执行环境\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8111-02/","summary":"JavaScript的混淆案例 控制流平坦化 打乱函数原有代码的执行流程及函数调用关系，使代码逻辑变得混乱 下面的代码定义了两个变量，一个是要被混","title":"崔庆才爬虫书阅读摘要11 02"},{"content":"JavaScript逆向爬虫 网站为了保护数据一般分为两种情况：\n URL和API参数加密 JavaScript压缩、混淆和加密\n 网站数据防护介绍 下面介绍一些网站常用的加密手段\nURL和API参数加密 对某些数据接口的参数进行加密，比如给某些URL的参数加上校验码，给一些ID信息编码。给某些API请求加上token、sign等签名，这些请求发送到服务器时，服务器会通过客户端发送过来的请求信息和约定好的密钥进行校验，只有校验通过后才返回响应的数据。\n比如客户端和服务端约定的一种接口校验，当客户端每次请求服务端接口的时候都会附带一个sign参数，该参数是当前的时间信息、请求的URL、请求的数据、设备ID、双方约定的密钥，在经过一些加密算法后生成。客户端会实现该加密算法，然后每次请求时都会附带该sign参数，服务器则会根据约定好的算法来对请求数据sign进行验证\n又比如登录状态作为校验，有些请求则必须要求传递一个token，这个token必须在登录以后才能获取\nJavaScript压缩、混淆和加密 在前端开发中现在JavaScript混淆的主流实现方式是：Javascript-obfuscator和terser这两个库\n接口加密技术看起来是一个不错的反爬手段，但单纯依靠它并不能很好的解决问题。 对于网页来说，逻辑是依赖于JavaScript来实现的，JavaScript有如下的优点：\n JavaScript代码运行于客户端，必须在客户端才能运行 JavaScript代码公开透明，浏览器可以直接获取正在运行的JavaScript代码  所以基于这两个原因JavaScript代码是不安全的，所以接口加密技术如果不在JavaScript层面做一些适当的加密，其实接口加密技术起不到什么防护作用。\nJavaScript的压缩、混淆和加密简述如下：\n 代码压缩：去除JavaScript代码中不必要的空格、换行等内容，使JavaScript代码压缩到几行，降低代码的可读性，加快网站的运行速度。 代码混淆：使用变量替换、字符串阵列化、控制流平坦化、多态变异、僵尸函数、调试保护等手段，使代码变得难以阅读和分析，达到最终保护目的。这部影响代码的原有功能，是一种很好的JavaScript保护手段。 代码加密：可以通过某种手段将JavaScript代码进行加密，转成人无法阅读或者解析的代码，如借用WebAssembly技术，可以直接将JavaScript代码用C/C++实现，JavaScript调用其编译后形成的文件来执行相应的功能。   下面对上面讲述的技术做详细介绍\n URL和API参数加密(详细介绍) 限制绝大多数的网站的数据都是通过服务器提供的API来获取的，网站或app可以请求到对应的API来获取对应的数据，然后再把获取的数据进行展示，但是有些数据比较宝贵或私密，所以需要在一定层面上对数据进行不同的安全防护。\n为了提升接口的安全性，客户端会和服务端约定一种接口校验方式，一般来说会使用到各种加密和编码算法。如Base64、Hex编码、MD5、AES、DES、RSA等对称或非对称加密。\n举个例子，比如客户端和服务端会约定一个sign作为接口校验的签名校验，其生成逻辑是客户端先将URL路径进行MD5加密，然后拼接上URL的某个参数再进行Base64编码，最后得到一个字符串sign，这个sign会通过Request URL的某个参数发送给服务器，服务器收到请求后，同样对该URL进行一样的加密方式然后将得到的sign和客户端发送过来的sign做比较。（还可以增加当前时间戳来增加时效性）\n这里要实现的接口参数加密，就需要用到一些加密算法如JavaScript的crypto-js、Python的hashlib、Crypto等等，当然如果不对JavaScript采取一些压缩、混淆等方法来对JavaScript的逻辑进行一些保护的话，还是很容易被破解的。\nJavaScript压缩（详细介绍） JavaScript的压缩即去除JavaScript中不必要的空格、换行、变量名压缩等内容，但是如果仅仅是去除空格、换行这样的压缩几乎没有任何防范作用，因为浏览器有一些自带的格式化工具可以把压缩的JavaScript代码还原。\n目前主流的前端开发技术大多会利用Webpack、Rollup等工具进行打包。Webpack、Rollup会对源代码进行编译和压缩，输出打包好的几个JavaScript文件，其中可以看到输出的JavaScript文件名带有一些不规则的字符串，同时内容可能只有几行，变量名都用一些简单的字母来表示，这就是JavaScript的压缩技术，比如一些公共的库就输出成bundle文件，一些调用逻辑压缩和转义成冗长的几行代码，这些都属于JavaScript的压缩，甚至还有将变量名和方法名用单个字母来表示来降低可读性。\nJavaScript混淆（详细介绍） 主要的混淆技术有：\n 变量名混淆：将一些有意义的变量名，方法名等随便变换成无意义的类乱码字符串，降低代码的可读性，或者变换成单个字符或十六进制的字符串 字符串混淆：将字符串阵列化集中放置并可进行MD5或Base64加密存储，使代码中不出现明文字符串，这样可以避免全局搜索字符串的方式定位到入口 对象键名替换：针对JavaScript对象的属性进行加密转化，隐藏代码之间的调用关系 控制流平坦化：打乱函数原有代码的执行流程及函数调用关系，使代码逻辑变得混乱 无用代码注入：随机再代码中插入不会被执行到的无用代码，进一步使代码看起来更加的混乱难以阅读 调试保护：基于调试器的特性，对当前环境进行检验时，加入一些debugger语句，使得其在调试模式下难以顺利执行JavaScript代码 多态变异：使JavaScript代码每次被调用时，将代码自动产生变异，变异成与之前不同的代码，即功能完全不变，但代码形式变异，以杜绝代码被动态分析和调试 域名锁定：JavaScript代码只能在特定的域名下执行 代码自我保护：如果将JavaScript代码进行格式化，则无法执行，导致浏览器假死 特殊编码：将JavaScript代码完全编码成人为不可读的代码，如表情符号和特殊表示内容等等  在前端开发中，现在JavaScript混淆的主流实现是javascript-obfuscator和terser这两个库，他们都能提供一些代码混淆功能，也有对应的Webpack和Rollup打包工具的插件。\n接下来阅读混淆详情案例 ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8111/","summary":"JavaScript逆向爬虫 网站为了保护数据一般分为两种情况： URL和API参数加密 JavaScript压缩、混淆和加密 网站数据防护介绍 下面","title":"崔庆才爬虫书阅读摘要11"},{"content":" 引言：模拟登录现在一般分为两种模式，一种是基于session和cookie的模拟登录，一种是基于JWT（JSON Web Token）的模拟登录。\n对于第一种模式，简单来说就是我们打开网页模拟登录后，服务器就会返回一个Set-Cookie字段的响应头，客户端会生成对应的Cookie，其中保存着SessionID的相关信息，如果再次访问该网站时，服务器会校验客户端发送过来的Cookie信息是否有效。\n第二种模式，由于现在前后端的开发模式，所以使用JWT的校验登录，在请求数据时服务器会检查JWT是否有效。\n 模拟登录的基本原理 1.网站登录验证的实现 要模拟登录首先要了解网站是如何验证登录内容的。\n登录内容一般是\u0026ndash;用户名和密码，有的是手机短信等等，这些方式都是把一些可供认证的信息提交给服务器。 例如：用户在浏览器的登陆表单中输入了用户名和密码然后点击登录，这时，浏览器会向服务器发送一个登录请求（并携带刚刚输入的登录信息），服务器会去处理这些请求然后返回一个类似凭证的东西，让客户端浏览器可以继续访问，这个凭证就是JWT。\n2.基于Session和Cookie 不同网站对于用户的登录状态的实现可能不同但基本上都是Session和Cookie相互配合实现的。\n Cookie可能保存了SessionID的信息，服务器可以根据这个信息找到对应的Session（首次登录后服务器会生成一个Session并将SessionID返回给客户端浏览器） Cookie里直接保存某些凭证的信息，客户端发起登录请求，服务器通过校验后，返回给客户端的信息里面可能带有Set-Cookie字段，里面包含有类似凭证的信息，（登录过一次后）每次访问网站时，都会拿着这些信息去校验。  3.基于JWT 随着前后端项目的流行，传统的基于Session和Cookie的登录校验存在着一些弊端，例如服务器需要维护Session信息，分布式部署不方便等的问题，所以JWT才孕育而生。\nJWT（JSON Web Token）是为了在网络环境中传递声明而执行的一种基于JSON的开放标准，实际上每次登录时通过一个Token字段校验登录状态。JWT一般是要经过一个Base64编码技术的加密的字符串类似于下面的格式 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ一般有两个点分隔开三个字符串，因此可以把JWT看成是一个三段式加密的字符串，这三部分分别是Header、Payload、Signature。\n Header：声明了JWT的签名算法（如RSA、SHA256等）还可能包含JWT编号和类型等数据。 Playload：通常是一些业务需要但是又不敏感的信息（如UserID）另外还有很多默认字段，如JWT签发者，JWT接受者，JWT过期信息等 Signature：这就是一个签名，利用密钥secret对Header和Playload的信息进行加密后形成的，这个密钥保存在服务端，如果Playload被篡改服务器就能通过Signature判断出是非法请求，从而拒绝。  JWT的登录认证流程：用户登陆后，服务器生成JWT字符串并返回给客户端，之后客户端每次请求都会带着JWT，服务器会进行判断，JWT的传递方式：可以放在请求头中，可以放在URL中，还可以放在Cookie里，只要能传递给服务器校验即可。\n爬虫模拟登录 基于Session和Cookie的模拟登录 爬虫要实现Session和Cookie的模拟登录就是要让爬虫当成浏览器，把浏览器的事情模拟完成。下面给出实现步骤：\n 手动在浏览器中登录账号。登陆后直接把Cookie复制给爬虫（相当于手动登录）爬虫每次请求时就把Cookie放到请求头中进行登录。 爬虫自动化登录，爬虫模拟浏览器进行登录，爬虫把用户名、密码等信息提交给服务器，服务器返回可能会有Set-Cookie字段，我们只需把这字段保存下来。当然也可能会遭遇一些困难，例如登录过程中伴随着各种校验参数可能不好直接模拟，并且客户端设置Cookie时通过Javascript语言实现的还要分析其中的逻辑。 登录过程自动化。使用例如Selenium和Pyppeteer或Playwright驱动浏览器模拟执行一些操作（如填写用户名和密码）等操作实现登录后，然后再获取当前的Cookie并保存，同样之后再拿着保存的Cookie进行请求。  基于JWT的模拟登录 由于JWT的字符串就是用户访问的凭证，所以模拟登录的步骤如下：\n 模拟登录操作：输入用户名和密码请求登录，然后获取服务器返回的结果，这个结果通常包含有JWT信息，保存下来即可 之后只要保存的JWT信息不过期一直使用即可 如果过期则重复上面步骤即可  模拟登录过程可能会有加密参数，具体情况分析即可。\n账号池 如果要爬取的数据量比较大，网站对单个账号又有访问限制，我们可以建立一个账号池降低被封号的风险。如建立一个账号池，用多个账号访问网站（模拟登录）然后保存对应的Cookie或JWT然后每次随机选取一个来访问，从而降低被封号的风险\n基于Session和Cookie的模拟登录实战 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  \u0026#34;\u0026#34;\u0026#34; 基于Session和Cookie的模拟登录实现 \u0026#34;\u0026#34;\u0026#34; import requests from urllib.parse import urljoin BASE_URL = \u0026#34;https://login2.scrape.center/\u0026#34; LOGIN_URL = urljoin(BASE_URL, \u0026#34;/login\u0026#34;) INDEX_URL = urljoin(BASE_URL, \u0026#34;/page/1\u0026#34;) USERNAME = \u0026#34;admin\u0026#34; PASSWORD = \u0026#34;admin\u0026#34; data = { \u0026#39;username\u0026#39;: \u0026#39;admin\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;admin\u0026#39; } response_login = requests.post(LOGIN_URL, data=data) response_index = requests.get(INDEX_URL) print(response_index.status_code) # 200 print(response_index.url) # 该URL是需要登录的URL：https://login2.scrape.center/login?next=/page/1   这样单纯的请求的，每个get和post都是新的请求没有联系\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \u0026#34;\u0026#34;\u0026#34; 基于Session和Cookie的模拟登录实现 将第一次post请求时获取的Cookie交给第二次请求页面 \u0026#34;\u0026#34;\u0026#34; import requests from urllib.parse import urljoin BASE_URL = \u0026#34;https://login2.scrape.center/\u0026#34; LOGIN_URL = urljoin(BASE_URL, \u0026#34;/login\u0026#34;) INDEX_URL = urljoin(BASE_URL, \u0026#34;/page/1\u0026#34;) USERNAME = \u0026#34;admin\u0026#34; PASSWORD = \u0026#34;admin\u0026#34; data = { \u0026#39;username\u0026#39;: \u0026#39;admin\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;admin\u0026#39; } response_login = requests.post(LOGIN_URL, data=data, allow_redirects=False) login_cookie = response_login.cookies print(\u0026#34;login_cookie is \u0026#34;, login_cookie) #login_cookie is \u0026lt;RequestsCookieJar[\u0026lt;Cookie sessionid=d044t4gcwntzu2yz9hu48wbtmwwvz3u0 for login2.scrape.center/\u0026gt;]\u0026gt; response_index = requests.get(INDEX_URL, cookies=login_cookie) print(response_index.status_code) # 200 print(response_index.url) # https://login2.scrape.center/page/1   由于requests具有自动重定向的能力，所以要再模拟登录的过程中加入allow_redirects参数并设置成False使得requests不自动处理重定向，否则还是会模拟登录失败（该语句是response_login = requests.post(LOGIN_URL, data=data, allow_redirects=False)）\n 下面直接借助requests内置的session完成上面的繁琐步骤\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import requests from urllib.parse import urljoin BASE_URL = \u0026#34;https://login2.scrape.center/\u0026#34; LOGIN_URL = urljoin(BASE_URL, \u0026#34;/login\u0026#34;) INDEX_URL = urljoin(BASE_URL, \u0026#34;/page/1\u0026#34;) USERNAME = \u0026#34;admin\u0026#34; PASSWORD = \u0026#34;admin\u0026#34; data = { \u0026#39;username\u0026#39;: \u0026#39;admin\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;admin\u0026#39; } session = requests.Session() response_login = session.post(LOGIN_URL, data=data) cookie = session.cookies print(\u0026#34;Cookie is \u0026#34;, cookie) response_index = session.get(INDEX_URL) print(response_index.status_code) print(response_index.url)    下面使用Selenium模拟登录（在登录较为繁琐的情况下，带有验证码和加密参数）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests from urllib.parse import urljoin import time from selenium import webdriver from selenium.webdriver.common.by import By BASE_URL = \u0026#34;https://login2.scrape.center/\u0026#34; LOGIN_URL = urljoin(BASE_URL, \u0026#34;/login\u0026#34;) INDEX_URL = urljoin(BASE_URL, \u0026#34;/page/1\u0026#34;) # 使用selenium模拟登录并获取cookie信息 browser = webdriver.Chrome() browser.get(BASE_URL) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;app\u0026#34;]/div[2]/div/div/div/div/div/form/div[1]/div/div/input\u0026#39;).send_keys(\u0026#39;admin\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;app\u0026#34;]/div[2]/div/div/div/div/div/form/div[2]/div/div/input\u0026#39;).send_keys(\u0026#39;admin\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;app\u0026#34;]/div[2]/div/div/div/div/div/form/div[3]/div/input\u0026#39;).click() time.sleep(10) cookies = browser.get_cookies() print(\u0026#34;Cookie is \u0026#34;, cookies) browser.close() session = requests.Session() for cookie in cookies: session.cookies.set(name=cookie[\u0026#39;name\u0026#39;], value=cookie[\u0026#39;value\u0026#39;]) response_index = session.get(INDEX_URL) print(response_index.status_code) print(response_index.url)   基于JWT模拟登录实战 基于JWT的网站通常采用的是前后端分离式，前后端的数据交互依赖于AJAX，登录验证依赖于JWT\n从下图观察使用Ajax请求，请求体是JSON格式的数据\n接下来翻页后观察到多了一个Authorization字段翻译为批准，该字段其实是上面的Token字段内容加上jwt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import requests from urllib.parse import urljoin BASE_URL = \u0026#34;https://login3.scrape.center/\u0026#34; LOGIN_URL = urljoin(BASE_URL, \u0026#34;/api/login\u0026#34;) INDEX_URL = urljoin(BASE_URL, \u0026#34;/api/book\u0026#34;) USERNAME = \u0026#34;admin\u0026#34; PASSWORD = \u0026#34;admin\u0026#34; json = { \u0026#39;username\u0026#39; : USERNAME, \u0026#39;password\u0026#39; : PASSWORD } response_login = requests.post(url=LOGIN_URL, json=json) data = response_login.json() print(\u0026#34;data JSON is \u0026#34;, data) jwt = data.get(\u0026#39;token\u0026#39;) print(\u0026#34;JWT is \u0026#34;, jwt) headers = { \u0026#39;Authorization\u0026#39;: f\u0026#34;jwt {jwt}\u0026#34; } response_index = requests.get(url=INDEX_URL, params={\u0026#39;limit\u0026#39;:18, \u0026#39;offset\u0026#39;:0}, headers=headers) print(response_index.status_code) print(response_index.url) print(response_index.json())   输出截图如下：\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8110/","summary":"引言：模拟登录现在一般分为两种模式，一种是基于session和cookie的模拟登录，一种是基于JWT（JSON Web Token）的模拟登录。 对","title":"崔庆才爬虫书阅读摘要10"},{"content":" 引言：在执行爬虫程序的时候,有时候爬虫在最初的时候可以运行但运行一段时间后可能会出现”IP访问过于频繁“或者出现验证码让我们输入等问题。\n这些问题都是网站对爬虫采取的一种反爬虫措施，例如服务器会检测某个IP在单位时间内的请求次数，如果请求次数过多则会拒绝服务，这种手段被称为封IP。\n 常见的代理IP类型： 按匿名度分类\n按照匿名度分类可将代理IP分为高匿名代理IP、普通匿名代理IP、透明代理IP三种。高匿名代理IP是匿名度最高的代理IP，一般来说目标网站服务器无法识别出用户使用了代理；普通匿名代理IP的匿名度较低，目标网站服务器可以发现用户正在使用代理IP进行访问，往往会限制访问；透明代理IP不能匿名访问，将会直接暴露用户终端的真实IP，存在着一定的风险。\n按时效划分\n按照时效划分可将代理IP分为静态代理IP和动态代理IP。静态代理IP是固定的，动态代理IP有时效性，时效过后会失效。动态代理IP可以分为长期代理IP和短期代理IP，短期代理IP的时效一般从几秒到几分钟不等；长期代理IP的时效通常从几分钟到几天不等。\n按协议类型分类\n按照协议类型划分可将代理IP分为http(s)代理IP和socks5代理IP。当然互联网协议并不止这些，但这两者或者说三者是目前市场上较为常见的协议。http(s)代理IP同时支持http和https协议，socks5代理则只是单纯的进行数据传输，对于协议类型没有要求。\n按纯度分类\n按照纯度划分可将代理IP分为独独享IP池和共享IP池。使用代理IP的人越少，纯度越高。比如独享IP池，一个人使用的代理IP，纯度极高；共享IP池，不超过10人使用，可以算是纯度高的共享IP池；其他未解释的代理IP包通常是共享IP池。\nurllib的代理设置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  \u0026#34;\u0026#34;\u0026#34; urllib的代理设置 handler和opener \u0026#34;\u0026#34;\u0026#34; from urllib.error import URLError from urllib.request import ProxyHandler, build_opener proxy = \u0026#39;127.0.0.1:7890\u0026#39; proxy_handler = ProxyHandler({ \u0026#39;http\u0026#39;:\u0026#39;http://\u0026#39; + proxy, \u0026#39;https\u0026#39;: \u0026#39;https://\u0026#39; + proxy }) # opener = build_opener() opener = build_opener(proxy_handler) try: response = opener.open(\u0026#39;https://www.httpbin.org/get\u0026#39;) print(response.read().decode(\u0026#39;UTF-8\u0026#39;)) except URLError as e: print(e.reason)   输出结果如下：\n如果遇到需要认证的代理只需把上面的代码proxy = '127.0.0.1:7890'改写成proxy = 'username:password@127.0.0.1:7890'，username是用户名，password是密码。\n SOCKS类型的代理设置\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \u0026#34;\u0026#34;\u0026#34; 代理类型是SOCKS的如何设置 \u0026#34;\u0026#34;\u0026#34; import socks import socket from urllib import request from urllib.error import URLError socks.setdefaultproxy(socks.SOCKS5, \u0026#39;127.0.0.1\u0026#39;, 7890) socket.socket = socks.socksocket try: response = request.urlopen(\u0026#39;https://www.httpbin.org/get\u0026#39;) print(response.read().decode(\u0026#39;utf-8\u0026#39;)) except URLError as e: print(e.reason)   requests代理设置 1 2 3 4 5 6 7 8 9 10 11 12 13  import requests url = \u0026#39;https://www.httpbin.org/get\u0026#39; proxy = \u0026#39;127.0.0.1:7890\u0026#39; proxies = { \u0026#39;http\u0026#39;:\u0026#39;http://\u0026#39; + proxy, \u0026#39;https\u0026#39;:\u0026#39;http://\u0026#39; + proxy } try: response = requests.get(url=url, proxies=proxies) print(response.text) except requests.exceptions.ConnectionError as e: print(\u0026#39;代理发生错误\u0026#39;, e.args)   使用的https代理项的时候也要写成http匹配，认证代理同urllib的方法相同。\n SOCKS类型的代理设置\n 1 2 3 4 5 6 7 8 9 10 11 12 13  import requests url = \u0026#39;https://www.httpbin.org/get\u0026#39; proxy = \u0026#39;127.0.0.1:7890\u0026#39; proxies = { \u0026#39;http\u0026#39;:\u0026#39;socks5://\u0026#39; + proxy, \u0026#39;https\u0026#39;:\u0026#39;socks5://\u0026#39; + proxy } try: response = requests.get(url=url, proxies=proxies) print(response.text) except requests.exceptions.ConnectionError as e: print(\u0026#39;代理发生错误\u0026#39;, e.args)   httpx代理设置 httpx的代理设置和requests的代理设置稍有不同，在配置代理的时候将键名改为http://和https://即可\n1 2 3 4 5 6 7 8 9 10 11  import httpx url = \u0026#39;https://www.httpbin.org/get\u0026#39; proxy = \u0026#39;127.0.0.1:7890\u0026#39; proxies = { \u0026#39;http://\u0026#39;:\u0026#39;http://\u0026#39; + proxy, \u0026#39;https://\u0026#39;:\u0026#39;http://\u0026#39; + proxy } with httpx.Client(proxies=proxies) as client: response = client.get(url=url) print(response.text)    SOCKS类型的代理设置分为同步和异步代理\n 同步代理如下：\n1 2 3 4 5 6 7 8 9 10  import httpx from httpx_socks import SyncProxyTransport url = \u0026#39;https://www.httpbin.org/get\u0026#39; transport = SyncProxyTransport.from_url(\u0026#39;socks5://127.0.0.1:7890\u0026#39;) proxy = \u0026#39;127.0.0.1:7890\u0026#39; with httpx.Client(transport=transport) as client: response = client.get(url=url) print(response.text)   异步代理则使用AsyncProxyTransport即可\nselenium代理设置 1 2 3 4 5 6 7 8 9 10  from selenium import webdriver url = \u0026#34;https://www.httpbin.org/get\u0026#34; proxy = \u0026#39;127.0.0.1:7890\u0026#39; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--proxy-server=http://\u0026#34; + proxy) browser = webdriver.Chrome(options=chrome_opt) browser.get(url=url) print(browser.page_source) browser.close()   selenium的认证代理设置较为麻烦，要使用的时候网上查阅即可。\naiohttp代理设置 1 2 3 4 5 6 7 8 9 10 11 12 13  import aiohttp import asyncio url = \u0026#34;https://www.httpbin.org/get\u0026#34; proxy = \u0026#39;127.0.0.1:7890\u0026#39; async def main(): async with aiohttp.ClientSession() as session: async with session.get(url=url, proxy=proxy) as response: print(await response.text()) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   Pyppeteer代理设置 1 2 3 4 5 6 7 8 9 10 11 12 13 14  import asyncio from pyppeteer import launch url = \u0026#34;https://www.httpbin.org/get\u0026#34; proxy = \u0026#39;127.0.0.1:7890\u0026#39; async def main(): browser = await launch({\u0026#39;args\u0026#39;:[\u0026#39;--proxy-server=http://\u0026#39; + proxy], \u0026#39;headless\u0026#39;: False}) page = await browser.newPage() await page.goto(url=url) print(await page.content()) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8109/","summary":"引言：在执行爬虫程序的时候,有时候爬虫在最初的时候可以运行但运行一段时间后可能会出现”IP访问过于频繁“或者出现验证码让我们输入等问题。 这些","title":"各个请求库的代理运用崔庆才爬虫书"},{"content":" Playwright的特点 支持当前主流的浏览器并提供完善的自动化控制API\nPlaywright支持移动端页面测试，使用设备模拟技术，可以让我们在移动web浏览器中测试响应式的Web应用程序\nPlaywright支持所有浏览器的无头和由头测试，并且安装配置简单，无需额外配置浏览器驱动\nPlaywright提供和自动等待相关的API，在页面加载的时候后自动等待对应的节点加载，大大减小了API的编写复杂度\n 基本使用 Playwright支持两种编写模式，一种是类似Pyppeteer的异步模式，一种是类似Selenium的同步编写模式\n 同步模式的编写、\n 1 2 3 4 5 6 7 8 9 10 11 12  from playwright.sync_api import sync_playwright # 调用sync_playwright方法返回的是一个PlaywrightContextManager对象可以理解为一个浏览器上下文管理器 with sync_playwright() as p: # 依次调用chromium，firefox，webkit浏览器实例 for browser_type in [p.chromium, p.firefox, p.webkit]: # 接着调用launch返回一个Browser对象 browser = browser_type.launch(headless=False) page = browser.new_page() page.goto(\u0026#34;https://www.baidu.com\u0026#34;) page.screenshot(path=f\u0026#39;screenshot-{browser_type.name}.png\u0026#39;) print(page.title()) browser.close()   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8107-2/","summary":"Playwright的特点 支持当前主流的浏览器并提供完善的自动化控制API Playwright支持移动端页面测试，使用设备模拟技术，可以让我","title":"Playwright自动化-崔庆才爬虫书"},{"content":"Pyppeteer 为什么会出现Pyppeteer？因为在大规模使用Selenium时，部署的环境配置问题令人头疼（要安装对应的浏览器驱动，还要下载对应的Selenium库）\nPyppeteer的介绍：\nPuppeteer是Goole基于Node.js开发的一个工具，这个工具可以利用JavaScript控制Chrome浏览器，同样可以运用在爬虫上。 而Pyppeteer是日本人基于Puppeteer和Python开发的一个类似工具，且Pyppeteer背后的浏览器是Chromium（领先测试版Chrome）。另外，Pyppeteer基于Python的新特性async实现，一些操作支持异步的方式。\n基础用法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import asyncio from pyppeteer import launch from pyquery import PyQuery as pq async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch() # 打开一个选项卡 page = await browser.newPage() # 访问网页 await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#34; .item .name\u0026#34;) doc = pq(await page.content()) names = [items.text() for items in doc(\u0026#34;.item .name\u0026#34;).items()] print(\u0026#34;Names\u0026#34;, names) await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  \u0026#34;\u0026#34;\u0026#34; 使用Pyppeteer进行截图 \u0026#34;\u0026#34;\u0026#34; import asyncio from pyppeteer import launch width, height = 1366, 768 async def main(): browser = await launch() page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#34;.item .name\u0026#34;) await asyncio.sleep(2) await page.screenshot(path=\u0026#34;ts.png\u0026#34;) dimensions = await page.evaluate(\u0026#34;\u0026#34;\u0026#34;() =\u0026gt; { return{ width: document.documentElement.clientWidth, height: document.documentElement.clientHeight, deviceScaleFactor: window.devicePixeRatio, }} \u0026#34;\u0026#34;\u0026#34;) print(dimensions) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   上面使用的JavaScript语句中传入一个函数，使用return返回了页面的宽，高，像素大小比的三个值，最后得到的是JSON格式对象{'width': 1366, 'height': 768, 'deviceScaleFactor': 1}\nlaunch方法 使用pyppeteer第一步是启动浏览器，而启动浏览器就是使用launch方法，在观察源码后发现该方法是一个async修饰的方法所以在调用的时候要使用await。可以阅读该方法的源码非常简单。\n无头模式 只需要在创建浏览器对象的时候加入参数即可browser = await launch(headless=False)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import asyncio from pyppeteer import launch width, height = 1366, 768 async def main(): browser = await launch(headless=False) page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#34;.item .name\u0026#34;) await asyncio.sleep(2) await page.screenshot(path=\u0026#34;ts.png\u0026#34;) dimensions = await page.evaluate(\u0026#34;\u0026#34;\u0026#34;() =\u0026gt; { return{ width: document.documentElement.clientWidth, height: document.documentElement.clientHeight, deviceScaleFactor: window.devicePixeRatio, }} \u0026#34;\u0026#34;\u0026#34;) print(dimensions) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   页面大小设置 在某些网站可能会出现，浏览器窗口比页面窗口大（页面显示不完整）的情况出现，这时候我们可以设置窗口的大小，调用Page对象的setViewport方法即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) width, height = 1366, 768 page = await browser.newPage() await page.setViewport({\u0026#39;width\u0026#39;: width, \u0026#39;height\u0026#39;: height}) await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#39;.item .name\u0026#39;) await page.click(\u0026#39;.item .name\u0026#39;, options={ \u0026#39;button\u0026#39;: \u0026#39;right\u0026#39;, \u0026#39;clickCount\u0026#39;: 1, # 1 或 2 \u0026#39;delay\u0026#39;: 3000, # 毫秒 }) await asyncio.sleep(2) await page.close() await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   调试模式 再写爬虫的时候经常需要分析网页和网络请求，所以开启调试工具后比较便捷，使用browser = await launch(devtools=True)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import asyncio from pyppeteer import launch width, height = 1366, 768 async def main(): browser = await launch(devtools=True) page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#34;.item .name\u0026#34;) await asyncio.sleep(2) await page.screenshot(path=\u0026#34;ts.png\u0026#34;) dimensions = await page.evaluate(\u0026#34;\u0026#34;\u0026#34;() =\u0026gt; { return{ width: document.documentElement.clientWidth, height: document.documentElement.clientHeight, deviceScaleFactor: window.devicePixeRatio, }} \u0026#34;\u0026#34;\u0026#34;) print(dimensions) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   防止检测 类似于Selenium的防止检测出浏览器是爬虫程序模拟的浏览器使用await page.evaluateOnNewDocument('Object.defineProperty(navigator, \u0026quot;webdriver\u0026quot;, {get:()=\u0026gt;undefined})')\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import asyncio from pyppeteer import launch width, height = 1366, 768 async def main(): browser = await launch(devtools=True) page = await browser.newPage() await page.evaluateOnNewDocument(\u0026#39;Object.defineProperty(navigator, \u0026#34;webdriver\u0026#34;, {get:()=\u0026gt;undefined})\u0026#39;) await page.goto(url=\u0026#34;https://antispider1.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#34;.item .name\u0026#34;) await asyncio.sleep(2) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   还可以在launch中加入browser = await launch(headless=False, args=['--disable-infobars'])关闭显示在上面的控制词条\n用户数据的持久化 有时候在登陆了某些网站后想保留一些Cookie数据或者历史记录以及状态信息时，只需要设置userDataDir属性就好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import asyncio from pyppeteer import launch width, height = 1366, 768 async def main(): browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;], userDataDir=\u0026#39;./userdata\u0026#39;) page = await browser.newPage() await page.evaluateOnNewDocument(\u0026#39;Object.defineProperty(navigator, \u0026#34;webdriver\u0026#34;, {get:()=\u0026gt;undefined})\u0026#39;) await page.goto(url=\u0026#34;https://www.taobao.com\u0026#34;) await asyncio.sleep(100) await browser.close() if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   Browser 上面的launch方法，它的返回值是一个Browser对象，即浏览器对象\n开启无痕模式 无痕模式的开启通过createIncognitoBrowserContext()方法开启\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-inforbars\u0026#39;]) context = await browser.createIncognitoBrowserContext() page = await context.newPage() await page.goto(url=\u0026#34;https://www.baidu.com\u0026#34;) await asyncio.sleep(5) await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   选择器 Page对象内置了很多选取节点的选择器，例如J方法等价于querySelector方法。还有JJ方法相当于querySelectorAll方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-inforbars\u0026#39;]) page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#39;.item .name\u0026#39;) j_result1 = await page.J(\u0026#39;.item .name\u0026#39;) j_result2 = await page.querySelector(\u0026#39;.item .name\u0026#39;) print(j_result1, j_result2) await asyncio.sleep(5) await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   选项卡操作 在打开的不同选项卡中，先使用pages获取所有打开的页面，然后再选择一个页面调用其bringToFront方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) # 先后打开两个选项卡（网页） page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) page = await browser.newPage() await page.goto(url=\u0026#34;https://www.baidu.com\u0026#34;) pages = await browser.pages() page1 = pages[1] await page1.bringToFront() await asyncio.sleep(2) await browser.close() print(\u0026#34;OVER\u0026#34;)   页面操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) # 先后打开两个选项卡（网页） page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.goto(url=\u0026#34;https://www.baidu.com\u0026#34;) # 后退 await page.goBack() # 前进 await page.goForward() # 刷新 await page.reload() # 保存PDF await page.pdf() # 截图 await page.screenshot() # 设置页面HTML await page.setContent(\u0026#39;\u0026lt;h2\u0026gt;hello\u0026lt;/h2\u0026gt;\u0026#39;) # 设置UA await page.setUserAgent(\u0026#39;python\u0026#39;) # 设置headers await page.setExtraHTTPHeaders(headers={}) await asyncio.sleep(2) await page.close() await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   点击 调用click方法即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) # 先后打开两个选项卡（网页） page = await browser.newPage() await page.goto(url=\u0026#34;https://spa2.scrape.center/\u0026#34;) await page.waitForSelector(\u0026#39;.item .name\u0026#39;) await page.click(\u0026#39;.item .name\u0026#39;, options={ \u0026#39;button\u0026#39;: \u0026#39;right\u0026#39;, \u0026#39;clickCount\u0026#39;: 1, # 1 或 2 \u0026#39;delay\u0026#39;: 3000, # 毫秒 }) await asyncio.sleep(2) await page.close() await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())    button：鼠标按钮，取值有left、middle、right clickCount：点击次数，取值1和2 delay：延迟点击  输入文本 使用type键入1.要传入参数的选择器2.要传入的文本内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) # 先后打开两个选项卡（网页） page = await browser.newPage() await page.goto(url=\u0026#34;https://www.taobao.com\u0026#34;) await page.type(\u0026#39;#q\u0026#39;, \u0026#39;python\u0026#39;) await asyncio.sleep(2) await page.close() await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   获取信息 使用content等，使用Cookies获取Cookie\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import asyncio from pyppeteer import launch async def main(): # 创建了一个Browser对象，相当于启动了浏览器 browser = await launch(headless=False, args=[\u0026#39;--disable-infobars\u0026#39;]) # 先后打开两个选项卡（网页） page = await browser.newPage() await page.goto(url=\u0026#34;https://www.taobao.com\u0026#34;) print(\u0026#34;content\u0026#34;, await page.content()) print(\u0026#34;Cookie\u0026#34;, await page.cookies()) await asyncio.sleep(2) await page.close() await browser.close() print(\u0026#34;OVER\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main())   执行js语句 执行js语句通过使用evaluate执行\n延时等待 除了上面一贯的用例的等待方法，还有许多的等待方法，具体可以查看官方文档\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8107/","summary":"Pyppeteer 为什么会出现Pyppeteer？因为在大规模使用Selenium时，部署的环境配置问题令人头疼（要安装对应的浏览器驱动，还要下载对应的Se","title":"Pyppeteer的运用-崔庆才爬虫书"},{"content":"协程的用法 对于asyncio了解以下几个概念：\n event_loop：事件循环，相当于无限循环，把一些函数注册到事件循环上，当满足发生条件的时候，就调用注册的方法 coroutine：协程，在Python中代指协程类型的对象，我们可以将协程对象注册到事件循环中，会被事件循环调用。可以使用async关键字来定义一个方法，该方法在被调用时会返回一个协程对象 task：任务，是对协程对象的进一步封装，包含各个协程对象的状态 future：代表将来执行或没有执行的任务的结果，实际上和task没有本质区别  定义了协程函数async def func():后如果执行该协程函数后只是返回一个协程对象\n基本定义 注意：之前写过的爬虫文章里面使用的是asyncio.run方法是对asyncio.get_event_loop和asyncio.run_until_complete的封装，具体可参见库文件。asyncio.run方法是Python3.7之后新增的方法。 asyncio.run方法不需要显示声明事件循环，run方法内部会自动开启一个事件循环。asyncio.run()相当于asyncio.get_event_loop()和asyncio.run_until_complete()这两步。\n1 2 3 4 5 6 7 8 9 10 11 12 13  import asyncio async def exec(i): print(f\u0026#34;exec func{i}\u0026#34;) execute = exec(5) print(\u0026#34;first print\u0026#34;, execute) print(\u0026#34;___________________\u0026#34;) # 创建一个事件循环 loop = asyncio.get_event_loop() # 将协程对象注册到事件循环中，接着启动 loop.run_until_complete(execute) print(\u0026#34;OK\u0026#34;)   上述代码将execute这个协程对象传递给get_event_loop时的时候其实是隐式地将该协程对象封装成了task对象，下面我们也可也显示声明从而获得该协程对象当时的状态。\n协程对象封装成task显示状态 显示声明task对象方法1：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import asyncio async def exec(i): print(f\u0026#34;exec func{i}\u0026#34;) execute = exec(5) print(\u0026#34;first print\u0026#34;, execute) print(\u0026#34;___________________\u0026#34;) loop = asyncio.get_event_loop() task = loop.create_task(execute) print(\u0026#34;before run status\u0026#34;, task) # 简略输出：before run status \u0026lt;Task pending name=\u0026#39;Task-1\u0026#39;... loop.run_until_complete(task) print(\u0026#34;after run status\u0026#34;, task)\t# 简略输出：after run status \u0026lt;Task finished name=\u0026#39;Task-1\u0026#39;... print(\u0026#34;OK\u0026#34;)   显式声明task对象方法2： 使用ensure_future方法，返回的结果也是task对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import asyncio async def exec(i): print(f\u0026#34;exec func{i}\u0026#34;) execute = exec(5) print(\u0026#34;first print\u0026#34;, execute) print(\u0026#34;___________________\u0026#34;) task = asyncio.ensure_future(execute) print(\u0026#34;before run status\u0026#34;, task) loop = asyncio.get_event_loop() loop.run_until_complete(task) print(\u0026#34;after run status\u0026#34;, task) print(\u0026#34;OK\u0026#34;)    下面示例两组代码，一组单纯地使用爬虫，一组使用异步协程\n 单纯爬取（requests同步请求）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import requests import logging import time \u0026#34;\u0026#34;\u0026#34; 观察响应比较久的网站的爬取 \u0026#34;\u0026#34;\u0026#34; logging.basicConfig(level=logging.INFO, format=\u0026#34;%(asctime)s- %(levelname)s: %(message)s\u0026#34;) TOTAL_NUMBER = 3 URL = \u0026#34;https://www.httpbin.org/delay/5\u0026#34; start_time = time.time() for i in range(1, TOTAL_NUMBER + 1): logging.info(f\u0026#34;scraping {URL}\u0026#34;) response = requests.get(url=URL) end_time = time.time() logging.info(f\u0026#34;It\u0026#39;s spend time {end_time - start_time}\u0026#34;)    协程爬取（aiohttp异步请求）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import asyncio import time import aiohttp start_time = time.time() async def get_resp_text(url): async with aiohttp.ClientSession() as session: response = await session.get(url=url) await response.text() return response async def request(): url = \u0026#34;https://www.httpbin.org/delay/5\u0026#34; print(f\u0026#34;wait for {url}\u0026#34;) response = await get_resp_text(url) print(f\u0026#34;GET from {response}\u0026#34;) task = [asyncio.ensure_future(request()) for _ in range(3)] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(task)) end_time = time.time() print(f\u0026#34;spending time is {end_time - start_time}\u0026#34;)   可以自行运行代码后观察差异。\naiohttp的详细说明  aiohttp基本上都和asyncio联动一起使用，因为要使用异步爬取就需要协程的参与，而协程就要借助asyncio的事件循环才能执行 异步方法前面都要添加async来修饰 使用with as上下文管理器时，前面也要使用async，代表声明了一个支持异步上下文的管理器 对于一些返回协程对象的操作，前面需要加await来修饰  并发限制 由于aiohttp可以支持非常高的并发量，面对这么高的并发量，目标网站可能无法在短时间内响应，可能会瞬间将目标网站爬挂掉，所以我们可以限制一下并发量\n第六章项目实战 完整的项目应该是能充分利用资源进行爬取，实现思路是：维护一个动态变化的爬取队列，每产生一个新的task就把它放入队列中，有专门的爬虫消费者从队列中获取task并执行，能够做到最大并发的前提下充分利用等待时间进行额外爬取。但本项目只重在实现aiohttp的实战。\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8106/","summary":"协程的用法 对于asyncio了解以下几个概念： event_loop：事件循环，相当于无限循环，把一些函数注册到事件循环上，当满足发生条件的时","title":"爬虫的异步协程运用崔庆才爬虫书"},{"content":"RabbitMQ  在爬取数据的时候有可能需要一些进程间的通信机制，如：\n一个进程负责构造爬取请求，另一个进程负责爬取请求\n一个进程爬取数据完毕，通知另一进程处理数据\n 为了降低进程间的耦合度，使用消息中间件来存储和转发消息，实现进程间的通信\n基本使用 RabbitMQ就是一个消息队列，要实现进程间的通信问题，本质上就是生产者-消费者模型，进程1生产者将消息放入消息队列，进程2消费者监听并处理消息队列中的消息。需要关注的点：\n 声明队列：指定参数创建消息队列 生产内容：生产者根据队列的连接信息连接队列，往队列中放入消息 消费内容：消费者根据队列的连接信息连接队列，往队列中取出消息  基础示例  生产者\n 1 2 3 4 5 6 7 8 9 10 11 12  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列 channel.queue_declare(queue=QUEUE_NAME) channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, body=b\u0026#34;123\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) channel = connection.channel() channel.queue_declare(queue=QUEUE_NAME) def callback(ch, method, properties, body): print(f\u0026#34;GET {body}\u0026#34;) channel.basic_consume(queue=\u0026#34;spiders\u0026#34;, auto_ack=True, on_message_callback=callback) channel.start_consuming()   随取随用 使生产者可以自行控制的将消息放入队列，消费者也可以根据自己的能力来获得并处理数据\n 生产者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列 channel.queue_declare(queue=QUEUE_NAME) while True: data = input() channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, body=data.encode()) print(f\u0026#34;Put data is {data}\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) channel = connection.channel() while True: input() method_frame, header, body = channel.basic_get( queue=QUEUE_NAME, auto_ack=True ) if body: print(f\u0026#34;Get data {body}\u0026#34;)   优先级队列 只需要在声明队列的时候加上x-max-priority参数来指定最大优先级\n队列持久化 只需要在声明队列的时候加上durable=True参数来开启持久化存储，同时在添加消息的时候要指定pika.BasicProperties对象的delivery_mode=2\n简易运用  生产者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import pika import requests import pickle QUEUE_NAME = \u0026#34;spider2\u0026#34; MAX_PRIORITY = 100 # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列，并且开启持久化存储 channel.queue_declare(queue=QUEUE_NAME, durable=True) for i in range(1, 10): url = f\u0026#34;https://scrape.center/detail/{i}\u0026#34; req = requests.Request(\u0026#39;GET\u0026#39;, url) channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, properties=pika.BasicProperties(delivery_mode=2), body=pickle.dumps(req)) print(f\u0026#34;Put url is {url}\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pika import requests import pickle QUEUE_NAME = \u0026#34;spider2\u0026#34; MAX_PRIORITY = 100 # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() session = requests.session() def scrape(request): try: response = session.send(request.prepare()) print(f\u0026#34;success {response.url}\u0026#34;) except Exception as e: print(f\u0026#34;Exception if {e}\u0026#34;) while True: method_frame, header, body = channel.basic_get( queue=QUEUE_NAME, auto_ack=True ) if body: request = pickle.loads(body) print(f\u0026#34;GET {body}\u0026#34;) scrape(request)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103-3/","summary":"RabbitMQ 在爬取数据的时候有可能需要一些进程间的通信机制，如： 一个进程负责构造爬取请求，另一个进程负责爬取请求 一个进程爬取数据完毕，通知另一进程处理","title":"爬虫保存到数据库(RabbitMQ)的运用崔庆才爬虫书"},{"content":"Redis缓存存储 Redis是一个基于内存的、高效的键值型非关系型数据库\n可以在下面阅读\n redis官方网站 redis中文官网  连接Redis 1 2 3 4 5 6  from redis import StrictRedis redis = StrictRedis(host=\u0026#34;localhost\u0026#34;, port=6379, db=0, password=\u0026#34;winhansonserver\u0026#34;) print(redis.get(\u0026#34;a1\u0026#34;)) redis.close()   下面还可以使用ConnectionPool，来连接Redis\n1 2 3 4 5 6 7  import redis from redis import StrictRedis pool = redis.ConnectionPool(host=\u0026#34;localhost\u0026#34;, db=0, password=\u0026#34;winhansonserver\u0026#34;) redis = StrictRedis(connection_pool=pool) print(redis.get(\u0026#34;a1\u0026#34;))   通过源码发现，StrictRedis内就是用参数来构造了一个ConnectionPool。\n并且ConnectionPool还支持通过URL来构建连接。支持的格式有：\n Redis TCP连接  redis://[:password]@host:port/db redis://:winhansonserver@localhost:6379/0   Redis TCP+SSL连接  rediss://[:password]@host:port/db   Redis UNIX socket连接  unix://[:password]@/path/to/socket.sock?db=db    1 2 3 4 5 6 7 8  import redis from redis import StrictRedis url = \u0026#34;redis://:winhansonserver@localhost:6379/0\u0026#34; pool = redis.ConnectionPool.from_url(url) redis = StrictRedis(connection_pool=pool) print(redis.get(\u0026#34;a1\u0026#34;))   在自己使用时可以自行查询的方法 键操作 列表操作 集合操作 集合的元素都是不重复的\n有序集合操作 有序集合比集合多了一个分数字段，利用该字段可以对集合中的数据进行排序。\n散列操作 总结 Redis的便携以及高效，后面我们会使用Redis实现，维护代理池，账号池，ADSL拨号代理池，Scrapy-Redis分布式架构等。\nElasticsearch搜索引擎存储 Elasticsearch是一个开源的全文搜索引擎。可以实现自己的搜索引擎，是一个可以快速存储、搜索和分析海量数据的全文搜索引擎。\nElasticsearch是对Lucene的封装，并且是：\n 一个分布式的实时文档存储库，每个字段都可以被索引和搜索 一个分布式的实时搜索引擎 可以被上百个服务节点拓展，并支持PB级别的结构化和非结构化数据   Elasticsearch相关概念\n  节点和集群  Elasticsearch本质是一个分布式数据库，允许多个服务器协同工作，每台服务器可以运行一个Elasticsearch实列，单个Elasticsearch实列被称为一个节点，多个节点构成一个集群   索引  index，Elasticsearch会索引所有字段，处理后会写入一个反向索引。查询数据的时候直接查找该索引。Elasticsearch的顶层单位就是索引，相当于MySQL和MongoDB等的数据库的概念。注意：索引的名字必须小写   文档  索引里面的单条记录就是文档，许多文档构成索引，最好保持文档的数据结构相同便于搜索的效率   类型  文档可以分组，例如城市的分组，汽车品牌的分组，这种分组就叫做类型，，它是虚拟逻辑的分组，类似于MySQL中的数据表，MongoDB中的集合（Elastic6允许每个索引包含一个类型，在版本7将会开始移除类型）   字段  每个文档都有类似的JSON结构，包含许多字段，每个字段都有对应的值，多个字段组成了文档。类似于MySQL数据表中的字段   es8.X彻底删除了type，es全部是JSON   传统关系型数据库-》database\u0026mdash;\u0026mdash;》Tables\u0026mdash;》Rows \u0026mdash;\u0026mdash;-》Colums\nElasticsearch\u0026mdash;\u0026ndash;》indices（索引）-》Type \u0026mdash;-》Document -》Fields\n 创建索引（相当于数据库） 1 2 3 4 5 6 7 8  from elasticsearch import Elasticsearch # 创建了一个Elasticsearch对象传入参数是连接Elasticsearch的连接 es = Elasticsearch(\u0026#34;http://127.0.0.1:9200/\u0026#34;) # 下面的ignore=400表示如果返回的是400的话忽略这个错误 result = es.indices.create(index=\u0026#34;news\u0026#34;, ignore=400) print(result)\t# 返回的是JSON格式   1 2 3 4 5 6 7 8 9  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # 创建索引（数据库名） result = es.indices.create(index=\u0026#34;news3\u0026#34;) print(result) # output：{\u0026#39;acknowledged\u0026#39;: True, \u0026#39;shards_acknowledged\u0026#39;: True, \u0026#39;index\u0026#39;: \u0026#39;news3\u0026#39;}   删除索引 1 2 3 4 5 6 7 8 9  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # 创建索引（数据库名） result = es.indices.delete(index=\u0026#34;news3\u0026#34;) print(result) # output：{\u0026#39;acknowledged\u0026#39;: True}   插入数据 插入数据有两种方式，index和create方法，这两种方法的区别是使用index时可以不指定id它会自动生成。 create方法内部其实是调用了index方法，是对index方法的封装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} print(result1) result2 = es.index(index=\u0026#34;news3\u0026#34;, document={\u0026#34;title2\u0026#34;:\u0026#34;hello world\u0026#34;}) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;hXNtqYIB7TtztTgDmn0v\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 1, \u0026#39;_primary_term\u0026#39;: 1} print(result2)   更新数据 更新需要指定id和内容。除了使用update更新外还可以使用index更新数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} # print(result1) result = es.update(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, doc={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛很漂亮啊\u0026#34;}) print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 2, \u0026#39;result\u0026#39;: \u0026#39;updated\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 2, \u0026#39;_primary_term\u0026#39;: 1}   更新数据后输出结果还多了一个_version:2每次更新数据都会更新版本号。\n删除数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} # print(result1) # result = es.update(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, doc={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛很漂亮啊\u0026#34;}) # print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 2, \u0026#39;result\u0026#39;: \u0026#39;updated\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 2, \u0026#39;_primary_term\u0026#39;: 1} result = es.delete(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;) print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 3, \u0026#39;result\u0026#39;: \u0026#39;deleted\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 3, \u0026#39;_primary_term\u0026#39;: 1}   删除数据后_version的版本再次增加①。\n查询数据 由于查询数据的方法多种多样请参考es查询语法参考。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) properties = { \u0026#34;title\u0026#34;: {\u0026#39;type\u0026#39;: \u0026#39;text\u0026#39;} } # es.indices.delete(index=\u0026#34;news\u0026#34;) result = es.indices.put_mapping(index=\u0026#39;news3\u0026#39;, properties=properties) print(result) # datas = [ # { # \u0026#34;title\u0026#34;: \u0026#34;人之初性本善\u0026#34;, # }, # { # \u0026#34;title\u0026#34;: \u0026#34;中国越发强大，深感骄傲\u0026#34; # }, # { # \u0026#34;title\u0026#34;: \u0026#34;义不容辞\u0026#34; # } # ] # for data in datas: # es.index(index=\u0026#34;news3\u0026#34;, document=data) # # result = es.search(index=\u0026#34;news3\u0026#34;) # print(result) query = { \u0026#34;match\u0026#34;:{ \u0026#34;title\u0026#34;:\u0026#34;骄傲\u0026#34; } } result = es.search(index=\u0026#34;news3\u0026#34;, query=query) print(result) {\u0026#39;acknowledged\u0026#39;: True} # {\u0026#39;took\u0026#39;: 6, \u0026#39;timed_out\u0026#39;: False, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 1, \u0026#39;successful\u0026#39;: 1, \u0026#39;skipped\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0}, \u0026#39;hits\u0026#39;: {\u0026#39;total\u0026#39;: {\u0026#39;value\u0026#39;: 1, \u0026#39;relation\u0026#39;: \u0026#39;eq\u0026#39;}, \u0026#39;max_score\u0026#39;: 1.6285465, \u0026#39;hits\u0026#39;: [{\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;iHOLqYIB7TtztTgDHH20\u0026#39;, \u0026#39;_score\u0026#39;: 1.6285465, \u0026#39;_source\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;中国越发强大，深感骄傲\u0026#39;}}]}}   插眼：由于7.X版本和8.X版本方法变动太大，后续深入再记录学习\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103-2/","summary":"Redis缓存存储 Redis是一个基于内存的、高效的键值型非关系型数据库 可以在下面阅读 redis官方网站 redis中文官网 连接Redis 1 2","title":"爬虫保存到数据库(Redis、elasticsearch)的运用崔庆才爬虫书"},{"content":"MySQL(关系型数据库) pymysql默认会帮你自动开启事务，所以无需手动begin开启\nMySQL的学习将以代码的形式来展开，并辅以少量文字解释。\nsql语句使用格式化符%s来构造，避免使用+的拼接字符串、对于数据的插入、更新、删除操作都需要db的commit方法才行，如果执行失败则使用rollback执行回调。标准写法如下：\n1 2 3 4 5  try: cursor.execute(sql, (带传入的值)) db.commit() except: db.rollback()   下面给出所使用的数据表的生成代码：\n1 2 3 4 5 6  CREATETABLE`students`(`id`varchar(255)NOTNULL,`name`varchar(255)NOTNULL,`age`int(11)NOTNULL,PRIMARYKEY(`id`))ENGINE=InnoDBDEFAULTCHARSET=utf8mb4;  插入语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import pymysql person1 = {\u0026#34;id\u0026#34;:\u0026#34;2022812\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;Jhon\u0026#34;, \u0026#34;age\u0026#34;:18} # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = \u0026#34;insert into students(id, name, age) values(%s, %s, %s)\u0026#34; try: cursor.execute(sql, (person1[\u0026#34;id\u0026#34;], person1[\u0026#34;name\u0026#34;], person1[\u0026#34;age\u0026#34;])) db.commit() except: db.rollback() db.close()   对上述代码进行优化如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import pymysql data = {\u0026#34;id\u0026#34;:\u0026#34;202281203\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;Saliy\u0026#34;, \u0026#34;age\u0026#34;:18} table = \u0026#34;students\u0026#34; keys = \u0026#34;, \u0026#34;.join(data.keys())\t# output：\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34; values = \u0026#34;, \u0026#34;.join([\u0026#39;%s\u0026#39;] * len(data)) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;insert into {table}({keys}) values({values})\u0026#34; try: cursor.execute(sql, tuple(data.values())) print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   实现了动态SQL语句的实现，sql中的列名就是通过传入数据字典的键名来表示，excute方法的第一个参数是sql变量，第二个参数是传入data的值所构造的元组实现。\n更新语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import pymysql data = {\u0026#34;id\u0026#34;:\u0026#34;202281204\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;, \u0026#34;age\u0026#34;:118} table = \u0026#34;students\u0026#34; keys = \u0026#34;, \u0026#34;.join(data.keys()) values = \u0026#34;, \u0026#34;.join([\u0026#39;%s\u0026#39;] * len(data)) # print(values) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() # ON DUPLICATE KEY UPDATE是如果主键已经存在就会执行更新，否则就是插入 sql = f\u0026#34;insert into {table}({keys}) values ({values}) ON DUPLICATE KEY UPDATE \u0026#34; update = \u0026#34;,\u0026#34;.join([\u0026#34;{key}= %s\u0026#34;.format(key=key) for key in data]) sql += update try: if cursor.execute(sql, tuple(data.values())*2): print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   删除语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pymysql condition = \u0026#34;age \u0026gt; 100\u0026#34; table = \u0026#34;students\u0026#34; # print(values) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;delete from {table}where {condition}\u0026#34; try: if cursor.execute(sql): print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   查询语句 fetchone()返回的数据类型是元组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pymysql table = \u0026#34;students\u0026#34; condition = \u0026#34;age \u0026lt; 100\u0026#34; # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;select * from {table}where {condition}\u0026#34; try: cursor.execute(sql) print(\u0026#34;查询的数据条数为：\u0026#34;, cursor.rowcount) one = cursor.fetchone() print(\u0026#34;第一条：\u0026#34;, one) alldata = cursor.fetchall() print(alldata) print(\u0026#34;--------\u0026#34;) for i in alldata: print(i) except: print(\u0026#34;ERROR\u0026#34;) db.close()    查询语句比较推荐下面的写法可以减小开销（当数据体量很大时）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import pymysql table = \u0026#34;students\u0026#34; condition = \u0026#34;age \u0026lt; 100\u0026#34; # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;select * from {table}where {condition}\u0026#34; try: cursor.execute(sql) print(\u0026#34;查询的数据条数为：\u0026#34;, cursor.rowcount) one = cursor.fetchone() while one: print(\u0026#34;Row\u0026#34;, one) one = cursor.fetchone() except: print(\u0026#34;ERROR\u0026#34;) db.close()   MongoDB MongoDB中的集合类似于关系数据库中的表。 可能由于更新后某些语法失效，具体可以查看官方文档\n插入语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import pymongo # 创建mongodb连接对象 client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) # client = pymongo.MongoClient(\u0026#39;mongodb://localhost:27017/\u0026#39;) 该语句相当于上面的语句作用相同 # 连接数据库 db = client[\u0026#34;test\u0026#34;] # db = client.test 作用与上面的语句相同 # 指定集合（相当于关系数据库里面的表） collection = db[\u0026#34;students\u0026#34;] # collection = db.students student1 = { \u0026#34;id\u0026#34;:202281301, \u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;, \u0026#34;age\u0026#34;:18, \u0026#34;gender\u0026#34;:\u0026#34;man\u0026#34; } result = collection.insert_one(student1) print(result) print(result.inserted_id)   在MongoDB中每条数据都有一个_id属性作为唯一标识，如果没有显式指明该属性，那么MongoDB会自动产生一个object类型的_id属性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import pymongo # 创建mongodb连接对象 client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) # client = pymongo.MongoClient(\u0026#39;mongodb://localhost:27017/\u0026#39;) 该语句相当于上面的语句作用相同 # 连接数据库 db = client[\u0026#34;test\u0026#34;] # db = client.test 作用与上面的语句相同 # 指定集合（相当于关系数据库里面的表） collection = db[\u0026#34;students\u0026#34;] # collection = db.students student1 = { \u0026#34;id\u0026#34;:202281302, \u0026#34;name\u0026#34;:\u0026#34;jocker\u0026#34;, \u0026#34;age\u0026#34;:22, \u0026#34;gender\u0026#34;:\u0026#34;man\u0026#34; } student2 = { \u0026#34;id\u0026#34;:202281303, \u0026#34;name\u0026#34;:\u0026#34;sarah\u0026#34;, \u0026#34;age\u0026#34;:23, \u0026#34;gender\u0026#34;:\u0026#34;woman\u0026#34; } result = collection.insert_many([student1, student2]) print(result)   查询语句 1 2 3 4 5 6 7 8 9 10  import pymongo client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] result = collection.find_one({\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;}) print(result) # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;), \u0026#39;id\u0026#39;: 202281301, \u0026#39;name\u0026#39;: \u0026#39;tom\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;gender\u0026#39;: \u0026#39;man\u0026#39;} print(result[\u0026#34;id\u0026#34;])   还可以使用_id属性来查询不过需要使用bson库里面的Objectid：\n1 2 3 4 5 6 7 8 9 10  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] result = collection.find_one({\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;)}) print(result) # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;), \u0026#39;id\u0026#39;: 202281301, \u0026#39;name\u0026#39;: \u0026#39;tom\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;gender\u0026#39;: \u0026#39;man\u0026#39;} print(result[\u0026#34;id\u0026#34;])      符号 含义 例子     $lt 小于（less than） result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$lt\u0026quot;:50}})   $gt 大于（greater than） result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$gt\u0026quot;:50}})   $lte 小于等于 同上   $gte 大于等于 同上   $ne 不等于（not equal to） 同上   $in 在范围内 result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$in\u0026quot;:[18, 23]}})   $nin 不在范围内 同上   $regex 匹配正则表达式 语法同上   $exists 属性是否存在 同上   $text 文本查询 {\u0026quot;$text\u0026quot;:{\u0026quot;$search\u0026quot;:\u0026ldquo;hanson\u0026rdquo;}}    等等还有需要功能符号方法\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 查询多个语句相当于一个生成器 result = collection.find({\u0026#34;age\u0026#34;:{\u0026#34;$in\u0026#34;:[18, 23]}}) for i in result: print(i[\u0026#34;name\u0026#34;])   计数 统计查询结果包含多少条数据使用count方法\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 查询多个语句相当于一个生成器 result = collection.estimated_document_count() result1 = collection.count_documents({\u0026#34;age\u0026#34;:{\u0026#34;$lt\u0026#34;:22}}) print(result1) print(result)   偏移 可能只想获取某几个元素就是用skip方法，还可以使用limit指定想要的结果个数\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 表示忽略前两个元素获得第三个及之后 result = collection.find().sort(\u0026#34;name\u0026#34;,pymongo.ASCENDING).skip(2) # print(result) for i in result: print(i[\u0026#34;name\u0026#34;])   更新 upsert=True这个参数可以实现，存在就更新，不存在则插入的功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] condition = {\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;} # 查询要更新的数据 student = collection.find_one(condition) # 修改数据 student[\u0026#34;age\u0026#34;] = 100 # 调用update_one方法将 原条件 和 修改后的数据传入 result = collection.update_one(condition, {\u0026#34;$set\u0026#34;:student}, upsert=True) print(result)   更新后的结果可以调用matched_count和modified_count属性获得匹配的数据条数和影响的数据条数\n并且更新推荐使用{\u0026quot;$set\u0026quot;:student}的更新方式，因为这样可以只更新student字典内存在的字段，如果原先还有其他字段，则既不会更新，也不会删除。而如果不使用$set，就会把之前的数据全部使用student字典替换，要是原本存在其他字段，则会被删除。\n删除 1 2 3 4 5 6 7 8 9 10  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] condition = {\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;} result = collection.delete_one(condition) print(result) print(result.deleted_count)   MongoDB除了提供上面的方法还提供有一些组合方法例如find_one_and_delete和find_one_and_update\nsqlalchemy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115  from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import create_engine, Column, Integer, String from sqlalchemy.orm import sessionmaker from sqlalchemy import or_ \u0026#34;\u0026#34;\u0026#34; declarative_base：进行orm映射的类必须继承该类 Column：对当前表中的字段进行映射 Integer，String：对应数据库中的字段类型 sessionmaker：会话保持，保证当前连接的有效性 \u0026#34;\u0026#34;\u0026#34; HOST = \u0026#39;localhost\u0026#39; PORT = 3306 USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB = \u0026#39;gdhy\u0026#39; DB_URI = f\u0026#34;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DB}\u0026#34; engine = create_engine(DB_URI) Base = declarative_base(engine) session = sessionmaker(engine)() class UserInfo(Base): __tablename__ = \u0026#34;user_info\u0026#34; id = Column(Integer, primary_key=True, autoincrement=True) username = Column(String(10)) age = Column(Integer, default=0) address = Column(String(10)) Base.metadata.create_all() # 新增数据 info = UserInfo(username=\u0026#34;hanson\u0026#34;, age=22, address=\u0026#34;东莞\u0026#34;) # session.add(info) # session.commit() # 新增多条数据 # session.add_all([ # UserInfo(username=\u0026#34;周杰伦\u0026#34;, age=66, address=\u0026#34;东莞\u0026#34;), # UserInfo(username=\u0026#34;王菲\u0026#34;, age=55, address=\u0026#34;网上\u0026#34;), # UserInfo(username=\u0026#34;蔡徐坤\u0026#34;, age=44, address=\u0026#34;心里\u0026#34;) # ]) # # session.commit() # 查询数据 info_list = session.query(UserInfo).all() # print(info_list) for item in info_list: print(item.username, item.age) print(\u0026#34;===================================\u0026#34;) # 查询指定列 name_list = session.query(UserInfo.username).all() print(name_list) # [(\u0026#39;hanson\u0026#39;,), (\u0026#39;周杰伦\u0026#39;,), (\u0026#39;王菲\u0026#39;,), (\u0026#39;蔡徐坤\u0026#39;,)] # 获取返回数据的第一行 name_list = session.query(UserInfo.username).first() print(name_list) # filter()方法进行条件筛选过滤 item = session.query(UserInfo.username).filter(UserInfo.age \u0026lt; 50).all() print(item) # 排序 desc_list = session.query(UserInfo.username).order_by(UserInfo.id.desc()).all() print(desc_list) # 多条件查询，使用逗号分隔 item = session.query(UserInfo.username).filter(UserInfo.age \u0026lt; 50, UserInfo.username ==\u0026#34;hanson\u0026#34;).all() print(item) # 或查询 item = session.query(UserInfo.username).filter(or_(UserInfo.age \u0026lt; 50, UserInfo.username ==\u0026#34;hanson\u0026#34;)).all() print(item) # like item_list = session.query(UserInfo.username, UserInfo.age, UserInfo.address).filter( UserInfo.username.like(\u0026#39;周%\u0026#39;) ).all() print(item_list) # in item_list = session.query(UserInfo.username, UserInfo.age, UserInfo.address).filter( UserInfo.age.in_([16, 58]) ).all() print(item_list) # 聚合函数 count = session.query(UserInfo).count() print(count) # 切片 item_list = session.query(UserInfo.username).all()[:2] print(item_list) # 删除数据 # 删除名称为夏洛的数据 session.query(UserInfo).filter(UserInfo.username == \u0026#39;蔡徐坤\u0026#39;).delete() session.commit() item_list = session.query(UserInfo.username, UserInfo.age, UserInfo.address).all() print(item_list) # 数据修改 session.query(UserInfo).filter(UserInfo.username == \u0026#34;王菲\u0026#34;).update({\u0026#34;age\u0026#34;: 22}) session.commit() info_list = session.query(UserInfo).all() # print(info_list) for item in info_list: print(item.username, item.age) print(\u0026#34;===================================\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103/","summary":"MySQL(关系型数据库) pymysql默认会帮你自动开启事务，所以无需手动begin开启 MySQL的学习将以代码的形式来展开，并辅以少量文","title":"爬虫保存到数据库(Mysql、MongoDB)的运用崔庆才爬虫书"},{"content":"对爬虫书的第三章摘录 xpath etree.parse()解析本地文件 etree.HTML()解析服务器响应文件\n注意：\n 使用xpath help插件可能会出现解析前后不对的情况，因为在浏览器的解析，xpath解析的是渲染后网页可能与网页源代码有出入 通过xpath获取的值的返回是一个列表  xpath获取属性的值 直接找到对应的元素后使用/@属性名即可获得\nxpath属性多值的匹配 当某个节点的某个属性拥有多个值的时候该如何匹配，使用contains方法\n1 2 3 4 5 6 7 8 9 10 11  from lxml import etree tree = etree.parse(\u0026#34;../../ts.html\u0026#34;) alls = tree.xpath(\u0026#34;/html/body/ol/li/a[@href=\u0026#39;http://www.baidu.com\u0026#39;]/text()\u0026#34;) print(alls) # 下面两个语句均可打印 # alls2 = tree.xpath(\u0026#34;//li/a[contains(@href, \u0026#39;666\u0026#39;)]/text()\u0026#34;) alls2 = tree.xpath(\u0026#34;//li/a[contains(@href, \u0026#39;http://www.baidu.com\u0026#39;)]/text()\u0026#34;) print(alls2)   上面使用contains方法给第一个参数传入属性名称，第二个参数传入属性值（属性里面包含的值）。\nxpath的多属性匹配 1 2 3 4 5 6 7 8 9 10 11 12  from lxml import etree html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com 666\u0026#34; id=\u0026#34;baidu\u0026#34;\u0026gt;百度1\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com 666\u0026#34; id=\u0026#34;baidu2\u0026#34;\u0026gt;百度2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34; id=\u0026#34;guge\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026#34;\u0026#34;\u0026#34; tree = etree.HTML(html) alls = tree.xpath(\u0026#39;//li/a[contains(@href, \u0026#34;666\u0026#34;) and @id=\u0026#34;baidu2\u0026#34;]/text()\u0026#39;) print(alls)\t# [\u0026#39;百度2\u0026#39;]   xpath详细方法参考：\nhttps://www.cnblogs.com/mxjhaima/p/13775844.html\nBeautifulSoup  BeautifulSoup是Python的一个HTML或XML解析库，可以将输入的文档自动转换成Unicode编码，将输出的文档转换成UTF-8编码，除非文档没有指定的编码方式，不然无需指定编码方式。\n 对html 进行解析时，Beautiful Soup 支持解析器的选取，通常来说在选择解析器的时候需要记住两个点，一个是解析时间，另一个是兼容性。\nBeautiful Soup 支持的解析器有四种：\nhtml.parser，lxml，lxml-xml，html5lib。\n这四种解析器的优点和缺点，文档中下面的表很容易看清：\n在BeautifulSoup 4.0以前，大部分使用的是html.parser ，现在更多使用的是lxml。\n 基础代码演示如下：\n 如果使用的select，find_all()等返回的数据是列表，并没有.string属性，所以通常更推荐使用get_text来获取文本，并且如果标签对象中除了目标内容还有标签时，string就获取不到内容了\n标签对象才拥有属性，返回的列表没有\n节点信息 soup.Tagname.string：获取标签里面的内容 soup.Tagename.get_text()：get_text带不带括号都可以 soup.Tagename.attrs[\u0026ldquo;属性名\u0026rdquo;]：获取属性的值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 将要解析的字符串传入BeautifulSoup对象，该对象会自动补全不标准的HTML字符串 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.prettify()) # 将要解析的字符串以标准的缩进格式输出，只是缩进 print(soup.title.string) # output：The Dormouse\u0026#39;s story print(soup.p.attrs[\u0026#34;class\u0026#34;]) # output：[\u0026#39;title\u0026#39;]   节点选择器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p) # \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;   bs4.element.Tag即上面soup.标签名的形式只能匹配第一个符合的节点后面复合的节点会被忽略。\n提取信息  获取名称 利用name属性即可获取  1  print(soup.title.name) #output：title    获取属性 一个节点可能有多个属性，可以通过选中该节点的元素后调用attrs来获取属性  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34; id=\u0026#34;这是第一个P节点id\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p.attrs)\t# {\u0026#39;class\u0026#39;: [\u0026#39;title\u0026#39;], \u0026#39;id\u0026#39;: \u0026#39;这是第一个P节点id\u0026#39;} print(soup.p.attrs[\u0026#39;class\u0026#39;])\t# [\u0026#39;title\u0026#39;] print(soup.p.attrs[\u0026#39;id\u0026#39;])\t# 这是第一个P节点id print(soup.a.attrs[\u0026#39;id\u0026#39;])\t# link1 print(soup.a.attrs[\u0026#39;class\u0026#39;])\t# [\u0026#39;sister\u0026#39;]   **注意：**返回的属性值有的是字符串有的是字符串组成的列表，如果属性的值是唯一的就返回字符串，否则返回列表。\n 嵌套选择 即在节点里面再继续.Tagname即可接着往下选择节点  关联选择 有时候在选择的过程中不能一步到位选择到想要的节点就要先选中某一个节点然后再以它为基准再次选择子节点、父节点、兄弟节点等等。\n方法选择器  find_all，查询所有符合条件的元素 find_all(self, name=None, attrs={}, recursive=True, text=None,limit=None, **kwargs) select，节点选择器\n 使用该方法仍然可以嵌套继续调用。\n name：使用标签名来查询元素 attrs：传入一些属性作为字典来查询 text：还可以根据节点的文本来进行匹配（可以传入字符串也可以传入正则表达式）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import re from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34; id=\u0026#34;这是第一个P节点id\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) result = soup.find_all(name=\u0026#34;a\u0026#34;) result2 = soup.find_all(attrs={\u0026#34;id\u0026#34;:\u0026#34;link1\u0026#34;}) result3 = soup.find_all(text=re.compile(r\u0026#34;.*The.*\u0026#34;)) print(result) print(result2) print(result3) print(result[0].text) # 获取文本内容    find方法只会匹配第一个符合条件的元素节点\nselect方法，css语法类型的选择器（需要了解的知识：层级选择器）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  from bs4 import BeautifulSoup \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li id=\u0026#34;l1\u0026#34;\u0026gt;张三\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;l2\u0026#34;\u0026gt;李四\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;王五\u0026lt;/li\u0026gt; \u0026lt;a href=\u0026#34;baidu.com\u0026#34; class=\u0026#34;a666\u0026#34;\u0026gt;a666\u0026lt;/a\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;a href=\u0026#34;\u0026#34; title=\u0026#34;555\u0026#34;\u0026gt;a555\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt;我是span\u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(open(\u0026#39;ts.html\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;), \u0026#34;lxml\u0026#34;) print(soup.find(\u0026#39;a\u0026#39;, class_=\u0026#34;a666\u0026#34;)) print(soup.find_all(\u0026#39;a\u0026#39;)) # 以列表的形式返回 print(soup.find_all([\u0026#39;a\u0026#39;, \u0026#39;span\u0026#39;])) # 多个标签查询用列表 print(soup.find_all(\u0026#39;li\u0026#39;, limit=2)) # 返回前两个 print(soup.select(\u0026#39;.a666\u0026#39;)) print(soup.select(\u0026#39;li[id]\u0026#39;)) # 查找li里面有id属性的标签 print(soup.select(\u0026#39;li[id=\u0026#34;l2\u0026#34;]\u0026#39;))   插入一些层级选择器\npyquery 一个CCS选择器的解析库，使用方法PyQuery对象直接括号里面输入CSS选择器，当选择器选择的内容很多的时候要使用items()\n基础操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  from pyquery import PyQuery html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;XXXXXXX\u0026#34;\u0026gt;的撒\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li class=\u0026#34;one-group\u0026#34; id=\u0026#34;one\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;one-group\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34;\u0026gt;百度\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;two-group\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;http://www.tenxun.com\u0026#34;\u0026gt;腾讯\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;two-group\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;http://www.hanson.com\u0026#34;\u0026gt;hanson\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建了一个PyQuery对象 p = PyQuery(html) p_obj = p(\u0026#34;ol li a\u0026#34;) # 上面的语句和注释的语句相同 p_obj = p(\u0026#34;ol\u0026#34;)(\u0026#34;li\u0026#34;)(\u0026#34;a\u0026#34;) print(p_obj) print(type(p_obj)) # output:\u0026lt;class \u0026#39;pyquery.pyquery.PyQuery\u0026#39;\u0026gt; id_obj = p(\u0026#34;#one\u0026#34;) print(\u0026#34;id_obj \u0026#34;, id_obj) one_group = p(\u0026#34;.one-group\u0026#34;) print(\u0026#34;one_group \u0026#34;, one_group) print(type(one_group)) # 获取所有符合条件的对象的文本 text = p(\u0026#34;ul li a\u0026#34;).text() print(text) print(type(text)) # 获取单个（第一个）对象的属性 text = p(\u0026#34;ul li a\u0026#34;).attr(\u0026#34;href\u0026#34;) print(text) print(type(text)) # 获取所有符合条件的属性值 text = p(\u0026#34;ul li a\u0026#34;).items() for i in text: href = i.attr(\u0026#34;href\u0026#34;) text = i.text() print(text, href)   进阶小操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  from pyquery import PyQuery html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;ol\u0026gt; \u0026lt;li class=\u0026#34;FF\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;XXXXXXX\u0026#34;\u0026gt;的撒\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建了一个PyQuery对象 p = PyQuery(html) # 在对象后面添加新标签 ol_a = p(\u0026#34;ol li\u0026#34;).after(\u0026#34;\u0026#34;\u0026#34;\u0026lt;li class=\u0026#34;RR\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;WWWWWWWW\u0026#34;\u0026gt;6666666\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026#34;\u0026#34;\u0026#34;) # 在对象里面再插入新的标签 ol_a_in = p(\u0026#34;ol li\u0026#34;).append(\u0026#34;\u0026#34;\u0026#34;\u0026lt;img href=\u0026#34;Photo place\u0026#34;\u0026gt;【图片】\u0026lt;/img\u0026gt;\u0026#34;\u0026#34;\u0026#34;) # 有属性则可以修改，没有属性可以新增 ol_a_attr = p(\u0026#34;.FF a\u0026#34;).attr(\u0026#34;href\u0026#34;, \u0026#34;HELLO\u0026#34;) ol_a_attr_new = p(\u0026#34;.FF a\u0026#34;).attr(\u0026#34;id\u0026#34;, \u0026#34;WORLD\u0026#34;) # 删除节点或者删除属性 del_a_id = p(\u0026#34;.FF a\u0026#34;).remove_attr(\u0026#34;id\u0026#34;) del_a_in = p(\u0026#34;.RR\u0026#34;).remove() print(p)   parsel 一个可以穿插XPath和CSS选择器的解析库，同时支持XPath和CSS解析，并支持对内容进行提取和修改，同时也糅合了正则表达式的提取功能，灵活且强大。\n基本用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick\u0026#34;) print(oneobj) twoobj = selector.css(\u0026#34;#10086\u0026#34;) print(twoobj)   输出截图如下：\n提取文本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick\u0026#34;) for i in oneobj: print(i.get()) # 使用text()才可以提取文本内容否则还是返回节点对象内容 print(\u0026#34;加上text()\u0026#34; + i.xpath(\u0026#34;.//text()\u0026#34;).get()) twoobj = selector.css(\u0026#34;#10086\u0026#34;) # *用来提取所有子节点（包括纯文本节点）提取文本则加上::text threeobj = selector.css(\u0026#34;#10010 *::text\u0026#34;) print(twoobj.xpath(\u0026#34;.//text()\u0026#34;).get()) print(threeobj.get())   提取属性 提取属性的方法基本上和上面的大体相同\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick/@id\u0026#34;) for i in oneobj: print(i.get()) twoobj = selector.css(\u0026#34;#10086 *::attr(id)\u0026#34;) # *用来提取所有子节点（包括纯文本节点）提取文本则加上::text threeobj = selector.css(\u0026#34;#10010 *::text\u0026#34;) print(twoobj.get()) print(threeobj.get())   正则提取 正则提取以及更多的用法可以参见parsel的官方文档在这里不过多介绍\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8102/","summary":"对爬虫书的第三章摘录 xpath etree.parse()解析本地文件 etree.HTML()解析服务器响应文件 注意： 使用xpath help插件可能会","title":"爬虫的网页数据解析库崔庆才爬虫书"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93  import json import requests import re from urllib.parse import urljoin from concurrent.futures import ThreadPoolExecutor \u0026#34;\u0026#34;\u0026#34; 利用requests爬取站点的每一页电影列表，顺着列表再爬取电影详情页 利用正则表达式爬取电影的名称，封面，类别，上映时间，评分剧情介绍 保存为JSON文件 使用多进程加速爬取 \u0026#34;\u0026#34;\u0026#34; data_list = {} # 提取规则 childhref_reg = re.compile(\u0026#39;\u0026lt;a data-v-7f856186=\u0026#34;\u0026#34; href=\u0026#34;(?P\u0026lt;childhref\u0026gt;.*?)\u0026#34; class=\u0026#34;name\u0026#34;\u0026gt;\u0026#39;, re.S) title_reg = re.compile(\u0026#39;\u0026lt;h2 data-v-63864230=\u0026#34;\u0026#34; class=\u0026#34;m-b-sm\u0026#34;\u0026gt;(?P\u0026lt;title\u0026gt;.*?)\u0026lt;/h2\u0026gt;\u0026#39;, re.S) kinds_reg = re.compile(\u0026#39;\u0026lt;button data-v-7f856186=\u0026#34;\u0026#34; type=\u0026#34;button\u0026#34;.*?\u0026lt;span\u0026gt;(?P\u0026lt;kinds\u0026gt;.*?)\u0026lt;/span\u0026gt;\u0026#39;, re.S) time_reg = re.compile(\u0026#39;class=\u0026#34;m-v-sm info.*?\u0026lt;div data-v-7f856186=\u0026#34;\u0026#34; class=\u0026#34;m-v-sm info\u0026#34;\u0026gt;.*?\u0026lt;span data-v-7f856186=\u0026#34;\u0026#34;\u0026gt;(?P\u0026lt;time\u0026gt;.*?)\u0026lt;/span\u0026gt;\u0026#39;, re.S) img_reg = re.compile(\u0026#39;class=\u0026#34;item.*?src=\u0026#34;(?P\u0026lt;img\u0026gt;.*?)\u0026#34;.*?class=\u0026#34;cover\u0026#34;\u0026gt;\u0026#39;, re.S) score_reg = re.compile(\u0026#39;class=\u0026#34;score m-t-md m-b-n-sm\u0026#34;\u0026gt;(?P\u0026lt;score\u0026gt;.*?)\u0026lt;/p\u0026gt;\u0026#39;, re.S) contents_reg = re.compile(\u0026#39;剧情简介.*?\u0026lt;p data-v-63864230=\u0026#34;\u0026#34;\u0026gt;(?P\u0026lt;contents\u0026gt;.*?)\u0026lt;/p\u0026gt;\u0026#39;, re.S) def append_data(child_html): title = title_reg.finditer(child_html) img = img_reg.finditer(child_html) kinds = kinds_reg.findall(child_html) time = time_reg.finditer(child_html) score = score_reg.finditer(child_html) contents = contents_reg.finditer(child_html) data_title = \u0026#34;\u0026#34; data_img = \u0026#34;\u0026#34; data_kinds = \u0026#34;\u0026#34; data_time = \u0026#34;\u0026#34; data_score = \u0026#34;\u0026#34; data_contents = \u0026#34;\u0026#34; for i in title: data_title = i.group(\u0026#34;title\u0026#34;) for i in img: data_img = i.group(\u0026#34;img\u0026#34;) for i in kinds: data_kinds = i for i in time: data_time = i.group(\u0026#34;time\u0026#34;) for i in score: data_score = i.group(\u0026#34;score\u0026#34;).strip() for i in contents: data_contents = i.group(\u0026#34;contents\u0026#34;).strip() return { \u0026#34;title\u0026#34;:data_title, \u0026#34;img\u0026#34;:data_img, \u0026#34;kinds\u0026#34;:data_kinds, \u0026#34;time\u0026#34;:data_time, \u0026#34;score\u0026#34;:data_score, \u0026#34;contents\u0026#34;:data_contents } def main(): for i in range(1, 11): url = f\u0026#34;https://ssr1.scrape.center/page/{i}\u0026#34; response = requests.get(url=url) html = response.text # 拿到每一个页面的子详情页 for i in childhref_reg.finditer(html): # print(i.group(\u0026#34;childhref\u0026#34;)) catch_url = urljoin(url, i.group(\u0026#34;childhref\u0026#34;)) # print(catch_url) child_response = requests.get(url=catch_url) child_html = child_response.text # 添加数据到列表里 data = append_data(child_html) print(f\u0026#34;正在保存{catch_url}的数据中。。。\u0026#34;) # print(type(data_list)) # print(data) savedata(data) def savedata(data): with open(\u0026#34;../../saves/pj1.json\u0026#34;, \u0026#34;a\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: # ensure_ascii的值为False使得中文字符可以显示出来而不是Unicode字符 # indent=2显示首行缩进两个字符 json.dump(data, f, ensure_ascii=False, indent=2) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(10) as t: t.submit(main)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%AB%E4%B9%A6%E7%B1%8D%E5%B0%8F%E9%A1%B9%E7%9B%AE/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70","title":"爬虫书籍小项目"},{"content":"对爬虫书的第二章摘录 urllib的高级用法  urllib的一些基本用法已经可以满足绝大部分的使用，但有时候又需要一些更高级的操作（Cookie处理，代理设置，登录问题等），需要使用更高级的用法实现。\n 介绍urllib.request模块里面比较重要的两个类：BaseHandler类、OpenerDirector类也可以称为Opener。我们之前使用的urlopen方法就是urllib库为我们提供的一个Opener。\n为什么要引入这两个类，就是为了完成一些更高要求的请求功能。可以利用Handler类来构建Opener类。\n验证示例（登录需要认证的网站） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # coding:utf-8 # @Time : 2022/8/8 8:03 # @file : urllib01.py # $software : PyCharm from urllib.request import HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm, build_opener from urllib.error import URLError \u0026#34;\u0026#34;\u0026#34; urlopen的高级使用方法1 访问一些网站时遇到认证窗口 url：https://ssr3.scrape.center/ \u0026#34;\u0026#34;\u0026#34; url = \u0026#34;https://ssr3.scrape.center/\u0026#34; username = \u0026#34;admin\u0026#34; password = \u0026#34;admin\u0026#34; # 建立一个处理验证的Handler类 p = HTTPPasswordMgrWithDefaultRealm()\t# 具有默认领域的 HTTP 密码管理器 p.add_password(realm=None, uri=url, user=username, passwd=password) auth_handler = HTTPBasicAuthHandler(p)\t# HTTP 基本身份验证处理程序 # 通过刚刚建立的Handler类来构建一个Opener opener = build_opener(auth_handler) try: result = opener.open(url) html = result.read().decode(\u0026#34;utf8\u0026#34;) print(html) except URLError as e: print(e.errno)   代理 则通过使用from urllib.request import ProxyHandler来构建一个处理代理的Handler。 步骤还是同上先创建一个Handler处理对象然后再通过Handler对象构建一个Opener对象。\n示例|片段代码：\n1 2 3 4 5 6  from urllib.request import ProxyHandler,build_opener proxy = ProxyHandler({ \u0026#39;http\u0026#39;:XXXX, \u0026#39;https\u0026#39;:XXXX }) opener = build_opener(proxy)   urllib-Cookie Cookie部分的内容用到时了解即可\n处理异常 urllib库中的error模块定义了由request模块产生的异常。由request模块产生的异常都由urllib中的error模块捕获。\n URLError：时error异常模块的基类，由request模块产生的异常都可以由该模块来捕获处理 HTTPError：专门处理HTTP请求错误，例如认证失败等  示例|片段代码：\n1 2 3 4 5 6 7  from urllib import error, request url = \u0026#34;XXXXX\u0026#34; try: response = request.urlopeen(url=url) except error.URLError as e: print(e.reason)   解析链接（parse模块） urllib.parse.urlparse：实现URL的识别和分段urlparse(url, scheme='', allow_fragments=True)\n1 2 3 4 5 6 7 8  import urllib.parse url = \u0026#39;https://www.baidu.com/s?ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;tn=88093251_47_hao_pg\u0026amp;wd=4399\u0026#39; result = urllib.parse.urlparse(url=url) print(result) # output:ParseResult(scheme=\u0026#39;https\u0026#39;, netloc=\u0026#39;www.baidu.com\u0026#39;, path=\u0026#39;/s\u0026#39;, # params=\u0026#39;\u0026#39;, query=\u0026#39;ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;tn=88093251_47_hao_pg\u0026amp;wd=4399\u0026#39;, # fragment=\u0026#39;\u0026#39;)   urllib.parse.urlunparse：用于构造（拼接）URL。这个方法接受的参数是一个可迭代对象，长度必须为6\n1 2 3 4 5  import urllib.parse url2 = [\u0026#34;https\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;index.html\u0026#34;, \u0026#34;user\u0026#34;, \u0026#34;a=6\u0026#34;, \u0026#34;comment\u0026#34;] print(urllib.parse.urlunparse(url2)) # output: https://www.baidu.com/index.html;user?a=6#comment   和上面两个方法类似的方法是：urlsplit和urlunsplit只不过这两个方法不再单独解析params这部分，所以指定长度为5\nurllib.parse.urlencode：该方法在构造GET请求参数的时候非常有用\n1 2 3 4 5 6 7 8 9  import urllib.parse params = { \u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;:18 } baseurl = \u0026#34;https://www.baidu.com\u0026#34; url = baseurl + urllib.parse.urlencode(params) print(url) # output: https://www.baidu.comname=hanson\u0026amp;age=18   urllib.parse.urljoin：上面的urlunparse和urlparse两个方法可以完成对链接的合并，但有长度和参数限制。所以可以使用更方便的urljoin来代替。 该方法的参数：我们提供一个基础的链接来作为第一个参数，将新的链接作为第二个参数，该方法会分析基础链接（第一个参数）然后对新链接缺失部分进行补充，最后返回结果。\n1 2 3 4 5 6  # 电影天堂案例 import urllib.parse parturl = \u0026#34;/html/gndy/dyzz/20220806/62861.html\u0026#34; baseurl = \u0026#39;https://m.dytt8.net/index2.htm\u0026#39; print(urllib.parse.urljoin(baseurl, parturl)) # https://m.dytt8.net/html/gndy/dyzz/20220806/62861.html   在爬虫爬到一些网页的子url时可以使用该方法进行方便快捷的拼接。\n反序列化 urllib.parse.parse_qs：将一串GET请求参数转换回字典\n1 2 3 4  import urllib.parse baseurl = \u0026#34;https://www.baidu.com/s?ie=UTF-8\u0026amp;wd=4399\u0026#34; print(urllib.parse.parse_qs(baseurl)) #output: {\u0026#39;https://www.baidu.com/s?ie\u0026#39;: [\u0026#39;UTF-8\u0026#39;], \u0026#39;wd\u0026#39;: [\u0026#39;4399\u0026#39;]}   urllib.parse.parse_qsl：将一串GET请求参数转换回元组组成的列表\nurllib.parse.quote：将内容转化成URL编码的格式，并且会自动将中文字符转换成URL编码\nurllib.parse.unquote：将URL编码转换成原来的样子，并且还原成中文字符\nrequests的高级用法 先插入一些requests的基础代码\n1 2 3 4 5 6 7 8 9 10 11  import requests url = \u0026#39;http://httpbin.org/get\u0026#39; params = { \u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;:18 } r = requests.get(url=url, params=params) print(r.text) print(r.url) # http://httpbin.org/get?name=hanson\u0026amp;age=18   抓取二进制数据（音频|视频|图片）等等 图片，视频，音频这些文件本质上就是一堆的二进制码组成，由于有特定的保存格式和对应的解析方式才可以让我们看到不同。\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#39;https://scrape.center/favicon.ico\u0026#39; response = requests.get(url=url) with open(url.split(\u0026#39;/\u0026#39;)[-1], \u0026#34;wb\u0026#34;) as f: f.write(response.content) print(\u0026#34;OVER\u0026#34;)   response.content，打印出来会发现前面是带有b\u0026rsquo;XXX\u0026rsquo;的bytes类型的数据。\n文件上传 1 2 3 4 5 6 7  import requests url = \u0026#39;http://httpbin.org/post\u0026#39; files = {\u0026#34;file\u0026#34;: open(\u0026#34;../../favicon.ico\u0026#34;, \u0026#34;rb\u0026#34;)} r = requests.post(url=url, files=files) print(r.text)   上述代码输出如下图：\nCookie设置 下图中的Cookie里面的每一个分号隔开的键值对就是一个Cookie\n注意：Cookie会有过期时间，不过大多数网站的Cookie都能维持较长的时间\n如何看Cookie是否为服务器返回，如果HttpOnly✔就是服务器返回\n写一个简单的示例来展开获取Cookie的流程\n1 2 3 4 5 6 7 8 9 10  import requests url = \u0026#39;https://www.baidu.com\u0026#39; response = requests.get(url=url) cookies = response.cookies for k, v in cookies.items(): print(k,v)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import requests \u0026#34;\u0026#34;\u0026#34; 在请求中使用Cookie，以知乎为例子 \u0026#34;\u0026#34;\u0026#34; headers = { \u0026#34;user-agent\u0026#34;:\u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\u0026#34; } ori_cookie = \u0026#39;_zap=f00a9a8d-6d68-4c49-a63f-5f607c881e87; d_c0=\u0026#34;ALBYdkyOmBKPTgGo9fY_tZBQIJLqZ-dLQnA=|1612255495\u0026#34;; q_c1=5c836d1485a4425cb749c720a56ae926|1616338519000|1616338519000; _xsrf=7lQouhDAhek8eD6LqYjgHhGsHz0PoUt6; _9755xjdesxxd_=32; YD00517437729195%3AWM_TID=ZR4Os4xKoMhBVQFBBROUQO%2BA8v0CB%2Fbz; gdxidpyhxdE=IKUuGL6ZET8RrZqXckf0938QHxr5yyKsMz7jQ8MwoDEII0u%2FYk1mKc9hO5nhmeTqWNWIzIDOMOyYW%2B8CMXWhhaOQ57M7tVH1rZHMZVPKbSAuhxOETf8f5QPylde5MqUrBPqVeTyuaE%5C%2B2RGMVZ6HRTKcT2m2QrwPzMD3bQdZzk1m%2BdVb%3A1659011795335; YD00517437729195%3AWM_NI=X2JaSyYgFLIZzdGQGoO0OytKX6iU8bJmx6C4%2FgdZqEDqTHs1vMid5tADGqOjj9sfuwomgHE6RoJOucNeWMA8kJXtyvsY1E4BPzmi%2FOCDuFMIBQPpgKfodzmeiXQXcFvZV1Q%3D; YD00517437729195%3AWM_NIKE=9ca17ae2e6ffcda170e2e6eea3d77bb78bfcaad964aca88bb2c55f978f8b82c8598c9ba9d5c846f1989f8ef12af0fea7c3b92a9cee8682d2739bb98aadcc52babc89b2c6658df0819af7419ceaa7b6bb6a8ab3b8d7c240edb9acbbf44f88af9ca9f23bb693a785b880839c8992ce62aae7bd87f87dadb3a9a9c83e92b186b0f8698ebb99aec434a9969895fc7eb187ffa6b74a94a6a38aae45819cbba7d043828fa091e641a6b1ac9bf06fab889fb9e24eb6869ad2d437e2a3; z_c0=2|1:0|10:1659010923|4:z_c0|92:Mi4xcDVuaURnQUFBQUFBc0ZoMlRJNllFaGNBQUFCZ0FsVk5hODNQWXdEWk1sdW5IY01GZldxY1l4bWIwTHZTUkJhSW1B|cccd3186f881e079d08d3cc229a30566ac272d8435bf8207605080d3893b4f48; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1662129715,1662169562,1662341423,1662511885; tst=r; NOT_UNREGISTER_WAITING=1; SESSIONID=Lrw7DCthYiM7U0TDQvH6Modnl745iuwF3QLjwVnSQCd; JOID=UlgUCkj40inS2prlIfFxuZa3gWI0tKFW753ZkEGiu2XthamxZLWVM7jZm-wjie_c-KBPJxjvPBcDZtw11QF6Yzs=; osd=V14TA0791C7b3J_jJvh3vJCwiGQxsqZf6Zjfl0ikvmPqjK-0YrKcNb3fnOUljOnb8aZKIR_mOhIFYdUz0Ad9aj0=; KLBRSID=76ae5fb4fba0f519d97e594f1cef9fab|1662537315|1662537288; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1662537317\u0026#39; url = \u0026#39;https://www.zhihu.com/api/v4/me?include=ad_type%2Cavailable_message_types%2Cdefault_notifications_count%2Cfollow_notifications_count%2Cvote_thank_notifications_count%2Cmessages_count%2Cemail%2Caccount_status%2Cis_bind_phone%2Curl_token\u0026#39; # 将Cookie每一项切割 ori_cookie2 = ori_cookie.split(\u0026#34;; \u0026#34;) # 重新拼接Cookie cookie = {cookie.split(\u0026#34;=\u0026#34;)[0] : cookie.split(\u0026#34;=\u0026#34;)[-1] for cookie in ori_cookie2} print(cookie) response = requests.get(url=url, headers=headers, cookies=cookie) print(response.text)   除了在请求中使用Cookie还可以直接在headers中使用Cookie\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import requests url = \u0026#39;http://www.cninfo.com.cn/new/data/szse_stock.json\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34;, \u0026#34;Cookie\u0026#34;:\u0026#34;JSESSIONID=DF32D632AF6D7F3829B8F00C20020BB4; routeId=.uc1; _sp_id.2141=d5645360-63cb-466f-b011-64f41014b275.1667437600.1.1667437776.1667437600.350917c0-d82b-4bb6-9095-6d31ed791ce2\u0026#34; } response = requests.get(url=url,headers=headers) # print(response.json()) print(\u0026#34;请求头\u0026#34;, response.request.headers) print(\u0026#34;------------------------\u0026#34;) print(\u0026#34;服务器响应的响应头\u0026#34;, response.headers) response.close()   session的维持 第一次如果使用了post方法登录了某个网站，再用get想获取登录后的网站源代码，这相当于打开了两个不同的浏览器，登录信息不会保留。\n解决该方法主要是维持同一个session（会话），要利用到下面说的session对象。\nrequests.session()相当于requests对象，二者的GET请求都要通过get。\n1 2 3 4 5 6 7 8 9 10 11 12  import requests url = \u0026#39;https://www.baidu.com\u0026#39; session = requests.session() # 这步相当于requests response = session.get(url=url) cookies = response.cookies for k, v in cookies.items(): print(k,v)   SSL证书验证 某些网站可能没有设置好HTTPS证书，或者HTTPS证书不被CA机构认可，就有可能出现SSL证书错误的提示，只需要使用varify=False不去验证证书即可\n简易代码repsonse = requests.get(url=url, varify=False)\ntime超时设置，retrying设置最大连接次数 语法基本和urllib的语法相同较为简单，这里不做过多的介绍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  import requests from retrying import retry \u0026#34;\u0026#34;\u0026#34; retrying模块：通过装饰器的方式可以将某个函数执行指定的次数 \u0026#34;\u0026#34;\u0026#34; url = \u0026#34;https://www.baidu.com\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34; } @retry(stop_max_attempt_number=3) def request_get_url(url): print(\u0026#34;1111\u0026#34;) response = requests.get(url=url, headers=headers, timeout=0.01) return response def parse_url(url): try: res = request_get_url(url) except Exception as e: print(e) res = None return res print(parse_url(url=url))   身份验证 前面urllib中提到的个别会用身份认证的网站，以下展开requests的用法\n 方法1较为繁琐的方法\n 1 2 3 4 5 6 7 8 9 10  import requests from requests.auth import HTTPBasicAuth \u0026#34;\u0026#34;\u0026#34; 身份认证，较为繁琐的方法 \u0026#34;\u0026#34;\u0026#34; url = \u0026#39;https://ssr3.scrape.center/\u0026#39; response = requests.get(url=url, auth=HTTPBasicAuth(\u0026#34;admin\u0026#34;, \u0026#34;admin\u0026#34;)) print(response.status_code)    方法2较为简单的方法\n直接传入一个元组给auth会默认调用HTTPBasicAuth这个方法\n 1 2 3 4 5 6 7 8 9 10  import requests \u0026#34;\u0026#34;\u0026#34; 身份认证，较为繁琐的方法 \u0026#34;\u0026#34;\u0026#34; url = \u0026#39;https://ssr3.scrape.center/\u0026#39; response = requests.get(url=url, auth=(\u0026#34;admin\u0026#34;, \u0026#34;admin\u0026#34;)) print(response.status_code)   代理设置 代理设置也较为简单\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#39;XXXXXXXXX\u0026#39; proxy = { \u0026#39;http\u0026#39;:\u0026#39;XXXX\u0026#39;, \u0026#39;https\u0026#39;:\u0026#39;XXXX\u0026#39; } response = requests.get(url=url, proxies=proxy)   httpx 前面的urllib和requests可以爬取绝大多数的网站，但是对于使用HTTP2.0协议的网站，目前（2022.8）urllib和requests是不支持爬取的。所以对于使用了HTTP2.0的网站可以使用httpx库来进行爬取。\n 注意：httpx默认使用的是HTTP1.1，需要我们手动声明才能使用HTTP2.0，代码如下\n 1 2 3 4 5 6 7  import httpx url = \u0026#34;https://spa16.scrape.center/\u0026#34; client = httpx.Client(http2=True) # 开启对HTTP2.0的支持《==》可以理解为声明了一个HTTP2.0的requests对象 response = client.get(url=url) print(response.text)   其余的用法基本和requests相同。\nhttpx支持异步请求\n可以了解下re的sub用法 ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8101/","summary":"对爬虫书的第二章摘录 urllib的高级用法 urllib的一些基本用法已经可以满足绝大部分的使用，但有时候又需要一些更高级的操作（Cookie","title":"Python网络请求的库（urllib，requests）崔庆才爬虫书"},{"content":"浏览器在分析JS中的作用 xhr属于AJAX请求\n想找到对应加密解密函数的实现可以在找到后,把鼠标放上去然后定位(找来源)\nJS中方法是在前面定义,函数是在后面\n不能够回溯的js方法，一般是js的内置方法\n方法:\n函数:\nfilter过滤 可以在filter中输入部分url地址，对所有的url地址起到一定的过滤效果\n对人人网分析  模拟登录前的抓包情况如下\n  模拟登录后的抓包\n  想要搜索某文件里面的关键字可以在search中搜索\n JS解析  通过initiator定位js文件  从上面标记的initator中点击就进入到下图（initiator就是触发前面的文件的来源）\n也可以在点击该文件后的initiator中进入\n通过search关键字搜索定位js文件  在使用search搜索时会遇到以下弊端：\n  如果网页的数据是加密数据是无法搜索的\n  页面的数据是js生成的，异步加载的数据等\n  通过元素绑定的事件监听函数（比如浏览器的登录click等）可以通过EventListeners再次查到js文件来源\n  JS调试  关键字 XHR断点 路径 启动器引导  示例：\n 找到要分析的路径Request URL: https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\n然后复制后面的路径weapi/comment/resource/comments/get?csrf_token=\n  接着点击Sources(源代码)在左侧的XHR/fetch Breakpoints中粘贴刚刚的路径\n  接着刷新网页进行JS调试\n  对书上总结的浏览器爬虫调试  Console：控制面板，用于查看调试日志或者异常，又或者可以用来输入JavaScript代码方便调试和观察 Sources：源代码面板，用于查看网页的HTML文件源代码、JavaScript源代码、CSS源代码。还可以在此面板对JavaScript代码进行调试，比如添加和修改JavaScript断点，观察JavaScript变量等 Performance：性能面板，用于记录和分析页面占用内存的情况 Application：应用面板，用于记录网站加载的所有资源，如缓存、存储、图片、字体等，同时也可以对一些资源进行修改和删除  查看节点事件  通过在Elements面板中查看页面节点信息，可以查看当前页面的HTML源代码及其在网页中的位置 点击右侧的Style选项卡，可以看到对应节点的CSS样式，可以自行删减样式进行观察 接下来点击更右侧的EventListeners选项卡，这里可以显示各个节点当前已经绑定的事件，都是JavaScript原生支持的  change：HTML元素改变时会触发的事件 click：用户点击HTML元素时会触发的事件 mouseover：用户在一个HTML元素上移动鼠标时会触发的事件 mouseout：用户在一个HTML元素上移开鼠标时会触发的事件 Keydown：用户按下键盘时会触发的事件 load：浏览器完成页面加载时会触发的事件    如在下图中，浏览器中选中下面的翻页后查看对应的EventListeners我们可以点击后面的JavaScript代码后跟踪到Sources面板打开对应的JavaScript代码逻辑\n利用好EventListeners，可以轻松找到各个节点绑定事件的处理方法，帮助我们在JavaScript逆向中找到一些突破口\n代码美化 断点调试 在调试JavaScript代码的时候，我们可以在需要的位置上打上断点，当对应事件触发时，浏览器会自动停在断点的位置等待调试，此时我们可以选择单步调试，在面板中观察调用栈、变量值，以更好地跟踪对应位置的执行逻辑\n在点击页面下的换页按钮时并选中断点调试后，点击第二页的按钮，接着可以在右侧的Scope面板处观察到各个变量的值，比如Local域下的当前方法的局部变量，还可以看到对象的各个属性\nWatch面板可以自定添加想要查看的变量和方法，只要点击上面的+号即可\n Step Over Next Function Call：逐语句执行 Step Into Function Call：进入方法内部执行 Step Out of Current Function：跳出当前方法  恢复JavaScript执行 如果在调试过程中，想快速跳到下一个断点或者让JavaScript代码运行下去可以点击Resume script execution按钮，这时如果有断点浏览器则会自动跳转到下一个断点处，否则没有断点时会自动恢复页面的正常执行\n取消断点\n则是切换到Sources面板下然后展开XHR/fetch Breakpoint面板取消即可\nAjax断点 为什么有时候要使用Ajax断点，因为有时候通过使用DOM节点的监听器可能难以找到事件从而继续突破JS逆向，这时候可以使用Ajax断点来逆向破解。\n首先点击第二页后观察Ajax请求\n接着来到Sources面板下面的XHR/fetch Breakpoint下面按+意识是当Ajax请求的URL包含填写的内容时，会进入断点停止\n添加Ajax断点后，接着点击第三页\n一路翻找下来最终可以找到JavaScript的突破口\n改写JavaScript文件 通过在浏览器的Overrides中改写JavaScript代码以达到，通过自己修改JavaScript文件来调试代码\nCall Stack调用栈 调用栈可以帮助我们回溯到某个逻辑的执行流程中，比如有时候在调试JS代码时，点击Step Over Next Function Call按钮时，突然跳转到了另一个函数功能模块时，可以通过Call Stack调用栈里面观察它是从哪里来的\n代码逆向的方法 逆向JS的方法：\n Call Stack调用栈 页面事件监听  ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%ABjs%E5%88%86%E6%9E%90%E6%8A%80%E5%B7%A7/","summary":"浏览器在分析JS中的作用 xhr属于AJAX请求 想找到对应加密解密函数的实现可以在找到后,把鼠标放上去然后定位(找来源) JS中方法是在前面定义","title":"爬虫js分析技巧"},{"content":"Scrapy学习 使用Scrapy创建爬虫项目使用如下步骤：  创建爬虫项目scrapy startproject 项目名字(项目名字不允许使用数字开头，不能包含中文) 创建爬虫文件，在spiders文件夹中创建爬虫文件  先进入到要创建爬虫文件的目录cd 项目名字\\项目名字\\spiders 创建爬虫文件scrapy genspider 爬虫文件的名字 要爬取的网页（要爬取的网页一般不写http只写www）   运行爬虫代码scrapy crawl 爬虫名字   scrapy项目结构 项目名字\n​\t项目名字\n​\tspiders文件（存储的是爬虫文件）\n​\tinit\n​\t自定义的爬虫文件 核心功能文件\n​\tinit\n​\titmes\t定义数据结构的地方爬取的数据包含哪些，是一个继承自scrapy.Item的类，通俗的说要下载哪些数据\n​\tmiddlewares\t中间件\t代理\n​\tpipelines\t管道 里面只有一个类用来处理下载的数据，默认值是300优先级，值越小优先级越高（1-1000）\n​\tsettings\t配置文件robots协议、UA等配置\n response的属性和方法  response.text\t获取的是响应字符串 response.body\t获取的是二进制数据 response.xpath\t可以直接通过xpath方法来解析response中的内容\nresponse.extract()\t提取seletor对象的data属性值，现在一般使用get()\nresponse.extract_first()\t提取seletor列表的第一个数据下面，现在一般使用getall()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import scrapy class TcSpider(scrapy.Spider): name = \u0026#39;tc\u0026#39; allowed_domains = [\u0026#39;https://hanson00.github.io/posts/technology/python/python-json/\u0026#39;] start_urls = [\u0026#39;https://hanson00.github.io/posts/technology/python/python-json/\u0026#39;] def parse(self, response, **kwargs): # 获取网页字符串 # content = response.text # 二进制数据 # content = response.body # content = response.xpath(\u0026#39;//*[@id=\u0026#34;top\u0026#34;]/main/article/header/div[1]/a[4]/text()\u0026#39;) # print(\u0026#34;=====================================\u0026#34;) # print(content.extract()) content = response.xpath(\u0026#39;//*[@id=\u0026#34;top\u0026#34;]/main/article/header/div[1]/a[4]\u0026#39;).get() print(\u0026#34;=====================================\u0026#34;) print(content)    管道的使用 管道有优先级，值越小优先级越高\n 现在pipelines中定义自己的功能管道 在settings中开启管道 写管道功能  settings数据库配置  导入settingsfrom scrapy.utils.project import get_project_settings 在管道文件中配置数据库连接参数  settings = get_project_settings() self.host = settings['DB_HOST']，参考在settings中的配置等等。    scrapy架构  引擎 下载器 spiders 调度器 管道  官网架构概述图如下：\n自己总结版如下：\n 引擎向spiders要url 引擎在得到url后，将要爬取的url给调度器 调度器将url生成对象放入指定的队列中 调度器从队列中出队一个请求 引擎将请求交给下载器进行处理 下载器发送请求到互联网获取数据 下载器将response数据返回给引擎 引擎将数据再次给到spiders spiders通过xpath解析数据，得到url或者数据 spiders将数据或者url返回给引擎 引擎判断是数据还是url，是数据则交给管道处理，是url则交给调度器处理(即重复上诉逻辑)  日志 一般推荐直接在settings文件中写入LOG_FILE = log.log来解决执行时调式显示过多。\n项目学习 当当网项目  爬取数据（spiders爬虫文件） 定义数据结构（items） 开启管道（settings） 下载数据（pipeline）  项目目录：\n 记得在settings中开启管道\n items.py文件如下： 定义要下载的数据的数据结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Define here the models for your scraped items # # See documentation in: # https://docs.scrapy.org/en/latest/topics/items.html import scrapy class ScrapyDangdangItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 就是下载的数据有哪些 src_pic = scrapy.Field() name = scrapy.Field() price = scrapy.Field()   spiders.dang.py文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  import scrapy from ..items import ScrapyDangdangItem class DangSpider(scrapy.Spider): name = \u0026#39;dang\u0026#39; # allowed_domains在多页下载中可以只写域名（category.dangdang.com）或者可以不写 allowed_domains = [\u0026#39;category.dangdang.com\u0026#39;] start_urls = [\u0026#39;http://category.dangdang.com/cp01.01.07.00.00.00.html\u0026#39;] page = 1 def parse(self, response, **kwargs): print(\u0026#34;=============================\u0026#34;) li_list = response.xpath(\u0026#39;//ul[@id=\u0026#34;component_59\u0026#34;]/li\u0026#39;) for i in li_list: src_pic = i.xpath(\u0026#39;.//img/@data-original\u0026#39;).extract_first() name = i.xpath(\u0026#39;./p[@name=\u0026#34;title\u0026#34;]/a/@title\u0026#39;).extract_first() price = i.xpath(\u0026#39;./p[@class=\u0026#34;price\u0026#34;]/span[1]/text()\u0026#39;).extract_first() if src_pic: src_pic = src_pic else: src_pic = i.xpath(\u0026#39;.//img/@src\u0026#39;).extract_first() # print(src_pic, name, price) # 开始管道封装下载数据 # 导入items的数据结构 book = ScrapyDangdangItem(src_pic=src_pic, name=name, price=price) # 获取一个book就将一个book交给pipeline yield book if self.page \u0026lt; 100: self.page = self.page + 1 more_url = f\u0026#34;http://category.dangdang.com/pg{self.page}-cp01.01.07.00.00.00.html\u0026#34; # scrapy.Request就是scrapy的get请求，还可以通过Request中的mete传递参数， # 并用response.meta[\u0026#39;参数名\u0026#39;]接受，callback就是要执行的那个函数 yield scrapy.Request(url=more_url, callback=self.parse)   pipeline文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  # Define your item pipelines here # # Don\u0026#39;t forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter import requests # 如果想要使用管道，就要在setting中开启管道 class ScrapyDangdangPipeline: # 该方法会在爬虫文件开始前就执行 def open_spider(self, spider): self.f = open(\u0026#34;book.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) def process_item(self, item, spider): #item就是yield后面的book对象 # 如果单单在这里使用文件的读写会使文件读写过于频繁 # with open(\u0026#39;book.json\u0026#39;, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: # f.write(str(item)) self.f.write(str(item)) return item # 该方法会在爬虫文件结束时执行 def close_spider(self, spider): self.f.close() # 多条管道的开启 # 1.首先创建一个管道类 # 2.在settings中开启管道 class DangDangDownloadPipeline: def process_item(self, item, spider): #item就是yield后面的book对象 url = \u0026#34;http:\u0026#34; + item.get(\u0026#34;src_pic\u0026#34;) name = \u0026#34;./books/\u0026#34; + item.get(\u0026#34;name\u0026#34;) + \u0026#34;.jpg\u0026#34; response = requests.get(url=url) with open(name, \u0026#34;wb\u0026#34;) as f: f.write(response.content)   在settings打开管道\n1 2 3 4 5  ITEM_PIPELINES = { # 管道可以有很多个，有优先级，优先级是1-1000，越低越高 \u0026#39;scrapy_dangdang.pipelines.ScrapyDangdangPipeline\u0026#39;: 300, \u0026#39;scrapy_dangdang.pipelines.DangDangDownloadPipeline\u0026#39;:301, }   # scrapy.Request就是scrapy的get请求，还可以通过Request中的mete传递参数，并用response.meta['参数名']接受\nCrawlSpider-读书网  继承自scrapy.Spider CrawlSpider可以自定义规则，再解析html内容的时候可以根据链接规则提取出指定的链接，然后载器向这些链接发送请求 提取链接  allow = ()，\t正则表达式，提取符合正则表达式的链接 restrict_xpaths = (), 提取符合xpath规则的链接 restrict_css = (), 提取符合选择器规则的链接     rules = ( Rule(LinkExtractor(allow=r'/book/1158_\\d+.html'), callback=\u0026lsquo;parse_item\u0026rsquo;, follow=False), ) 的\tfollwo如果为True则表示为是否按照链接提取规则一直跟进提取\n 使用步骤：\n 创建项目scrapy startproject 项目名 进入到spiders后执行scrapy genspider -t crawl 爬虫文件名 爬取链接  spiders.read.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from ..items import ScrapyReadbookItem class ReadSpider(CrawlSpider): name = \u0026#39;read\u0026#39; allowed_domains = [\u0026#39;www.dushu.com\u0026#39;] start_urls = [\u0026#39;https://www.dushu.com/book/1158_1.html\u0026#39;] rules = ( Rule(LinkExtractor(allow=r\u0026#39;/book/1158_\\d+.html\u0026#39;), callback=\u0026#39;parse_item\u0026#39;, follow=False), ) def parse_item(self, response): name = response.xpath(\u0026#39;//div[@class=\u0026#34;book-info\u0026#34;]/h3/a/@title\u0026#39;).getall() book = ScrapyReadbookItem(name=name) yield book   POST请求 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import scrapy import json class TestpostSpider(scrapy.Spider): name = \u0026#39;testpost\u0026#39; # POST请求 如果没有参数那么请求就没有意义所以start_urls也没有用了导致需要执行的parse方法无效 allowed_domains = [\u0026#39;fanyi.baidu.com\u0026#39;] # start_urls = [\u0026#39;https://fanyi.baidu.com/sug\u0026#39;] # # def parse(self, response, **kwargs): # pass def start_requests(self): url = \u0026#39;https://fanyi.baidu.com/sug\u0026#39; data = { \u0026#39;kw\u0026#39;:\u0026#39;apple\u0026#39; } yield scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second) def parse_second(self, response): content = response.text res = json.loads(content) print(res)   items.py\n1 2 3 4 5 6 7 8 9 10 11 12  # Define here the models for your scraped items # # See documentation in: # https://docs.scrapy.org/en/latest/topics/items.html import scrapy class ScrapyReadbookItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() name = scrapy.Field()   pipeline.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # Define your item pipelines here # # Don\u0026#39;t forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter class ScrapyReadbookPipeline: def open_spider(self, spider): self.f = open(\u0026#34;book.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) def process_item(self, item, spider): self.f.write(str(item)) return item def close_spider(self, spider): self.f.close()   注意： 在使用Scrapy时会遇到懒加载，比如上面的当当网项目\n如果请求的接口是html为结尾的是不需要加\\的。\n在开发爬虫项目中如果使用crawlspider时一定要注意爬取的start_urls与后续要爬取的url之间的规律与规则。\n","permalink":"https://hanson00.github.io/posts/technology/python/scrapy%E5%AD%A6%E4%B9%A01.0/","summary":"Scrapy学习 使用Scrapy创建爬虫项目使用如下步骤： 创建爬虫项目scrapy startproject 项目名字(项目名字不允许使用数字开头，不能包含中文) 创建","title":"Scrapy学习1.0"},{"content":"汽车之家项目概述 首先对要爬取的网页代码进行分析https://you.autohome.com.cn/index/searchkeyword?keyword=%E9%A9%AC%E5%B0%94%E4%BB%A3%E5%A4%AB 在查看网页源代码后发现没有要的数据就对该页面进行抓包如下：\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  import requests import re from concurrent.futures import ThreadPoolExecutor url0 = \u0026#39;https://you.autohome.com.cn\u0026#39; # 保存下载图片 def download_pic(urlpic): response3 = requests.get(urlpic) file_path = urlpic.split(\u0026#34;/\u0026#34;)[-1] with open(f\u0026#34;pic/{file_path}\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(response3.content) def main(): # 1.在原始的列表帖子页面获取 url = \u0026#34;https://you.autohome.com.cn/summary/getsearchresultlist\u0026#34; params1 = { \u0026#39;ps\u0026#39;: 20, \u0026#39;pg\u0026#39;: 0, \u0026#39;type\u0026#39;: 4, \u0026#39;q\u0026#39;: \u0026#39;马尔代夫\u0026#39;, \u0026#39;dataType\u0026#39;: \u0026#39;4,9\u0026#39;, \u0026#39;_\u0026#39;: 1659220150512, } response = requests.get(url=url,params=params1) dic1 = response.json() list_blog_place = response.json()[\u0026#39;result\u0026#39;][\u0026#39;hitlist\u0026#39;] # 获得每个文章的部分url列表 # print(dic1) pic_re = re.compile(\u0026#39;{\u0026#34;imgurl\u0026#34;:\u0026#34;(?P\u0026lt;pic\u0026gt;.*?)\u0026#34;\u0026#39;) for i in list_blog_place: url4 = i[\u0026#39;url\u0026#39;] # print(url4) realurl = url0 + url4 response2 = requests.get(url=realurl) pichtml = response2.text # print(response2.text) allpic = pic_re.finditer(pichtml) piclist = [] for i in allpic: # print(i.group(\u0026#39;pic\u0026#39;)) piclist.append(i.group(\u0026#39;pic\u0026#39;)) # 下载图片 with ThreadPoolExecutor(10) as t: for pic in piclist: t.submit(download_pic, pic) response.close() if __name__ == \u0026#39;__main__\u0026#39;: main()   使用from urllib.parse import urljoin 对URL进行方便的拼接，大大方便拼接完整url，在m3u8中非常适用。替换小部分即可\n1 2 3 4 5 6 7 8 9 10 11 12  for i in list_blog_place: url4 = i[\u0026#39;url\u0026#39;] # print(url4) realurl = urljoin(url, url4) response2 = requests.get(url=realurl) pichtml = response2.text # print(response2.text) allpic = pic_re.finditer(pichtml) piclist = [] for i in allpic: # print(i.group(\u0026#39;pic\u0026#39;)) piclist.append(i.group(\u0026#39;pic\u0026#39;))   ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%AB-%E6%B1%BD%E8%BD%A6%E4%B9%8B%E5%AE%B6/","summary":"汽车之家项目概述 首先对要爬取的网页代码进行分析https://you.autohome.com.cn/index/searchkeyword","title":"爬虫 汽车之家"},{"content":"json模块 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。下面给出json模块的常用方法\n json.dump()：将Python数据对象以JSON格式数据流写入文件 json.load()：解析包含JSON数据的文件为Python对象 json.dumps()：将Python数据对象转换成JSON格式的字符串 json.loads()：将包含JSON的字符串、字节以及字节数组解析成为Python对象  生成的json字符串一定是双引号引导的。\n Python中的json模块的使用方法\n    方法 功能     json.dump(obj,fp) 将Python数据类型转换并保存到json格式的文件里   json.dumps(s) 将Python数据类型转换成json格式的字符串   json.load(fp) 将json格式的文件读取数据并转换成Python的类型   json.loads(s) 将json格式的字符串转换成Python类型的字符串     dump和dumps方法\n 1 2 3 4 5 6 7 8 9 10 11 12  import json person = {\u0026#34;name\u0026#34;: \u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;tel\u0026#34;:[123, 321]} print(type(person)) print(person) # 将字典转换成json格式的字符串,intent指定缩进，不指定也行 j = json.dumps(person, indent=2) print(type(j)) # \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; print(j) with open(\u0026#34;../saves/data.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(person, f)    loads方法\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # coding:utf-8 # @Time : 2022/8/11 20:39 # @Author : 软件1194温铭军 # @file : json01.py # $software : PyCharm import json person = {\u0026#34;name\u0026#34;: \u0026#34;杰克\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;tel\u0026#34;:[123, 321]} # 将字典转换成json格式的字符串,intent指定缩进，不指定也行,ensure_ascii显示中文 j = json.dumps(person, indent=2, ensure_ascii=False) print(j) # 将json string 转换 python对象 pythonObj = json.loads(j) print(type(pythonObj)) # output: \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; print(pythonObj)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import json dic = { \u0026#34;name\u0026#34; : \u0026#34;饿饿饿\u0026#34;, \u0026#34;age\u0026#34;:[ { \u0026#34;shuah\u0026#34;:\u0026#34;sodjsijd\u0026#34; } ] } with open(\u0026#34;22.txt\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(dic, f) # 将上述json数据写入文件 with open(\u0026#34;22.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.read()) # output:{\u0026#34;name\u0026#34;: \u0026#34;\\u997f\\u997f\\u997f\u0026#34;, \u0026#34;age\u0026#34;: [{\u0026#34;shuah\u0026#34;: \u0026#34;sodjsijd\u0026#34;}]} print(\u0026#34;--------------------------------\u0026#34;) with open(\u0026#34;22.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: print(json.load(f)) # output:{\u0026#39;name\u0026#39;: \u0026#39;饿饿饿\u0026#39;, \u0026#39;age\u0026#39;: [{\u0026#39;shuah\u0026#39;: \u0026#39;sodjsijd\u0026#39;}]} print(\u0026#34;OVER\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-json/","summary":"json模块 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。下面给出json模块的常用方法 json.dump(","title":"Python Json"},{"content":"Python Flask的项目实战 项目概述：使用Flask将爬虫爬取到的数据结合echarts可视化的展示到页面(Python+Flask+Echarts)，使用jQuery和AJAX实现数据交互与局部刷新\n效果展示如下：\n项目目录如下：\n爬取并存储疫情数据 数据来源：腾讯疫情实时数据\n 获取疫情数据\nget_tenxun_nowdate_data()：将返回爬取到的腾讯疫情数据\nupdate_details()|insert_history()|update_history()：对数据进行数据库的存储操作\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166  # @file : spiders1.py # $software : PyCharm import re import time import pymysql import requests import time import traceback from configsql import get_conn, close_conn def get_tenxund_history_ata_and_all(): \u0026#34;\u0026#34;\u0026#34; 第一个函数先返回历史数据 :return:history[历史数据], nowdate_data[省级数据] \u0026#34;\u0026#34;\u0026#34; history = {} url = \u0026#34;https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=chinaDayListNew,chinaDayAddListNew\u0026amp;limit=30\u0026#34; response = requests.get(url=url) data_resp = response.json() for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;chinaDayListNew\u0026#34;]: ds = \u0026#34;2022.\u0026#34; + i[\u0026#34;date\u0026#34;] tup = time.strptime(ds, \u0026#34;%Y.%m.%d\u0026#34;) ds = time.strftime(\u0026#34;%Y-%m-%d\u0026#34;, tup) confirm = i[\u0026#34;confirm\u0026#34;] suspect = i[\u0026#34;suspect\u0026#34;] heal = i[\u0026#34;heal\u0026#34;] dead = i[\u0026#34;dead\u0026#34;] history[ds] = {\u0026#34;confirm\u0026#34;: confirm, \u0026#34;suspect\u0026#34;: suspect, \u0026#34;heal\u0026#34;: heal, \u0026#34;dead\u0026#34;: dead} for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;chinaDayAddListNew\u0026#34;]: ds = \u0026#34;2022.\u0026#34; + i[\u0026#34;date\u0026#34;] tup = time.strptime(ds, \u0026#34;%Y.%m.%d\u0026#34;) ds = time.strftime(\u0026#34;%Y-%m-%d\u0026#34;, tup) confirm = i[\u0026#34;confirm\u0026#34;] suspect = i[\u0026#34;suspect\u0026#34;] heal = i[\u0026#34;heal\u0026#34;] dead = i[\u0026#34;dead\u0026#34;] history[ds].update({\u0026#34;confirm_add\u0026#34;: confirm, \u0026#34;suspect_add\u0026#34;: suspect, \u0026#34;heal_add\u0026#34;: heal, \u0026#34;dead_add\u0026#34;: dead}) nowdate_data = get_tenxun_nowdate_data() return history, nowdate_data def get_tenxun_nowdate_data(): details = [] # 当日详细数据 url = \u0026#34;https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=localCityNCOVDataList,diseaseh5Shelf\u0026#34; data = { \u0026#34;modules\u0026#34;: \u0026#34;localCityNCOVDataList,diseaseh5Shelf\u0026#34; } response = requests.post(url=url) data_resp = response.json() update_time = data_resp[\u0026#34;data\u0026#34;][\u0026#34;diseaseh5Shelf\u0026#34;][\u0026#34;lastUpdateTime\u0026#34;] for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;diseaseh5Shelf\u0026#34;][\u0026#34;areaTree\u0026#34;][0][\u0026#34;children\u0026#34;]: province = i[\u0026#34;name\u0026#34;] # 省名 # print(province) for city_info in i[\u0026#34;children\u0026#34;]: # if city_info[\u0026#34;name\u0026#34;] == \u0026#34;地区待确认\u0026#34;: # city_info[\u0026#34;name\u0026#34;] = province # print(city_info[\u0026#34;name\u0026#34;]) city = city_info[\u0026#34;name\u0026#34;] confirm = city_info[\u0026#34;total\u0026#34;][\u0026#34;confirm\u0026#34;] confirm_add = city_info[\u0026#34;total\u0026#34;][\u0026#34;continueDayZeroLocalConfirmAdd\u0026#34;] heal = city_info[\u0026#34;total\u0026#34;][\u0026#34;heal\u0026#34;] dead = city_info[\u0026#34;total\u0026#34;][\u0026#34;dead\u0026#34;] details.append([update_time, province, city, confirm, confirm_add, heal, dead]) return details def update_details(): cursor = None conn = None try: li = get_tenxund_history_ata_and_all()[1]\t# 获取上面爬取的数据 conn, cursor = get_conn()\t# 获取游标对象 sql = \u0026#34;insert into details(update_time, province, city, confirm, confirm_add, heal, dead) \u0026#34; \\ \u0026#34;values(%s, %s, %s, %s, %s, %s, %s)\u0026#34; # 下面两句是获取最新的时间，并判断是否要更新数据 sql_query = \u0026#34;select %s=(select update_time from details order by id desc limit 1)\u0026#34; cursor.execute(sql_query, li[0][0]) if not cursor.fetchone()[0]: print(f\u0026#34;{time.asctime()}开始更新数据\u0026#34;) for item in li: cursor.execute(sql, item) conn.commit() print(f\u0026#34;{time.asctime()}数据更新完毕\u0026#34;) else: print(f\u0026#34;{time.asctime()}已经是最新数据了\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def insert_history(): cursor = None conn = None try: dic = get_tenxund_history_ata_and_all()[0] print(f\u0026#34;{time.asctime()}开始插入历史数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into history values(%s, %s, %s, %s, %s, %s, %s, %s, %s)\u0026#34; for k, v in dic.items(): cursor.execute(sql, [k, v.get(\u0026#34;confirm\u0026#34;), v.get(\u0026#34;confirm_add\u0026#34;), v.get(\u0026#34;suspect\u0026#34;), v.get(\u0026#34;suspect_add\u0026#34;), v.get(\u0026#34;heal\u0026#34;), v.get(\u0026#34;heal_add\u0026#34;), v.get(\u0026#34;dead\u0026#34;), v.get(\u0026#34;dead_add\u0026#34;)]) conn.commit() print(f\u0026#34;{time.asctime()}插入历史数据完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def update_history(): cursor = None conn = None try: dic = get_tenxund_history_ata_and_all()[0] print(f\u0026#34;{time.asctime()}开始更新历史数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into history values(%s, %s, %s, %s, %s, %s, %s, %s, %s)\u0026#34; sql_query = \u0026#34;select confirm from history where ds=%s\u0026#34; for k, v in dic.items(): if not cursor.execute(sql_query, k): cursor.execute(sql, [k, v.get(\u0026#34;confirm\u0026#34;), v.get(\u0026#34;confirm_add\u0026#34;), v.get(\u0026#34;suspect\u0026#34;), v.get(\u0026#34;suspect_add\u0026#34;), v.get(\u0026#34;heal\u0026#34;), v.get(\u0026#34;heal_add\u0026#34;), v.get(\u0026#34;dead\u0026#34;), v.get(\u0026#34;dead_add\u0026#34;)]) conn.commit() print(f\u0026#34;{time.asctime()}插入历史数据完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def get_baidu_hot(): url = \u0026#34;https://top.baidu.com/board?tab=realtime\u0026#34; response = requests.get(url=url) # print(response.text) reword = re.compile(\u0026#39;\u0026#34;word\u0026#34;:\u0026#34;(?P\u0026lt;content\u0026gt;.*?)\u0026#34;\u0026#39;) result = reword.finditer(response.text) # for i in result: # print(i.group(\u0026#34;content\u0026#34;)) content = [i.group(\u0026#34;content\u0026#34;) for i in result] # print(content) return content response.close() def update_baidu_hot(): cursor = None conn = None try: content = get_baidu_hot() print(f\u0026#34;{time.asctime()}开始更新热搜数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into hotsearch(dt, content) values(%s, %s)\u0026#34; ts = time.strftime(\u0026#34;%Y-%m-%d%X\u0026#34;) for i in content: cursor.execute(sql, (ts, i)) conn.commit() print(f\u0026#34;{time.asctime()}热搜数据更新完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) if __name__ == \u0026#39;__main__\u0026#39;: insert_history() update_details() update_history() update_baidu_hot()    存储疫情数据库的配置\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # @file : configsql.py # $software : PyCharm import pymysql def get_conn(): # 打开数据库连接 db = pymysql.connect(host=\u0026#39;localhost\u0026#39;, user=\u0026#39;root\u0026#39;, password=\u0026#39;1234\u0026#39;, database=\u0026#39;cov\u0026#39;) # 创建一个游标对象 cursor cursor = db.cursor() return db, cursor def close_conn(conn, cursor): if cursor: cursor.close() if conn: conn.close()   Flask业务逻辑的编写  右上角的时间显示的代码\n 后台返回时间\n1 2 3 4 5 6 7 8  # @file : utils.py # $software : PyCharm import time import pymysql def gettime(): time_str = time.strftime(\u0026#34;%Y{}%m{}%d{}%X\u0026#34;) return time_str.format(\u0026#34;年\u0026#34;,\u0026#34;月\u0026#34;,\u0026#34;日\u0026#34;)   controller.js\n1 2 3 4 5 6 7 8 9 10 11  function gettime() { $.ajax({ url: \u0026#34;/time\u0026#34;, timeout: 10000, success: function (data) { $(\u0026#34;#time\u0026#34;).html(data) }, error: function (xhr, type, errorThrown) { } }) }   main.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;疫情监控\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;css/main.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/echarts.min.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/china.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;title\u0026#34;\u0026gt;全国疫情实时跟踪\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;time\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;l1\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;l2\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;c1\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计确诊\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;剩余疑似\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计治愈\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计死亡\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;c2\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;r1\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;r2\u0026#34;\u0026gt;\u0026lt;h3\u0026gt;在家办公，减少出门次数，勤洗手多通风，尽自己的最大努力为这场战疫献出自己的一份力。中国加油!\u0026lt;/h3\u0026gt;\u0026lt;br\u0026gt; \u0026lt;h3\u0026gt;拼尽全力抗击疫情，医护人员是最美逆行者!为自己加油，让我们一起并肩作战，让疫情早日消散!\u0026lt;/h3\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_center.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/controller.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_left1.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_left2.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_right1.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   controller.js完整代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90  function gettime() { $.ajax({ url: \u0026#34;/time\u0026#34;, timeout: 10000, success: function (data) { $(\u0026#34;#time\u0026#34;).html(data) }, error: function (xhr, type, errorThrown) { } }) } function getc1data() { $.ajax({ url: \u0026#34;/c1\u0026#34;, success: function (data) { $(\u0026#34;.num h1\u0026#34;).eq(0).text(data.confirm); $(\u0026#34;.num h1\u0026#34;).eq(1).text(data.suspect); $(\u0026#34;.num h1\u0026#34;).eq(2).text(data.heal); $(\u0026#34;.num h1\u0026#34;).eq(3).text(data.dead); }, error: function (xhr, type, errorThrown) { } }) } function getc2data() { $.ajax({ url: \u0026#34;/c2\u0026#34;, success: function (data) { optionMap.series[0].data = data.data ec_center.setOption(optionMap) }, error: function (xhr, type, errorThrown) { } }) } function getl1data() { $.ajax({ url: \u0026#34;/l1\u0026#34;, success: function (data) { ec_left1_Option.xAxis[0].data = data.day ec_left1_Option.series[0].data = data.confirm ec_left1_Option.series[1].data = data.suspect ec_left1_Option.series[2].data = data.heal ec_left1_Option.series[3].data = data.dead ec_left1.setOption(ec_left1_Option) }, error: function (xhr, type, errorThrown) { } }) } function getl2data() { $.ajax({ url: \u0026#34;/l2\u0026#34;, success: function (data) { ec_left2_Option.xAxis[0].data = data.day ec_left2_Option.series[0].data = data.confirm_add ec_left2_Option.series[1].data = data.suspect_add ec_left2.setOption(ec_left2_Option) }, error: function (xhr, type, errorThrown) { } }) } function get_r1_data(){ $.ajax({ url:\u0026#34;/r1\u0026#34;, success: function(data) { ec_right1_Option.xAxis.data=data.city; ec_right1_Option.series[0].data=data.confirm; ec_right1.setOption(ec_right1_Option); } }) } getc1data() getl1data() getl2data() getc2data() get_r1_data() setInterval(gettime, 1000) setInterval(getc1data, 1000*10) setInterval(getc2data, 10000*10) setInterval(getl1data, 10000*10) setInterval(getl2data, 10000*10) setInterval(get_r1_data, 10000*10)   Echarts语法简明  初始化echarts实例  1 2  //初始化echarts实例 var ec_center = echarts.init(document.getElementById(\u0026#34;c2\u0026#34;),\u0026#34;dark\u0026#34;);    找到你想要的数据可视化模板代码，或者自己编写\n  使用制定的配置项和数据显示图表\n1 2  //使用制定的配置项和数据显示图表 ec_center.setOption(optionMap);     例如：\nec_center.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  //初始化echarts实例 var ec_center = echarts.init(document.getElementById(\u0026#34;c2\u0026#34;),\u0026#34;dark\u0026#34;); var mydata = [] var optionMap = { title: { text: \u0026#39;\u0026#39;, subtext: \u0026#39;\u0026#39;, x: \u0026#39;left\u0026#39; }, tooltip: { trigger: \u0026#39;item\u0026#39; }, //左侧小导航图标  visualMap: { show: true, x: \u0026#39;left\u0026#39;, y: \u0026#39;bottom\u0026#39;, textStyle: { fontSize: 8 }, splitList: [{ start: 1, end: 9 }, { start: 10, end: 99 }, { start: 100, end: 999 }, { start: 1000, end: 9999 }, { start: 10000 } ], color: [\u0026#39;#8A3310\u0026#39;,\u0026#39;#C64918\u0026#39;, \u0026#39;#E55B25\u0026#39;,\u0026#39;#F2AD92\u0026#39;, \u0026#39;#F9DCD1\u0026#39;] }, //配置属性  series: [{ name: \u0026#39;累积确诊人数\u0026#39;, type: \u0026#39;map\u0026#39;, mapType: \u0026#39;china\u0026#39;, roam: false, itemStyle: { normal: { borderWidth: .5, borderColor: \u0026#39;#009fe8\u0026#39;, areaColor: \u0026#39;#ffefd5\u0026#39; }, emphasis: { borderWidth: .5, borderColor: \u0026#39;#4b0082\u0026#39;, areaColor: \u0026#39;#fff\u0026#39; } }, label: { normal: { show: true, //省份名称  fontSize: 8 }, emphasis: { show: true, fontSize: 8 } }, data: mydata //数据  }] }; //使用制定的配置项和数据显示图表  ec_center.setOption(optionMap);   ec_left1.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  var ec_left1 = echarts.init(document.getElementById(\u0026#39;l1\u0026#39;), \u0026#34;dark\u0026#34;); var ec_left1_Option = { title: { text: \u0026#34;全国累计趋势\u0026#34;, textStyle: { //color:\u0026#39;white\u0026#39;,  }, left: \u0026#39;left\u0026#39;, }, tooltip: { trigger: \u0026#39;axis\u0026#39;, axisPointer: { type: \u0026#39;line\u0026#39;, lineStyle: { color: \u0026#39;#7171C6\u0026#39; } }, }, legend: { data: [\u0026#34;累计确诊\u0026#34;, \u0026#34;现有疑似\u0026#34;, \u0026#34;累积治愈\u0026#34;, \u0026#34;累计死亡\u0026#34;], left: \u0026#34;right\u0026#34; }, //图形位置  grid: { left: \u0026#39;4%\u0026#39;, right: \u0026#39;6%\u0026#39;, bottom: \u0026#39;4%\u0026#39;, top: 50, containLabel: true }, xAxis: [{ type: \u0026#39;category\u0026#39;, data: [] // \u0026#39;01.24\u0026#39;, \u0026#39;01.25\u0026#39;, \u0026#39;01.26\u0026#39;  }], yAxis: [{ type: \u0026#39;value\u0026#39;, axisLabel: { show: true, color: \u0026#39;white\u0026#39;, fontSize: 12, formatter: function(value) { if (value \u0026gt;= 1000) { value = value / 1000 + \u0026#39;k\u0026#39;; } return value; } }, //y轴线设置显示  axisLine: { show: true }, //与x轴平行的线样式  splitLine: { show: true, lineStyle: { color: \u0026#39;#17273B\u0026#39;, width: 1, type: \u0026#39;solid\u0026#39;, } } }], series: [{ name: \u0026#34;累计确诊\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] // 260, 406, 529  }, { name: \u0026#34;现有疑似\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: []}, { name: \u0026#34;累积治愈\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] },{ name: \u0026#34;累计死亡\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }] }; ec_left1.setOption(ec_left1_Option)   ec_left2.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  var ec_left2 = echarts.init(document.getElementById(\u0026#39;l2\u0026#39;), \u0026#34;dark\u0026#34;); var ec_left2_Option = { tooltip: { trigger: \u0026#39;axis\u0026#39;, //指示器  axisPointer: { type: \u0026#39;line\u0026#39;, lineStyle: { color: \u0026#39;#7171C6\u0026#39; } }, }, legend: { data: [\u0026#39;新增确诊\u0026#39;, \u0026#39;新增疑似\u0026#39;], left: \u0026#34;right\u0026#34; }, //标题样式  title: { text: \u0026#34;全国新增趋势\u0026#34;, textStyle: { color:\u0026#39;yellow\u0026#39;, fontSize: 16 }, left: \u0026#39;left\u0026#39; }, //图形位置  grid: { left: \u0026#39;4%\u0026#39;, right: \u0026#39;6%\u0026#39;, bottom: \u0026#39;4%\u0026#39;, top: 50, containLabel: true }, xAxis: [{ type: \u0026#39;category\u0026#39;, //x轴坐标点开始与结束点位置都不在最边缘  // boundaryGap : true,  data: [] }], yAxis: [{ type: \u0026#39;value\u0026#39;, //y轴字体设置  //y轴线设置显示  axisLine: { show: true }, axisLabel: { show: true, color: \u0026#39;white\u0026#39;, fontSize: 12, formatter: function(value) { if (value \u0026gt;= 1000) { value = value / 1000 + \u0026#39;k\u0026#39;; } return value; } }, //与x轴平行的线样式  splitLine: { show: true, lineStyle: { color: \u0026#39;#17273B\u0026#39;, width: 1, type: \u0026#39;solid\u0026#39;, } } }], series: [{ name: \u0026#34;新增确诊\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }, { name: \u0026#34;新增疑似\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }] }; ec_left2.setOption(ec_left2_Option)   ec_right1.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  var ec_right1=echarts.init(document.getElementById(\u0026#39;r1\u0026#39;),\u0026#34;dark\u0026#34;); var ec_right1_Option={ title:{ text:\u0026#34;非湖北地区城市确诊TOP5\u0026#34;, textStyle:{ color:\u0026#39;white\u0026#39;, }, left:\u0026#39;left\u0026#39;, }, color:[\u0026#39;#3398DB\u0026#39;], tooltip:{ trigger:\u0026#39;axis\u0026#39;, axisPointer:{ type:\u0026#39;shadow\u0026#39;, } }, xAxis:{ type:\u0026#39;category\u0026#39;, data:[] }, yAxis:{ type:\u0026#39;value\u0026#39;, }, series:[{ data:[], type:\u0026#39;bar\u0026#39;, barMaxWidth:\u0026#34;50%\u0026#34; }] }; ec_right1.setOption(ec_right1_Option)   ","permalink":"https://hanson00.github.io/posts/technology/python/flask%E7%96%AB%E6%83%85%E5%AE%9E%E6%88%98/","summary":"Python Flask的项目实战 项目概述：使用Flask将爬虫爬取到的数据结合echarts可视化的展示到页面(Python+Flask+Echart","title":"Flask疫情实战"},{"content":"Python编码 编码浅谈  软件运行前（代码运行前）软件和代码等数据存放于硬盘中 软件运行时（代码运行时）将代码等数据从硬盘中读取到内存中【内存中的数据以Unicode方式存储】 软件运行时产生的数据存在于内存中，想要永久保存，就需要把内存中的数据保存到硬盘中  文件出现乱码的原因：\n 存错了 取错了  Python编码与爬虫 Python3在内存中使用的是Unicode编码方式存储的，所以不能直接存储和传输，必须要转换成其他编码进行存储和传输。\nPython的编码解码过程：源文件===》encode(编码方式)===》decode(解码方式) 。即字符串通过编码转换成字节码(str-\u0026gt;bytes)，字节码通过解码转换成字符串(bytes-\u0026gt;str) 如果在爬虫遇到返回文件不正确时，建议使用encode和decode的方式来处理文本。\n我们在使用requests的时候Python会自动猜测源文件的编码方式，然后根据猜测的编码方式将源文件转换成Unicode编码，而爬虫中出现乱码也就是Python猜测编码方式错误，所以我们需要手动来指定编码方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import sys import locale # 返回当前系统所使用的默认字符编码 print(sys.getdefaultencoding()) # 返回用于转换Unicode文件名至系统文件名所使用的编码 print(sys.getfilesystemencoding()) # 获取默认的区域设置并返回元祖(语言, 编码) print(locale.getdefaultlocale()) # 返回用户设定的文本数据编码 # 文档提到this function only returns a guess print(locale.getpreferredencoding())   网页一般会告诉你是什么编码方式，而Python默认是使用UTF-8解码，例如我们可以指定编码字符集response.encoding = \u0026quot;GBK\u0026quot;\n  一般可以使用res.text.encode(res.encoding).decode(\u0026quot;GBK\u0026quot;)或者res.text.encode(res.encoding).decode(\u0026quot;UTF-8\u0026quot;)可以解决编码问题   1 2 3 4 5 6 7 8 9 10 11 12 13 14  import requests url = \u0026#39;https://www.dy2018.com/\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\u0026#34; } res = requests.get(url=url, headers=headers) print(res.encoding) # res.encoding = \u0026#34;gb2312\u0026#34; print(res.text.encode(res.encoding).decode(\u0026#34;GBK\u0026#34;)) res.close()    使用res.encoding = \u0026quot;指定的编码方式\u0026quot;再打印即可   1 2 3 4 5 6 7 8 9 10 11 12 13  import requests url = \u0026#39;https://www.dy2018.com/\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\u0026#34; } res = requests.get(url=url, headers=headers) res.encoding = \u0026#34;gb2312\u0026#34; print(res.text) res.close()   ","permalink":"https://hanson00.github.io/posts/technology/python/python%E7%BC%96%E7%A0%81%E6%B5%85%E8%B0%88/","summary":"Python编码 编码浅谈 软件运行前（代码运行前）软件和代码等数据存放于硬盘中 软件运行时（代码运行时）将代码等数据从硬盘中读取到内存中【内存中","title":"Python编码浅谈"},{"content":"Python-Flask学习  微框架、简洁、只做它需要做的，给开发者提供了很大的扩展性。 Flask和相应的插件写得很好，如果实在找不到相应插件，就动手造轮子。 开发效率非常高，比如使用SQLAlchemy的ORM操作数据库可以节省开发者大量书写sql的时间。 默认的Jinija2模板引擎替换成其他模板引擎都是非常方便的。 开发项目时的命名尽量避开Python自带的命名   一般普通开发者的项目结构如下： 外置数据库配置文件是为了防止导包循环\n配置文件 官方配置文档：https://dormousehole.readthedocs.io/en/latest/config.html\n配置项文件里面都是变量名大写\n视图函数(路由函数)转URL 知道视图函数(路由函数)从而寻找对应的URL\n使用url_for(函数名,对应的URL实现规则)实现\n页面跳转和重定向  参数传递的两种方式：\n 作为url的组成部分如/book/1 查询字符串如/book?id=1使用request.args.get获取参数   1 2 3 4 5 6 7 8 9  from flask import redirect @app.route(\u0026#34;/profile\u0026#34;) def tttsss(): uid = request.args.get(\u0026#34;id\u0026#34;) if uid: return \u0026#34;用户验证成功\u0026#34; else: return redirect(url_for(\u0026#34;index\u0026#34;))   模板的使用 使用render_template来对模板进行使用from flask import render_template，如果想传递变量到模板则通过变量定义为字典以关键字参数的方式传递render_template(\u0026quot;about.html\u0026quot;, **context)\n知识点：\n  模板过滤器\n  if语句\n  1 2 3 4 5  {% if age \u0026gt;= 18 %} \u0026lt;div\u0026gt;成年\u0026lt;/div\u0026gt; {% if age \u0026lt; 18 %} \u0026lt;div\u0026gt;没成年\u0026lt;/div\u0026gt; {% endif %}       for语句\n  1 2 3 4  @app.route(\u0026#34;/about\u0026#34;) def about(): context = {\u0026#34;username\u0026#34;:\u0026#34;温铭军\u0026#34;} return render_template(\u0026#34;about.html\u0026#34;, **context)    静态文件的使用\n  1  \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#34;static\u0026#34;, filename=\u0026#34;css/about.css\u0026#34;) }}\u0026#34;\u0026gt;     1 2 3 4 5  在蓝图中使用静态文件需要使用蓝图名+.+static来引用 \u0026lt;link href=\u0026#34;{{ url_for(\u0026#39;admin.static\u0026#39;,filename=\u0026#39;about.css\u0026#39;) }}\u0026#34;\u0026gt;       模板的继承 base.html\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;{%block title %}{% endblock %}\u0026lt;/title\u0026gt; {% block head %}{% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %}{% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   about.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  {% extends \u0026#34;base.html\u0026#34; %} {% block title %} 我是关于页面，要善用block+tab生成句子 {% endblock %} {% block head %} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#34;static\u0026#34;, filename=\u0026#34;css/about.css\u0026#34;) }}\u0026#34;\u0026gt; {% endblock %} {% block body %} 这里是{{ username|length }} \u0026lt;h1\u0026gt;温铭军爱学习\u0026lt;/h1\u0026gt; {% endblock %}   蓝图 app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from flask import Flask from apps.book import bp as bookbp from apps.course import bp as coursebp from apps.user import bp as userbp app = Flask(__name__) # 注册蓝图 app.register_blueprint(bookbp) app.register_blueprint(coursebp) app.register_blueprint(userbp) @app.route(\u0026#34;/\u0026#34;) def index(): return \u0026#34;hello\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   1 2 3 4 5 6 7 8 9 10 11 12 13  # coding:utf-8 # @Time : 2022/7/13 20:22 # @Author : 软件1194温铭军 # @file : course.py # $software : PyCharm from flask import Blueprint # 类似于创建app bp = Blueprint(\u0026#34;course\u0026#34;, __name__, url_prefix=\u0026#34;/course\u0026#34;) @bp.route(\u0026#34;/list\u0026#34;) def courselist(): return \u0026#34;课程列表\u0026#34;   Flask连接数据库 1.0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   数据库ORM映射  一对多\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[\u0026#34;SQLALCHEMY_TRACK_MODIFICATIONS\u0026#34;] = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) class User(db.Model): __tablename__ = \u0026#34;user\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) username = db.Column(db.String(20), nullable=False) # 定义ORM模型 class Aticle(db.Model): __tablename__ = \u0026#34;article\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) title = db.Column(db.String(60), nullable=False) content = db.Column(db.Text, nullable=False) # 一对多 # 外键的数据类型要看所引用的字段类型，db.ForeignKey(\u0026#34;表名.字段名\u0026#34;) # 外键是属于数据库层面，不推荐在ORM中直接使用 author_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # ORM绑定关系db.relationship(\u0026#34;所创建的数据库类模型字段class 名\u0026#34;) # 第一个参数是模型的名字，backref代表反向引用代表对方访问我的时候的名称字段 author = db.relationship(\u0026#34;User\u0026#34;, backref=\u0026#34;aticles\u0026#34;) # 将模型映射到数据库 db.drop_all() db.create_all() @app.route(\u0026#34;/onetomany\u0026#34;) def otm(): article = Aticle(title=\u0026#34;555\u0026#34;, content=\u0026#34;文章\u0026#34;) user = User(username=\u0026#34;温铭军\u0026#34;) article.author = user db.session.add(article) db.session.commit() print(user.aticles)\t#上面的反向引用名backref return \u0026#34;数据保存成功otm\u0026#34; @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; @app.route(\u0026#34;/article\u0026#34;) def addarticle(): # # 添加数据 # # 创建数据对象 # article = Aticle(title=\u0026#34;温铭军爱学习\u0026#34;, content=\u0026#34;6666\u0026#34;) # db.session.add(article) # db.session.commit() # return \u0026#34;数据添加成功\u0026#34; # # 查询数据 # article = Aticle.query.filter_by(id=1)[0] # print(article.title) # return \u0026#34;数据查询完成\u0026#34; # # 修改数据 # article = Aticle.query.filter_by(id=1)[0] # article.content = \u0026#34;修改后真的强\u0026#34; # db.session.commit() # return \u0026#34;数据修改成功\u0026#34; # 删除数据 Aticle.query.filter_by(id=1).delete() db.session.commit() return \u0026#34;数据删除成功\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()    一对一表的实现\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[\u0026#34;SQLALCHEMY_TRACK_MODIFICATIONS\u0026#34;] = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) class UserExtension(db.Model): __tablename__ = \u0026#34;user_extension\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) school = db.Column(db.String(20)) user_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # 一对一引用 db.backref在反向引用时需要传递一些其他参数否则不需要使用 # uselist=False 表示在代表反向引用时使用的是一个对象不是一个列表 user = db.relationship(\u0026#34;User\u0026#34;, backref=db.backref(\u0026#34;extension\u0026#34;, uselist=False)) class User(db.Model): __tablename__ = \u0026#34;user\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) username = db.Column(db.String(20), nullable=False) # 定义ORM模型 class Aticle(db.Model): __tablename__ = \u0026#34;article\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) title = db.Column(db.String(60), nullable=False) content = db.Column(db.Text, nullable=False) # 一对多 # 外键的数据类型要看所引用的字段类型，db.ForeignKey(\u0026#34;表名.字段名\u0026#34;) # 外键是属于数据库层面，不推荐在ORM中直接使用 author_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # ORM绑定关系db.relationship(\u0026#34;所创建的数据库类模型字段class 名\u0026#34;) # 第一个参数是模型的名字，backref代表反向引用代表对方访问我的时候的名称字段 author = db.relationship(\u0026#34;User\u0026#34;, backref=\u0026#34;aticles\u0026#34;) # 将模型映射到数据库 db.drop_all() db.create_all() @app.route(\u0026#34;/onetomany\u0026#34;) def otm(): article1 = Aticle(title=\u0026#34;555\u0026#34;, content=\u0026#34;文章\u0026#34;) article2 = Aticle(title=\u0026#34;2222\u0026#34;, content=\u0026#34;文章222\u0026#34;) user = User(username=\u0026#34;温铭军\u0026#34;) article1.author = user article2.author = user db.session.add(article1) db.session.add(article2) db.session.commit() # 反向引用 print(user.aticles) return \u0026#34;数据保存成功otm\u0026#34; @app.route(\u0026#34;/onetoone\u0026#34;) def oto(): user = User(username=\u0026#34;wmj\u0026#34;) extension = UserExtension(school=\u0026#34;北京大学\u0026#34;) user.extension = extension db.session.add(user) db.session.commit() return \u0026#34;一对一\u0026#34; @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; @app.route(\u0026#34;/article\u0026#34;) def addarticle(): # # 添加数据 # # 创建数据对象 # article = Aticle(title=\u0026#34;温铭军爱学习\u0026#34;, content=\u0026#34;6666\u0026#34;) # db.session.add(article) #添加数据到数据库 # db.session.commit() # return \u0026#34;数据添加成功\u0026#34; # # 查询数据 # article = Aticle.query.filter_by(id=1)[0] # print(article.title) # return \u0026#34;数据查询完成\u0026#34; # # 修改数据 # article = Aticle.query.filter_by(id=1)[0] # article.content = \u0026#34;修改后真的强\u0026#34; # db.session.commit() # return \u0026#34;数据修改成功\u0026#34; # 删除数据 Aticle.query.filter_by(id=1).delete() db.session.commit() return \u0026#34;数据删除成功\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   灵活管理ORM与数据库表 不用使用db.create_all()、db.drop_all(),使用前面语句的弊端模型新增或删除字段，只会在第一次生成表的时候处理\n1  from flask_migrate import Migrate   1  migrate = Migrate(app, db)    flask db init flask db migrate -m \u0026ldquo;first commit\u0026rdquo; flask db upgrade  初始化后，后面更新数据库后只需要执行后面两句\nflask中使用cookie和session  cookies：在Flask中操作cookie是通过response对象来操作，可以在response回之前，通过response.set_cookie来设置，这个方法有以下几个参数需要注意：  key：设置的cookie的key。 value：key对应的value。 max_age：改cookie的过期时间，如果不设置，则浏览器关闭后就会自动过期。 expires：过期时间，应该是一个datetime类型。 domain：该cookie在哪个域名中有效。一般设置子域名，比如cms.example.com。 path：该cookie在哪个路径下有效。   session：Flask中的session是通过from flask import session。然后添加值key和value进去即可。并且，Flask中的session机制是将session信息加密，然后存储在cookie中。专业术语叫做client side session。  Flask-WTF表单验证 flask表单验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  from flask import Flask,request,render_template from exts import db from flask_migrate import Migrate import config1 from models import Aticle, User, UserExtension from froms import LoginForm #导入表单 app = Flask(__name__) app.config.from_object(config1) # 绑定app db.init_app(app) migrate = Migrate(app, db) @app.route(\u0026#34;/login\u0026#34;, methods=[\u0026#34;POST\u0026#34;, \u0026#34;GET\u0026#34;]) def login(): if request.method == \u0026#34;GET\u0026#34;: return render_template(\u0026#34;login.html\u0026#34;) else: form = LoginForm(request.form) if form.validate(): return \u0026#34;登录成功\u0026#34; else: return \u0026#34;邮箱或密码失败\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()    validators:验证器\n 1 2 3 4 5 6 7 8 9 10 11  # coding:utf-8 # @Time : 2022/7/14 12:04 # @Author : 软件1194温铭军 # @file : froms.py # $software : PyCharm import wtforms from wtforms.validators import length, email class LoginForm(wtforms.Form): email = wtforms.StringField(validators=[length(min=5, max=20), email()]) password = wtforms.StringField(validators=[length(min=6, max=15)])   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;登录\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;form action=\u0026#34;/login\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;邮箱：\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;密码：\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;提交\u0026#34;\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   项目重构例子 app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  from flask import Flask, url_for import config # 导入配置文件 from exts import db,mail from blueprints.qa import bp as qa_bp from blueprints.user import bp as user_bp from flask_migrate import Migrate # from models import EmailModel app = Flask(__name__) # 绑定config配置 app.config.from_object(config) # 初始化，绑定数据库 db.init_app(app) mail.init_app(app) migrate = Migrate(app, db) app.register_blueprint(qa_bp) app.register_blueprint(user_bp) if __name__ == \u0026#39;__main__\u0026#39;: app.run()   models.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # coding:utf-8 # @Time : 2022/7/14 22:46 # @Author : 软件1194温铭军 # @file : models.py # $software : PyCharm from exts import db from datetime import datetime class EmailModel(db.Model): __tablename__ = \u0026#34;emailmodel\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) email = db.Column(db.String(20), nullable=False, unique=True) captcha = db.Column(db.String(10), nullable=False) nowtimes = db.Column(db.DateTime, default=datetime.now)   1 2 3 4 5 6 7 8 9  # coding:utf-8 # @Time : 2022/7/14 13:33 # @Author : 软件1194温铭军 # @file : exts.py # $software : PyCharm from flask_sqlalchemy import SQLAlchemy from flask_mail import Mail db = SQLAlchemy() mail = Mail()   config1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # coding:utf-8 # @Time : 2022/7/12 10:38 # @Author : 软件1194温铭军 # @file : config.py # $software : PyCharm JSON_AS_ASCII = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;flaskpj\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; SQLALCHEMY_DATABASE_URI = DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = True SECRET_KEY = \u0026#34;hanson2000\u0026#34; # 邮箱配置 QQ MAIL_SERVER = \u0026#34;smtp.qq.com\u0026#34; MAIL_PORT = 465 MAIL_USE_TLS = False MAIL_USE_SSL = True MAIL_DEBUG = True # 正式上线用False MAIL_USERNAME = \u0026#34;954952920@qq.com\u0026#34; MAIL_PASSWORD = \u0026#34;bmyrgvagcitvbfdf\u0026#34; MAIL_DEFAULT_SENDER = \u0026#34;954952920@qq.com\u0026#34;   Flask的小点 1  from flask import jsonify\t# 将列表数据转换成json数据返回   如果有一个列表，想要在路由函数中返回该数据则可以用return jsonify(该列表)\n1 2 3 4 5 6 7 8 9  #如果不想定制子路径来传递参数，也可以通过传统的?=的形式来传递参数，例如：/article?id=xxx，这种情况下，可以通过request.args.get(\u0026#39;id\u0026#39;)来获取id的值。如果是post方法，则可以通过request.form.get(\u0026#39;id\u0026#39;)来进行获取。 @app.route(\u0026#34;/book/\u0026lt;int:bookid\u0026gt;\u0026#34;, methods=[\u0026#34;POST\u0026#34;,\u0026#34;GET\u0026#34;]) def onebook(bookid): for boo in books1: if bookid == boo[\u0026#34;id\u0026#34;]: return boo return f\u0026#34;{bookid}is not found\u0026#34;   1 2 3 4 5 6 7  @app.route(\u0026#34;/profile\u0026#34;) def tttsss(): uid = request.args.get(\u0026#34;id\u0026#34;) if uid: return \u0026#34;用户验证成功\u0026#34; else: return redirect(url_for(\u0026#34;index\u0026#34;))   ","permalink":"https://hanson00.github.io/posts/technology/python/flask/","summary":"Python-Flask学习 微框架、简洁、只做它需要做的，给开发者提供了很大的扩展性。 Flask和相应的插件写得很好，如果实在找不到相应插件","title":"Python-Flask"},{"content":"Python-PEP8代码风格指南摘要  PEP8代码风格指南仅仅只是提供建议，最重要是保持与项目内部保持一致性，有时在遇到与代码风格不适用的地方要根据自己的最佳判断进行决定。\n下面给出一些可以忽略代码风格的一些理由：\n 应用代码风格指南时会使代码可读性降低时，不应使用 与历史代码(周围代码)的代码风格不一致时   代码布局 缩进 续行应该使用 Python 的隐式行在括号、方括号和大括号内连接，或者使用悬挂缩进 垂直对齐包裹的元素。使用悬挂缩进时，应考虑以下事项；第一行不应该有任何参数，并且应该使用进一步的缩进来清楚地将自己区分为续行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # 正确的用法: # 与开始分隔符对齐。 foo = long_function_name(var_one, var_two, var_three, var_four) # 添加 4 个空格（额外的缩进级别）以将参数与函数体区分开来。 def long_function_name( var_one, var_two, var_three, var_four): print(var_one) # 悬挂缩进应该增加一个层次。 foo = long_function_name( var_one, var_two, var_three, var_four) # 悬挂缩进*可能*缩进到 4 个空格以外。 foo = long_function_name( var_one, var_two, var_three, var_four)   空格是首选的缩进方法 最大行长  将所有行限制为最多 79 个字符。\n对于结构限制较少的长文本块（文档字符串或注释），行长应限制为 72 个字符。\n包装长行的首选方法是在括号、方括号和大括号内使用 Python 的隐含行继续。通过将表达式括在括号中，可以将长行分成多行。这些应该优先使用反斜杠来继续行。\n 在二元运算符之前换行 1 2 3 4 5 6  #正确的： income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest)   import语句 1 2 3 4  #正确的： import os import sys from flask import Flask,jsonity,request    导入总是放在文件的顶部，就在任何模块注释和文档字符串之后，模块全局变量和常量之前。\n进口应按以下顺序分组：\n 标准库导入。 相关第三方进口。 本地应用程序/库特定的导入。   表达式和语句中的空格 在下面的情况应该避免多余的空格：\n 紧接在圆括号，方括号或大括号内：   1 2 3 4 5  # 正确的: spam(ham[1], {eggs: 2}) # 错误的: spam( ham[ 1 ], { eggs: 2 } )    在结尾的逗号和后面的右括号之间 紧接在逗号、分号或冒号之前 在切片中   1 2 3 4 5 6  # 正确的: ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:] ham[lower:upper], ham[lower:upper:], ham[lower::step] ham[lower+offset : upper+offset] ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)] ham[lower + offset : upper + offset]     始终在这些二元运算符的两边加上一个空格：赋值 ( =)、扩充赋值 (+=等-= )、比较 ( ==, \u0026lt;, \u0026gt;, !=, \u0026lt;\u0026gt;, \u0026lt;=, \u0026gt;=, in, , , ) 布尔值 ( , , )。\n  如果使用具有不同优先级的运算符，请考虑在具有最低优先级的运算符周围添加空格。使用您自己的判断；但是，永远不要使用多个空格，并且在二元运算符的两侧总是有相同数量的空格：\n 1 2 3 4 5 6  # 正确的: i = i + 1 submitted += 1 x = x*2 - 1 hypot2 = x*x + y*y c = (a+b) * (a-b)      函数注释应该使用冒号的常规规则，并且-\u0026gt;如果存在，箭头周围总是有空格。\n 1 2  # 正确的: def munge() -\u0026gt; PosInt: ...      =当用于指示关键字参数或用于指示未 注释函数参数的默认值时，请勿在符号周围使用空格：\n 1 2 3  # 正确的: def complex(real, imag=0.0): return magic(r=real, i=imag)      注释 与代码相矛盾的注释比没有注释更糟糕。当代码更改时，请始终优先更新评论！\n块注释通常由一个或多个由完整句子组成的段落组成，每个句子都以句点结尾。\n 块注释通常适用于跟随它们的代码，并且缩进到与该代码相同的级别。块注释的每一行都以一个#和一个空格开头。\n谨慎使用内联注释。内联注释是与语句在同一行上的注释。内联注释应与语句至少间隔两个空格。它们应该以 # 和一个空格开头。内联注释是不必要的，如果它们陈述明显的话，实际上会分散注意力。\n 命名约定  Python 库的命名约定有点混乱，所以我们永远不会完全一致——尽管如此，这里是目前推荐的命名标准。应该按照这些标准编写新的模块和包（包括第三方框架），但如果现有库具有不同的风格，则首选内部一致性。\n 至高原则  作为 API 的公共部分对用户可见的名称应遵循反映使用而非实现的约定。\n 描述性:命名样式 通常区分以下命名样式：\n  b（单个小写字母）\n  B（单个大写字母）\n  lowercase\n  lower_case_with_underscores\n  UPPERCASE\n  UPPER_CASE_WITH_UNDERSCORES\n  CapitalizedWords（或 CapWords 或 CamelCase——因其字母的凹凸外观而得名）。这有时也称为StudlyCaps。\n注意：在 CapWords 中使用首字母缩写词时，首字母缩写词的所有字母大写。因此 HTTPServerError 比 HttpServerError 好。\n  mixedCase（与 CapitalizedWords 的不同之处在于初始小写字符！）\n  Capitalized_Words_With_Underscores（丑陋的！）\n  还有一种使用简短的唯一前缀将相关名称组合在一起的风格。这在 Python 中使用不多，但为了完整性而提及。其项传统上具有st_mode、 st_size等名称st_mtime。（这样做是助于程序员熟悉它。）\n规定性:命名约定  注意避免的命名   切勿使用字符“l”（小写字母 el）、“O”（大写字母 oh）或“I”（大写字母 eye）作为单字符变量名。\n在某些字体中，这些字符与数字 1 和 0 无法区分。当想使用“l”时，请改用“L”。\n 包和模块   模块应该有简短的全小写名称。如果提高可读性，可以在模块名称中使用下划线。Python 包也应该有简短的全小写名称，尽管不鼓励使用下划线。\n当用 C 或 C++ 编写的扩展模块附带提供更高级别（例如更面向对象）接口的 Python 模块时，C/C++ 模块具有前导下划线（例如_socket）。\n 类名   类名一般首字母大写\n 函数名和变量名   函数名称应为小写，必要时用下划线分隔单词以提高可读性。\n变量名遵循与函数名相同的约定。\n 函数和方法参数   始终self用于实例方法的第一个参数。\n始终cls用于类方法的第一个参数。\n 方法名称和实例变量   使用函数命名规则：小写单词，必要时用下划线分隔以提高可读性。\n 常数   常量通常在模块级别定义，并以全大写字母书写，并用下划线分隔单词。\n 编程建议 None的建议 1 2  # 正确的: if foo is not None:   1 2  # 错误的: if not foo is None:   函数返回 在返回语句中保持一致。函数中的所有 return 语句都应该返回一个表达式，或者它们都不应该返回。如果任何 return 语句返回一个表达式，则任何没有返回值的 return 语句都应该明确地将 this 声明为，并且明确的 return 语句应该出现在函数的末尾（如果可访问）：return None\n1 2 3 4 5 6 7 8 9 10 11 12  # Correct: def foo(x): if x \u0026gt;= 0: return math.sqrt(x) else: return None def bar(x): if x \u0026lt; 0: return None return math.sqrt(x)     对象类型比较应始终使用 isinstance() 而不是直接比较类型：\n1 2  # 正确的: if isinstance(obj, int):   1 2  # 错误的: if type(obj) is type(1):     对于序列（字符串、列表、元组），使用空序列为假的事实：\n1 2 3  # 正确的: if not seq: if seq:   1 2 3  # 错误的: if len(seq): if not len(seq):     编程注释建议待续\u0026hellip;. 注意：Python 中没有属性是真正私有的。 ","permalink":"https://hanson00.github.io/posts/technology/python/python_pep8/","summary":"Python-PEP8代码风格指南摘要 PEP8代码风格指南仅仅只是提供建议，最重要是保持与项目内部保持一致性，有时在遇到与代码风格不适用的地","title":"Python_PEP8"},{"content":"Python数组篇 关于如何使用Python统计多维数组的行和列的长度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 创建多维数组的方法 # row ：行，col：列 row = 3 col = 4 list1 = [[0] * col for row in range(row)] print(list1) # 分别统计多维数组的行和列的长度 # 创建了一个4行3列的数组 alist = [[8, 8, 8], [8, 8, 8], [8, 8, 8], [8, 8, 8]] # 在打印时如果 len(数组名) 这就是打印多维数组的行的长度 # 使用 len(数组名[0]) 就是打印列的长度 for row in range(len(alist)): print(row, end= \u0026#34; \u0026#34;) #output：0 1 2 3 print(\u0026#34;\\n-------------\u0026#34;) for col in range(len(alist[0])): print(col, end= \u0026#34; \u0026#34;) #output：0 1 2   ","permalink":"https://hanson00.github.io/posts/technology/python/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B901/","summary":"Python数组篇 关于如何使用Python统计多维数组的行和列的长度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 创建多维数组的方法 # row ：行，c","title":"Python小知识点01"},{"content":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下\n UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence\n 尝试1：在不断尝试了很多种网页编码问题后仍然没有解决\n之后让别的小伙伴尝试了同样的代码，别人可以运行但自己无法运行，问题关键在于解决编码和解码问题。通常这种问题是由于编译器编码出现问题\n解决方法： 针对编码问题，主要从两个方面进行出发，其一是网页编码，其二是Pycharm编码，顺利解决这个小bug。\n解决步骤如下图：\nselenium浏览器打开后闪退  selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 97 Current browser version is 103.0.5060.114 with binary path C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n  selenium.common.exceptions.SessionNotCreatedException：消息：会话未创建：此版本的 ChromeDriver 仅支持 Chrome 版本 97 当前浏览器版本为 103.0.5060.114\n **解决方法：**重新安装浏览器驱动即可解决\n问题：关于Scrapy框架 问题： Signature of method \u0026lsquo;QiubaiSpider.parse()\u0026rsquo; does not match signature of base method in class \u0026lsquo;Spider\u0026rsquo;\n问题描述： 方法 \u0026lsquo;BaiduSpider.parse()\u0026rsquo; 的签名与类 \u0026lsquo;Spider\u0026rsquo; 中基方法的签名不匹配\n问题状态： 已解决\n解决方法： 将Scrapy生成的parse()方法的参数后面添加**kwargs即可 即将方法修改为\n1 2  def parse(self, response, **kwargs): pass   ","permalink":"https://hanson00.github.io/posts/thinking/%E7%88%AC%E8%99%AB%E8%A7%81%E9%97%BB%E5%BD%95/","summary":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下 UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 尝试1：在不断尝试了很多种网页","title":"爬虫见闻录"},{"content":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档\npandas官方文档\n以下代码是简单的为单元格赋值操作\n1 2 3 4 5 6 7 8 9 10 11 12 13  import win32com.client from win32com.client import Dispatch xlapp = win32com.client.Dispatch(\u0026#34;Excel.Application\u0026#34;) #创建一个Excel程序 xlapp.DisplayAlerts = False #取消弹窗 xlapp.Visible = True #显示Excel wb = xlapp.Workbooks.Add() #添加工作簿 wb.Worksheets.Item(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A1\u0026#34;).Value = 2000 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A2\u0026#34;).Value = 100 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).cells(1,3).value = 200 #为单元格赋值 wb.SaveAs(r\u0026#34;C:\\Users\\Lenovo\\Desktop\\jb\\123.xlsx\u0026#34;) #保存工作簿 wb.Close() #关闭工作簿 xlapp.quit() #关闭Excel程序 print(\u0026#34;正常结束\u0026#34;)   1 2 3 4 5 6 7 8 9 10 11 12  from win32com.client import Dispatch # 创建Excel程序 xlapp = Dispatch(\u0026#34;Excel.Application\u0026#34;) # 视为可见 xlapp.Visible = True # 警告信息 xlapp.DisplayAlerts = True # 添加工作簿 wb = xlapp.Workbooks.Add() ws = wb.Sheets(\u0026#34;Sheet1\u0026#34;)\t#或者ws = wb.Sheets.Add()后面这个为新添加一个Sheet # ws2 = wb.Sheets.Add() ws.Range(\u0026#34;A1\u0026#34;).Value = 100    Python\u0026ndash;Word python-docx文档\n Python\u0026ndash;PowerPoint python-pptx文档\n Python\u0026ndash;PDF ","permalink":"https://hanson00.github.io/posts/technology/python/python-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC/","summary":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档 pandas官方文档 以下代码是简单的为单元格赋值操作 1 2 3 4 5 6 7 8 9 10 11 12 13 import","title":"Python 自动化办公"},{"content":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求]\n蛋白质 优质蛋白质\n 鱼类 蛋类 精瘦肉 漆皮肌肉  劣质蛋白质\n 牛腩 较肥的肉  淀粉（锻炼开始和结束可以和蛋白质一起吃） 优质淀粉\n 黑麦面包 红薯 全谷物面包 麸皮食品  劣质淀粉\n 土豆 高糖谷类 白米饭 白面包 干果不要吃太多  饮料 优质\n 花草茶 矿物质水 水 茶  健身周期计划 想要塑造肌肉就要摄入大量的热量\n健身-减脂周期：  先进行6个月的锻炼，同时摄入更多的热量 减脂阶段：在锻炼的途中减脂，控制热量的摄入，全程减脂 在稍微增加热量，增加肌肉 在稍微减少热量，减脂  ","permalink":"https://hanson00.github.io/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%86%E4%BA%AB/%E9%94%BB%E7%82%BC%E5%88%86%E4%BA%AB/","summary":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求] 蛋白质 优质蛋白质 鱼类 蛋类 精瘦肉 漆皮肌肉 劣质蛋白质 牛腩 较肥的肉 淀粉","title":"锻炼分享"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\ncsv模块实现了 CSV 格式表单数据的读写，提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;温铭军\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E6%A8%A1%E5%9D%97/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv模块"},{"content":"Pycharm 常用快捷键  连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ctrl+Alt+T\u0026ndash;添加Try/catch Alt+Shift+上下键\u0026ndash;上下移动 Ctrl+Shift+Enter\u0026ndash;补全代码 Ctrl+R\u0026ndash;替换 Ctrl+Shift+I\u0026ndash;快速查看方法的实现内容 Shift+F1\u0026ndash;查看API文档 Ctrl+Alt+H\u0026ndash;查看那里调用了方法  ","permalink":"https://hanson00.github.io/posts/technology/python/pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/","summary":"Pycharm 常用快捷键 连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ct","title":"Pycharm快捷键"},{"content":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解\n Servlet快速入门  浏览器使用URL访问时所发生的事情  Servlet的方法  根据请求方式的不同进行处理 ServletRequest转换成HttpServletReque才能获取是什么球球方式\n Servlet的运行过程 Servlet程序是由WEB服务器调用，web服务器收到客户端的Servlet访问请求后： ①Web服务器首先检查是否已经装载并创建了该Servlet的实例对象。如果是，则直接执行第④步，否则，执行第②步。 ②装载并创建该Servlet的一个实例对象。 ③调用Servlet实例对象的init()方法。 ④创建一个用于封装HTTP请求消息的HttpServletRequest对象和一个代表HTTP响应消息的HttpServletResponse对象，然后调用Servlet的service()方法并将请求和响应对象作为参数传递进去。 ⑤WEB应用程序被停止或重新启动之前，Servlet引擎将卸载Servlet，并在卸载之前调用Servlet的destroy()方法。\n Request(请求)和Response(响应) request对象里有很多请求数据，可以通过该对象获取请求数据\nresponse返回设置的响应数据\n1.Tomcat需要解析请求数据，封装为request对象，并创建request对象传递到service方法中。\n SqlSessionFactory的优化 1.代码重复每次都要创建一个SqlSessionFactory\n使用静态代码块进行优化\n","permalink":"https://hanson00.github.io/posts/technology/java/04servlet/","summary":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解 Servlet快速入门 浏览器使用UR","title":"04Servlet"},{"content":"MyBatis(持久层/数据访问层)持久层的优秀框架  Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;x.x.x\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   2.编写Mybatis核心配置文件（mybatis-config.xml文件的命名）\u0026ndash;\u0026gt;==替换连接信息，解决硬编码问题==（官网）\n上面的是需要修改的 上面的mappers是指定sql的映射文件的\n\u0026lt;mapper resource=\u0026ldquo;这一段字符串是要替换的sql映射文件(sql语句的文件)的地址\u0026rdquo;\n 3.编写SQL映射文件\u0026ndash;》统一管理sql语句(官网也有)，，命名规则是要操作的表名Mapper.xml\n    编码\n 定义pojo类 加载核心配置文件，获取SQL session factory对象（官网）   获取sqlsession对象，执行sql语句（下面的Mapper代理开发可以替换）   释放资源    Mapper代理开发 !1.定义与SQL映射文件同名的Mapper接口，并放在同一目录下2.设置namespace为全限名3.接口的方法名是要与id的名称相同，并且返回值要与编码相同\nMabatis的具体使用(三步走:编写接口方法，编写sql，执行方法)  编写接口方法Mapper接口 生成相应的sql映射文件  2.步骤的图    执行方法\n 数据库的列名和实体类的名称对应不上的时候使用 除了主键外的起别名用 主键起别名用 上面图片的resultMap替换了原先的resultType\n void selectById(int id);\n占位符使用 #{}\n不同的接收参数的方式\n多条件查询(使用if实现)，\n单条件查询(类似于switch语句)\n增删改操作后需要提交事务\n添加-主键返回需要使用useGeneratedKeys和keyProperty\n批量删除接受的数组是要使用注解名的，否则传递不进去 separator分隔符\n","permalink":"https://hanson00.github.io/posts/technology/java/03mybatis/","summary":"MyBatis(持久层/数据访问层)持久层的优秀框架 Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包 1","title":"03MyBatis"},{"content":"maven  提供了一套标准化的项目结构\n提供了一套标准化的项目构建流程（编译，测试，打包，发布）\n提供了一套完整的依赖管理机制\n ","permalink":"https://hanson00.github.io/posts/technology/java/03maven/","summary":"maven 提供了一套标准化的项目结构 提供了一套标准化的项目构建流程（编译，测试，打包，发布） 提供了一套完整的依赖管理机制","title":"03maven"},{"content":"JDBC   注册驱动Class.forName()\n  获取连接DriverManager.getConnecti\n1 2 3 4  String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);     定义sql操作语句\n1  String sql=\u0026#34;update stu set math=10 where id=8\u0026#34;;     获取执行sql的对象Statement XXX = conn.createStatement();\n1  Statement stmt=conn.createStatement();     执行sql\n  处理结果\n  释放资源\n DriverManager  DriverManager(驱动管理类)：  注册驱动DriverManager.registerDriver 获取数据库连接    它下面的registerDriver()注册驱动 getCconnection()获取连接,语法jdbc:mysql://ip地址:端口号/数据库名称【jdbc:mysql://127.0.0.1:3306/DB1】\n  1 2 3 4 5  //获取连接 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);    Connection   Connection(数据库连接对象):\n 获取执行sql的对象  普通执行sql对象 Statement createStatement() 预编译SQL的执行sql,防止sql注入 PreparedStatement preareStatement(sql) 执行存储过程的对象 CallbleStatement prepareCall(sql)     int executeUpdate(sql):这个在下面(即参考回滚事务代码)\nResultSet executeQuery(sql):执行DQL语句，返回值是ResultSet结果集对象\n  管理事务  开启事务：setAutoCommit(boolean autoCommit) 提交事务commit() 回滚事务rollback()    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  try { //开启事务  conn.setAutoCommit(false); int count1=stmt.executeUpdate(sql1);//受影响的行数  //处理受影响的结果  System.out.println(count1); int count2=stmt.executeUpdate(sql2);//受影响的行数  //处理受影响的结果  System.out.println(count2); //提交事务  conn.commit(); } catch (Exception throwables) { //回滚事务  conn.rollback(); throwables.printStackTrace(); }      Statement   Statement作用:\n  执行sql语句\n  int executeUpdate(sql):执行DML、DDL语句【DDL操作数据库，DML对数据进行增删改查】 返回值:1.DML语句受影响的行数2.DDL语句执行后，执行成功也可能返回0\n  ResultSet executeQuery(sql)DQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  //定义sql  String sql = \u0026#34;select * from stu\u0026#34;; //获取执行sql对象,获取Statement对象  Statement stmt = conn.createStatement(); //5.执行sql  ResultSet rs = stmt.executeQuery(sql); //6.处理结果，遍历所有数据  while (rs.next()){ //获取数据  int id = rs.getInt(1); String name = rs.getString(2); double money = rs.getDouble(3); //输出  System.out.println(id); System.out.println(name); System.out.println(money); } //7.释放资源  rs.close(); stmt.close(); conn.close(); }         PreparedStatement作用\n 预防sql注入    ","permalink":"https://hanson00.github.io/posts/technology/java/02jdbc/","summary":"JDBC 注册驱动Class.forName() 获取连接DriverManager.getConnecti 1 2 3 4 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password); 定义sql操作语句","title":"02JDBC"},{"content":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )\n show databases; 查询本地数据库\n  create database 数据库名称; 创建数据库\n  一般采用该语句用于创建数据库\ncreate database if not exists 数据库名称; \n  drop database 数据库名称; 删除数据库\n  一般采用该语句用于删除数据库\ndrop database if exists 数据库名称;\n  use 数据库名称; 使用数据库\n  select database(); 查看当前使用的数据库\n  表操作：\n show tables; 查看当前数据库的表名称\n  desc 表名称; 查询表结构\n  1 2 3 4 5 6 7 8 9 10  --创建表 create table 表名( 列名1,数据类型， 列名2,数据类型， 列名3,数据类型， 列名n,数据类型)；     drop table 表名; 删除表\n  drop table if exists 表名; 删除表并判断是否存在\n  alter table 表名 rename 新表名 修改表名\n  alter table 表名 add 新列名 数据类型; 添加一列\n  alter table 表名 modify 列名 新的数据类型; 修改数据类型\n  alter table 表名 change 列名 新的数据类型; 修改列名和数据类型\n  alter table 表名 drop 列名; 删除列\n  insert into 表名(列名1,列名2,...) values(值1,值2,值3); 给指定的列添加数据\n  insert into 表名 values(值1,值2,值3,...); 给全部列添加数据\n  insert into 表名 values(值1,值2,值3,...),(值1,值2,值3,...),(值1,值2,值3,...); 批量添加数据\n  ==查询语法DQL详细学习见后面自学==\n 条件查询 排序查询 聚合函数 分组查询 分页查询 约束实现 多表查询中的内连接，外连接 多表查询的子查询 事务   ","permalink":"https://hanson00.github.io/posts/technology/java/01mysql/","summary":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )","title":"01mysql"},{"content":"  安装selenium 安装浏览器驱动   1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   1 2 3 4 5 6 7 8 9 10 11 12 13  from selenium import webdriver from selenium.webdriver.common.by import By url = \u0026#34;https://spa2.scrape.center/\u0026#34; browser = webdriver.Chrome() browser.get(url=url) inputs = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;top\u0026#34;]/main/article/header/h1\u0026#39;) attr1 = inputs.get_attribute(\u0026#39;class\u0026#39;) # 获取属性 value1 = inputs.text # 获取文本值 print(attr1, value1) # 下面分别是获取标签名和位置 print(inputs.tag_name, inputs.location)   切换到子窗口  功能代码如下：\n1 2 3 4 5 6  browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0])    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   selenium拖动滚动条  使用JavaScript代码实现拖动具体使用window.scrollTo()方法，通过x，y坐标进行滑动。  语法scrollTo(xpos, ypos)这两个参数都是必须的。 两个坐标位置参考浏览器左上角，参数是偏移左上角的位置 纵向滚动条最后是使用设置height = 0，与document.body.scrollHeight前后位置调换来实现    1 2 3 4 5 6 7 8 9 10 11 12 13 14  from selenium import webdriver import time url = \u0026#34;https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/git%E5%AD%A6%E4%B9%A0/\u0026#34; chrome = webdriver.Chrome() chrome.get(url=url) time.sleep(1) js = \u0026#34;window.scrollTo(0, document.body.scrollHeight)\u0026#34; chrome.execute_script(js) time.sleep(2) chrome.quit()   使用ActionChains()实现滚动条的拖拽  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time from selenium.webdriver.common.action_chains import ActionChains #鼠标操作导包,事件链 #防止被认出是爬虫程序，chrome版本88以上适用 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--disable-blink-features=AutomationControlled\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://kyfw.12306.cn/otn/resources/login.html\u0026#39; browser.get(url) time.sleep(1) #输入账号密码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-userName\u0026#34;]\u0026#39;).send_keys(\u0026#39;你的账号\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-password\u0026#34;]\u0026#39;).send_keys(\u0026#39;密码\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-login\u0026#34;]\u0026#39;).click() time.sleep(2) #拖拽目标按钮 btn = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1_n1z\u0026#34;]\u0026#39;) #找到目标拖拽验证码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1__scale_text\u0026#34;]/span\u0026#39;) #拖拽移动 ActionChains(browser).drag_and_drop_by_offset(btn, 300, 0).perform() #横推拽   execute_script(“document.documentElement.scrollTop= 位置”)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  from selenium import webdriver import time driver = webdriver.Chrome() # 访问网址 driver.get(\u0026#39;https://www.baidu.com/s?ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;rsv_idx=1\u0026amp;tn=baidu\u0026amp;wd=%E5%86%AC%E5%A5%A5%E4%BC%9A\u0026amp;fenlei=256\u0026amp;rsv_pq=c61031d20000416d\u0026amp;rsv_t=34329gsBNwKJFQ%2Bmyc957YqRGPTtQjrdzsvJ%2Fpfwl4Dta6B4YxTZjnzxVE8\u0026amp;rqlang=cn\u0026amp;rsv_enter=1\u0026amp;rsv_dl=tb\u0026amp;rsv_sug3=13\u0026amp;rsv_sug1=15\u0026amp;rsv_sug7=101\u0026amp;rsv_sug2=0\u0026amp;rsv_btype=i\u0026amp;inputT=4248\u0026amp;rsv_sug4=4635\u0026#39;) # 等待3秒 time.sleep(3) # 滚动到底部 driver.execute_script(\u0026#34;document.documentElement.scrollTop=10000\u0026#34;) # 等待1秒 time.sleep(1) # 关闭所有页面 driver.quit()   定位元素实现滑动  一般步骤：\n 获取元素定位通过tagetbtn = find_element() 再滚动到定位的目标位置chrome.execute_script(“arguments[0].scrollIntoView()”, targetbtn)  打开多个窗口 使用window.open(url)\n1 2 3 4 5 6 7 8 9 10 11 12  from selenium import webdriver import time chrome = webdriver.Chrome() chrome.maximize_window() chrome.get(\u0026#34;http://www.baidu.com\u0026#34;) time.sleep(2) js1 = \u0026#39;window.open(\u0026#34;https://www.sogou.com\u0026#34;)\u0026#39; chrome.execute_script(js1) time.sleep(2) chrome.quit()   不同frame间的转换 当网页代码出现无法定位获取的元素时，并且发现iframe的代码时，要进行iframe的定位与切换\n在网页中的iframe节点，相当于页面的子页面需要使用switch_to_frame()来切换到页面的frame中\n1 2  frame_place = chrome.find_element(\u0026#34;iframe的位置\u0026#34;) chrome.switch_to_frame(frame_place)   Selenium的等待 Selenium的等待有以下三种：\n 强制等待time.sleep()等到网页加载完成再等待指定的秒数才能继续往下执行。 隐式等待，在指定等待时间到来前加载完了，直接继续往下执行，如果等待了指定的时间后还未成功打开网页就直接继续往下执行。 显示等待，最为灵活。两个人去玩，其中一人到了目的地，另一人告诉他不来了，他就一个人进去。   隐式等待判断网页是否加载完，但可能网页上某个JavaScript脚本加载速度特别慢，而JavaScript脚本只是个限制脚本，不影响网页的正常浏览，但仍然需要等待全部页面加载完成。\n显示等待只需要判断某个特定元素是否加载出来就可以了。\n 隐式等待如下：隐式等待设置一次就会在整个程序中都起作用\n1 2 3 4 5 6 7 8 9 10 11 12  from selenium import webdriver import time from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys chrome = webdriver.Chrome() url = \u0026#34;https://www.python.org/downloads/\u0026#34; chrome.implicitly_wait(10) chrome.get(url=url) print(chrome.find_element(By.LINK_TEXT,\u0026#34;About\u0026#34;).get_attribute()) chrome.quit()   显示等待：下面列出显示等待所需的库\n from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from selenium import webdriver from selenium.webdriver.support import expected_conditions from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.common.by import By url = \u0026#39;https://www.csdn.net/\u0026#39; chrome = webdriver.Chrome() # 设置隐式等待，等待时间为隐式等待和显示等待时间长的那个 chrome.implicitly_wait(10) chrome.get(url=url) try: WebDriverWait(chrome, 20, 0.5).until(expected_conditions.presence_of_element_located((By.LINK_TEXT, u\u0026#39;博客\u0026#39;))) finally: print(chrome.find_element(By.LINK_TEXT, \u0026#34;博客\u0026#34;).get_attribute(\u0026#39;href\u0026#39;)) chrome.quit()   WebDriverWait()可选择的参数有四个，一般情况下只用前三个，第一个参数是设置要打开的浏览器，第二个参数是设置超时时间，第三个是设置检查频率。\n后面一部分的until(method,message=\u0026quot;\u0026quot;)，message返回的结果是0，即失败后要返回的消息。\n向expected_conditions.presence_of_element_located((元组))\n上面的代码可以同等替换为：\n1 2 3 4 5 6 7  # 定位条件 place = (By.LINK_TEXT, u\u0026#39;博客\u0026#39;) try: WebDriverWait(chrome, 20, 0.5).until(expected_conditions.presence_of_element_located(place)) finally: print(chrome.find_element(By.LINK_TEXT, \u0026#34;博客\u0026#34;).get_attribute(\u0026#39;href\u0026#39;)) chrome.quit()   更多的等待条件的参数以及用法可以参考selenium等待条件文档\n前进后退 前进后退的语法较为简单\n browser.back() browser.forward()  选项卡管理 1 2 3 4 5 6 7 8 9 10 11 12  import time from selenium import webdriver url = \u0026#34;https://spa2.scrape.center/\u0026#34; browser = webdriver.Chrome() browser.get(url) browser.execute_script(\u0026#34;window.open()\u0026#34;) time.sleep(1) browser.switch_to.window(browser.window_handles[1]) browser.get(\u0026#34;https://www.baidu.com\u0026#34;) time.sleep(1) browser.switch_to.window(browser.window_handles[0])   异常处理 使用try except语句捕获各种异常，具体参考官方文档\n项目实战 拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   暂时补充： selenium键盘操作和鼠标操作\n","permalink":"https://hanson00.github.io/posts/technology/python/python-selenium/","summary":"安装selenium 安装浏览器驱动 1 2 3 4 5 6 7 8 from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrom","title":"Python Selenium"},{"content":"Python并发与爬虫 DeprecationWarning:显式地将协程对象传递给asyncio.wait()自Python 3.8以来已弃用，并计划在Python 3.11中删除。\n进程与线程 每个主进程中至少都会有一个线程，当你创建一个新的线程时，就相当于有两个员工(线程)在工作具体的工作顺序由CPU决定，主线程会继续往下面执行，子线程也会自己执行自己的任务\n在Python中一般不创建多进程，因为进程消耗的内存资源较大\n异步爬虫的方式：\n 多线程，多进程（不建议）：弊端，无法无限制的开启多线程或多进程，使用时会频繁销毁线程或进程 线程池，进程池：可以降低系统销毁线程或进程的频率。弊端是池中的线程和进程有限（在适当轻微的使用时）  线程执行案例 t.start()，当前线程准备就绪，等待CPU调度 t.join()等待当前任务执行完成，当前任务执行完成之后才会往后面执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # import threading # # # 线程如果想传参一定要通过arg=(元组的形式进行传递) # def func(name): # for i in range(1000): # print(name, i) # # if __name__ == \u0026#34;__main__\u0026#34;: # t1 = threading.Thread(target=func, args=(\u0026#34;Rose\u0026#34;,) ) # t1.start() # t2 = threading.Thread(target=func, args=(\u0026#34;你太牛\u0026#34;,) ) # t2.start() # for i in range(1000): # print(\u0026#34;main\u0026#34;, i) from threading import Thread class Mythread(Thread): def run(self): #固定写法 for i in range(1000): print(self.name, i) if __name__ == \u0026#34;__main__\u0026#34;: t1 = Mythread() t1.start() t2 = Mythread() t2.start() for i in range(1000): print(\u0026#34;main\u0026#34;, i) #Thread-2 main984  #Thread-2867  #985main  #Thread-2 868 #main986 #Thread-2 869 #main 870 #main987 #Thread-2 988 871 #Thread-2  #main989  #872 #Thread-2 main 873990 #Thread-2 #main 991 #Thread-2 992 #874Thread-2 # main 875993 #Thread-2 main994 #Thread-2 995 876   进程执行案例 1 2 3 4 5 6 7 8 9 10 11  from multiprocessing import Process def func(): for i in range(100000): print(\u0026#34;函数里\u0026#34;, i) if __name__ == \u0026#39;__main__\u0026#39;: t = Process(target=func) t.start() for i in range(100000): print(\u0026#34;main\u0026#34;, i)    线程池：一次性创建一批线程，然后我们用户把任务分配给线程池，然后线程池分配任务给线程池里面的线程\n1 2 3 4 5 6 7 8 9 10 11  from concurrent.futures import ThreadPoolExecutor def func(name): for i in range(100): print(name, i) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(50) as t: for i in range(10): t.submit(func, name=f\u0026#34;线程{i}\u0026#34;) print(\u0026#34;OVER\u0026#34;)    北京新发地爬虫与多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  # coding:utf-8 # @Time : 2022/5/17 14:41 # @Author : 软件1194温铭军 # @file : 北京新发地多线程.py # $software : PyCharm import requests from concurrent.futures import ThreadPoolExecutor import csv url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def getdata(data): url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; response = requests.post(url, headers=header, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;, \u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(20) as t: for i in range(1, 60): data = { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: f\u0026#39;{i}\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } t.submit(getdata, data) print(\u0026#34;OVER\u0026#34;)   协程(多任务异步协程) 协程是在单线程的基础上操作（多线程协作可以充分地利用CPU）\n 切换条件：当程序遇到IO操作时可以选择切换到其他任务(计算类型的任务)上\n在微观上一个任务一个任务进行切换，切换条件是遇到IO操作(单线程的多个任务来回切换运行)\n在宏观上就是多个任务共同运行\n 会使程序陷入阻塞状态的有：IO操作和requests.get()在网络请求返回前都会处于阻塞状态\n 协程分配的任务是由人来调度，线程池是由系统来调度的\n 异步任务和同步任务看是否需要等待某一操作\n在编写协程代码时要注意：await(挂起)要写到async的函数里面，放在协程对象里面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #协程的编写 import time import asyncio async def func1(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱我们\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作,time.sleep()是串形同步操作，使得异步中断了 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱你\u0026#34;) async def func2(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱22\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱22\u0026#34;) async def func3(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱33\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(4) #改成这个异步操作代码就可以实现协程的异步处理 print(\u0026#34;这是第二次我33\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: #input()也会让CPU处于阻塞状态 #requests.get()也会使程序处于阻塞状态 #当程序处于IO操作时程序会陷入阻塞状态 #协程：当程序遇到IO操作时，可以选择性的切换到其他任务 #如果执行函数就会得到一个协程对象 #协程函数的运行需要asyncio模块的支持 t1 = time.time() f1 = func1() f2 = func2() f3 = func3() task = [f1, f2, f3] #一次性启动多个任务（协程） #多任务同时启动就需要asyncio.wait,把多个任务交给asyncio.wait(),而启动就需要asyncio.run() asyncio.run(asyncio.wait(task)) t2 = time.time() print(t2-t1)   上面代码是同步操作的协程\n下面代码是加入异步的协程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import asyncio import time async def func1(): print(\u0026#34;我是第一\u0026#34;) await asyncio.sleep(3) print(\u0026#34;我是第一\u0026#34;) async def func2(): print(\u0026#34;我是第2\u0026#34;) await asyncio.sleep(2) print(\u0026#34;我是第2\u0026#34;) async def func3(): print(\u0026#34;我是第3\u0026#34;) await asyncio.sleep(4) print(\u0026#34;我是第3\u0026#34;) async def main(): task = [] task = [asyncio.create_task(func1()), asyncio.create_task(func2()), asyncio.create_task(func3())] await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1) #output:我是第一 # 我是第2 # 我是第3 # 我是第2 # 我是第一 # 我是第3 # 4.009669780731201   使用异步协程爬取图片\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # coding:utf-8 # @Time : 2022/5/18 15:41 # @Author : 软件1194温铭军 # @file : aiohttp_umei.py # $software : PyCharm #aiohttp异步爬取优美图库 # requests.get()该操作是同步操作，如果想要 # 使用异步操作的话，就得使用aiohttp import asyncio import aiohttp import time urls = [ \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/zkc0inje5x0.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/ufmo0xczdbg.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/15dujfems5v.jpg\u0026#34; ] async def downsave(url): name = url.split(\u0026#34;/\u0026#34;)[-1] async with aiohttp.ClientSession() as sessions: # 这部的操作就相当于 \u0026lt;==\u0026gt; requests async with sessions.get(url) as s: with open(f\u0026#34;{name}.jpg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(await s.content.read()) print(\u0026#34;OVER\u0026#34;) async def main(): task = [] for url in urls: task.append(downsave(url)) await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1)    个人对于协程的理解 asyncio.run()是用来运行最高层级的入口点\n","permalink":"https://hanson00.github.io/posts/technology/python/python%E5%B9%B6%E5%8F%91%E4%B8%8E%E7%88%AC%E8%99%AB/","summary":"Python并发与爬虫 DeprecationWarning:显式地将协程对象传递给asyncio.wait()自Python 3.8以来已弃用","title":"Python并发与爬虫"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\ncsv模块实现了 CSV 格式表单数据的读写，提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;君不愁\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E5%AD%A6%E4%B9%A0/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv学习"},{"content":"爬虫路飞进阶 视频网站工作原理   用户上传视频到视频网站\n  视频网站对视频进行转码(4K,2K,1080P,标清等)\n  对视频进行切片处理（个人理解不知道对错，分布式存储视频片段） ==================被切分的视频片段，当用户想拖到后面的片段播放时就直接播放后面被切分的一小段片段，节省流量资源\n  切分后的视频的排序问题，需要一个文件1.记录视频播放顺序2.视频的存放路径基本上会直接存放在M3U文件(固定格式的文本)如json，txt，M3U文件通过utf-8编码后的名字是M3U8\n 怎么爬取？\n 找到m3u8文件（通过各种手段） 通过m3u8下载视频文件 将下载到的视频文件合并成一个视频文件     selenium的视频学习 tips：\n 所打开的浏览器的左上角的浏览器受到控制提示可能后面会遇到反爬 如果遇到ajax的页面(局部刷新)所要操作时需要等待否则大概率报错 当代码只爬取到了部分数据就报错了一般是爬取太快所导致 有些视频网站会有iframe，必须先要拿到iframe(iframe = find_element()找到iframe)然后把视角切换到iframe(web.switch_to.window(iframe窗口))才能拿到数据，切换回原页面使用web.switch_to.default_content()  先粗细定位整体在精细定位局部\n如果你的程序chrome被检测到了是爬虫程序，一般分为两种情况解决chrome版本号小于88和大于88（浏览器控制台输入window.navigator.webdriver查看）  关于selenium的一些常见用法 最好能记住下面的案例所要导入的模块，多看\n1 2 3 4 5 6 7 8 9 10 11 12  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.page_source browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER)   selenium的微信读书小案例 1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from selenium import webdriver import re #使用无界模式访问,并获取网页源代码 sin = input(\u0026#34;请输入你要查找的新闻:\u0026#34;) url = f\u0026#34;https://search.sina.com.cn/news?q={sin}\u0026amp;c=news\u0026amp;from=index\u0026#34; # #无界模式 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) browser.get(url) # data = browser.page_source browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[1]\u0026#39;).send_keys(\u0026#34;阿里巴巴\u0026#34;) browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[4]\u0026#39;).click() retitle = re.compile(r\u0026#39;\u0026lt;div class=\u0026#34;box-result clearfix\u0026#34; data-sudaclick=\u0026#34;.*?\u0026#34;\u0026gt;.*?\u0026lt;a href=\u0026#34;(?P\u0026lt;url\u0026gt;.*?)\u0026#34;.*?\u0026gt;(?P\u0026lt;title\u0026gt;.*?)\u0026lt;/a\u0026gt;\u0026#39;, re.S) result = retitle.finditer(browser.page_source) for i in result: title = i.group(\u0026#34;title\u0026#34;).replace(\u0026#39;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026#39;,\u0026#34;\u0026#34;).replace(\u0026#39;\u0026lt;/font\u0026gt;\u0026#39;, \u0026#34;\u0026#34;) url = i.group(\u0026#34;url\u0026#34;)   拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) #上面//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1] for li in all_list: # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a jobname = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).text salary = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[2]/span\u0026#39;).text print(jobname, salary)   验证码  自己写图像识别 选择互联网上成熟的验证码破解工具  下拉框案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # coding:utf-8 # @Time : 2022/5/21 10:46 # @Author : 软件1194温铭军 # @file : 无头浏览器.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.select import Select #无头浏览器代码 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--headless\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://www.endata.com.cn/BoxOffice/BO/Year/index.html\u0026#39; browser.get(url) sel_el = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;OptionDate\u0026#34;]\u0026#39;) #选择下拉框的提取 sel = Select(sel_el) #将下拉框包装成select对象 #让浏览器调整选项 for i in range(len(sel.options)): sel.select_by_index(i) #拿到每一个下拉框的选项 time.sleep(1) table = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;TableList\u0026#34;]\u0026#39;) print(table.text) time.sleep(1)   窗口切换 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # coding:utf-8 # @Time : 2022/5/20 9:32 # @Author : 软件1194温铭军 # @file : lagou.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   12306 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/21 11:41 # @Author : 软件1194温铭军 # @file : 12306.py # $software : PyCharm #12306账号：a954952920 #密码：hansonwmj2000 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time from selenium.webdriver.common.action_chains import ActionChains #鼠标操作导包,事件链 #防止被认出是爬虫程序，chrome版本88以上适用 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--disable-blink-features=AutomationControlled\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://kyfw.12306.cn/otn/resources/login.html\u0026#39; browser.get(url) time.sleep(1) #输入账号密码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-userName\u0026#34;]\u0026#39;).send_keys(\u0026#39;你的账号\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-password\u0026#34;]\u0026#39;).send_keys(\u0026#39;密码\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-login\u0026#34;]\u0026#39;).click() time.sleep(2) #拖拽目标按钮 btn = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1_n1z\u0026#34;]\u0026#39;) #找到目标拖拽验证码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1__scale_text\u0026#34;]/span\u0026#39;) #拖拽移动 ActionChains(browser).drag_and_drop_by_offset(btn, 300, 0).perform() #横推拽   ","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%E7%88%AC%E8%99%AB/","summary":"爬虫路飞进阶 视频网站工作原理 用户上传视频到视频网站 视频网站对视频进行转码(4K,2K,1080P,标清等) 对视频进行切片处理（个人理解不知道","title":"路飞爬虫进阶爬虫"},{"content":"爬虫基础 爬虫基本流程\n1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到本地，爬取完的URL放到已爬取队列 3.分析网页内容，找出网页里面关心的URL链接和内容，继续执行第二步\n 获取网页 提取信息 保存数据 自动化程序  爬虫如果需要模拟则把该网站下面的所有请求信息封装(例如UA)然后发送\nHTTP请求方法 请求，是由客户端向服务器发出一般分为4部分内容：请求方法（request method）、请求的网址（request url）、请求头（request Headers）、请求体（request Body）\n   1 GET 请求指定的页面信息，并返回实体主体。     2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头   3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。   4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。   5 DELETE 请求服务器删除指定的页面。   6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。   7 OPTIONS 允许客户端查看服务器的性能。   8 TRACE 回显服务器收到的请求，主要用于测试或诊断。   9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。    请求头 请求头，用来说明服务器要使用的附加信息，比较重要的信息有，cookie、Refer、User-Agent\n响应 响应，由服务器端返回给客户端，分为响应状态码（Response Status Code）、响应头（Response headers）、响应体（Response Body）\n响应头 包含了服务器对请求的应答信息\n 小知识  Host:域名。表示请求的服务器网址   爬虫学习 urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)打开一个网页并把获取该URL的网页对象 data 必须是一个对象，用于给出要发送到服务器的附加数据，若不需要发送数据则为 None，data支持的对象类型包括字节串、类文件对象和可遍历的类字节串对象\nurllib.request.urlopen()  一个简单的get请求爬取\n 1 2 3 4 5 6 7 8 9 10 11  import urllib.request import urllib.parse #解析器 import urllib.error #Get请求 response = urllib.request.urlopen(\u0026#34;http://www.baidu.com\u0026#34;) print(response.read().decode(\u0026#34;UTF-8\u0026#34;))\t# response是\u0026lt;http.client.HTTPResponse object at 0x000002DC28A404C0\u0026gt; url = \u0026#34;http://httpbin.org/get\u0026#34; response2 = urllib.request.urlopen(url=url) print(response2.read().decode(\u0026#34;UTF-8\u0026#34;))    一个简单的post请求(post请求需要提交个表单信息)，需要提供一个封装的数据\n 1 2 3 4 5  #post请求 需要封装数据 http://httpbin.org/post url = \u0026#34;http://httpbin.org/post\u0026#34; data = bytes(urllib.parse.urlencode({\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;}), encoding = \u0026#34;utf-8\u0026#34;)\t#把数据变成二进制格式 response = urllib.request.urlopen(url=url, data=data) print(response.read().decode(\u0026#34;utf-8\u0026#34;))   urllib.request.Request() urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) 上面的urlopen过于简单直接把赋值的url直接打开，包含不了太多伪装信息，所以使用urllib.request.Request()\nheaders：告诉要访问的服务器，我们是什么类型的机器（浏览器），本质上是告诉浏览器我们可以接受什么水平的文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import urllib.request import urllib.parse #解析器 import urllib.error # http://httpbin.org/post https://www.douban.com url = \u0026#34;http://httpbin.org/post\u0026#34; headers = {\u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} data = bytes(urllib.parse.urlencode({\u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;}),encoding = \u0026#34;UTF-8\u0026#34;) #封装了一个请求对象 req = urllib.request.Request(url=url, data=data, headers=headers) response = urllib.request.urlopen(req)\t#响应对象 print(response.read().decode(\u0026#34;UTF-8\u0026#34;))   简单的爬取网页 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  import urllib.request import urllib.error def main(): #基础的URL # 1.爬取网页 baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xlsx\u0026#34; # datalist = getData(baseurl) getData(baseurl) #2.解析数据 #3.保存数据 # saveData(savepath) def getData(baseurl): datalist = [] for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) return datalist # 3.保存数据,在本列子中暂时不用 def saveData(savepath): pass # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main()   BeautifulSoup库的基本使用(爬虫的解析) BeautifulSoup4将HTML文档转换成树形结构，每个节点都是Python对象，归类成四种：\n Tag\t可以获取标签及其标签内容 NavigableString BeautifulSoup comment  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) # Tag 获取标签及内容 print(bs.title) #output: \u0026lt;title\u0026gt;百度一下，你就知道 \u0026lt;/title\u0026gt; print(bs.a) #output: \u0026lt;a class=\u0026#34;mnav\u0026#34; href=\u0026#34;http://news.baidu.com\u0026#34; name=\u0026#34;tj_trnews\u0026#34;\u0026gt;\u0026lt;!--新闻--\u0026gt;\u0026lt;/a\u0026gt; # 获取标签里面的内容 print(bs.title.string) #output: 百度一下，你就知道 print(bs.a.string) #output: 新闻 # NavigableString 获取标签里的所有属性，并返回一个字典 print(bs.a.attrs) #attrs是attribute属性的缩写 # print(bs) 打印整个html文件 # 文件的遍历 contents print(\u0026#34;-----文档的遍历-----\u0026#34;) print(bs.head.contents) #返回一个列表   BeautifulSoup2.0文档搜索 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  import re import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) #文档搜索 # 1. find_all()方法:字符串过滤会查找与字符串完全匹配的内容 list = bs.find_all(\u0026#34;a\u0026#34;) # list = bs.find_all(\u0026#34;a\u0026#34;, limit = 3) limit限制多少 print(list) #正则表达式搜索 print(\u0026#34;---正则表达式搜索---\u0026#34;) re_list = bs.find_all(re.compile(\u0026#34;a\u0026#34;)) print(re_list) print(len(re_list)) #传入一个函数作为参数进行搜索,按照函数进行搜索 print(\u0026#34;---函数搜索---\u0026#34;) def name_is_exists(tag): return tag.has_attr(\u0026#34;name\u0026#34;) funcres = bs.find_all(name_is_exists) print(funcres) #kwargs关键字搜索 print(\u0026#34;----kwargs关键字搜索----\u0026#34;) kwlist = bs.find_all(id = \u0026#34;head\u0026#34;) kwlist2 = bs.find_all(href = \u0026#34;https://www.hao123.com\u0026#34;) kwlist3 = bs.find_all(class_ = True) #这里class加下划线是因为class是Python的关键字 print(kwlist3) #text文本参数,查找标签里的字符串 print(\u0026#34;----text文本参数-----\u0026#34;) ts_list = bs.find_all(text=\u0026#34;hao123\u0026#34;) ts_list2 = bs.find_all(text=[\u0026#34;hao123\u0026#34;, \u0026#34;地图\u0026#34;]) print(ts_list2) #css选择器,和css选择器的语法一样 print(\u0026#34;------css选择器-------\u0026#34;) css_list = bs.select(\u0026#34;title\u0026#34;) css_list2 = bs.select(\u0026#34;.mnav\u0026#34;) css_list3 = bs.select(\u0026#34;#u1\u0026#34;) css_list4 = bs.select(\u0026#34;a[class = \u0026#39;bri\u0026#39;]\u0026#34;) print(css_list3)   liwei爬虫学习的最终 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  # coding:utf-8 # @Time : 2022/5/7 9:57 # @Author : 软件1194温铭军 # @file : ts.py # $software : PyCharm import bs4 import re import xlwt import urllib.request import urllib.error def main(): baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; #待爬取的URL savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xls\u0026#34; #保存位置 datalist = getData(baseurl) #爬取到的数据列表 # getData(baseurl) # print(datalist) #3.保存数据 saveData(datalist,savepath) # 提取影片的提取规则 recom_link = re.compile(r\u0026#39;\u0026lt;a href=\u0026#34;(.*?)\u0026#34;\u0026gt;\u0026#39;) recom_img = re.compile(r\u0026#39;\u0026lt;img alt=\u0026#34;.*? src=\u0026#34;(.*?)\u0026#34;.*?/\u0026gt;\u0026#39;, re.S) #re.S让换行符也包含在内 recom_title = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_rating = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_judge = re.compile(r\u0026#39;\u0026lt;span\u0026gt;(\\d*)人评价\u0026lt;/span\u0026gt;\u0026#39;) recom_inq = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;inq\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_db = re.compile(r\u0026#39;\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;(.*?)\u0026lt;/p\u0026gt;\u0026#39;,re.S) def getData(baseurl): datalist = [] #生成要爬取的网址 for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) #进行数据解析 soup = bs4.BeautifulSoup(html, \u0026#34;html.parser\u0026#34;) for item in soup.find_all(\u0026#34;div\u0026#34;, class_ = \u0026#34;item\u0026#34;):\t#在\u0026lt;class \u0026#39;bs4.BeautifulSoup\u0026#39;\u0026gt;中查找是div并且class=\u0026#34;item\u0026#34;的内容 # print(item) data = [] #保存每一部电影的信息 item = str(item) link = re.findall(recom_link, item)[0] data.append(link) #添加链接 img = re.findall(recom_img, item)[0] data.append(img) title = re.findall(recom_title, item) if (len(title) ==2): ctitle = title[0] data.append(ctitle) otitle = title[1].replace(\u0026#34;/\u0026#34;,\u0026#34;\u0026#34;) #去掉无关符号 data.append(otitle) else: data.append(title[0]) data.append(\u0026#34; \u0026#34;) #留空 rating = re.findall(recom_rating, item)[0] data.append(rating) judge = re.findall(recom_judge, item)[0] data.append(judge) inq = re.findall(recom_inq, item) if len(inq) !=0: inq = inq[0].replace(\u0026#34;。\u0026#34;, \u0026#34; \u0026#34;) data.append(inq) else: data.append(\u0026#34; \u0026#34;) db = re.findall(recom_db, item)[0] db = re.sub(\u0026#39;\u0026lt;br(\\s+)?/\u0026gt;(\\s+)?\u0026#39;, \u0026#34;\u0026#34;, db) db = re.sub(\u0026#39;/\u0026#39;, \u0026#34; \u0026#34;, db) data.append(db.strip()) datalist.append(data) # print(datalist) return datalist # 3.保存数据 def saveData(datalist,savepath): wb = xlwt.Workbook(encoding=\u0026#34;UTF-8\u0026#34;) ws = wb.add_sheet(\u0026#34;douban_dataTOP250\u0026#34;) col = (\u0026#34;电影详情链接\u0026#34;, \u0026#34;图片链接\u0026#34;, \u0026#34;影片中文名\u0026#34;, \u0026#34;影片外国名\u0026#34;, \u0026#34;评分\u0026#34;, \u0026#34;评价数\u0026#34;, \u0026#34;概况\u0026#34;, \u0026#34;相关信息\u0026#34;) for i in range(0,8): ws.write(0,i,col[i]) #列名 for i in range(0,250): print(\u0026#34;第%d条\u0026#34; %(i+1)) data = datalist[i] for j in range(0,8): ws.write(i+1,j,data[j]) #写入数据 wb.save(savepath) #保存 # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) # print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main() print(\u0026#34;爬取完毕\u0026#34;)   爬虫所用到的库 数据解析：\n bs4(beautifulsoup)解析【较慢】 re解析 xpath解析  re正则表达式  re.L 表示特殊字符集 \\w, \\W, \\b, \\B, \\s, \\S 依赖于当前环境 re.M 多行模式 re.S 即为' . \u0026lsquo;并且包括换行符在内的任意字符（\u0026rsquo; . \u0026lsquo;不包括换行符） re.U 表示特殊字符集 \\w, \\W, \\b, \\B, \\d, \\D, \\s, \\S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和\u0026rsquo; # \u0026lsquo;后面的注释   注意:\n 除了txt纯文本文件使用r来读取，其他类型的文件都是二进制文件用rb读取 乱码问题：Python内存层面使用的是Unicode编码，而Unicode不能存储和传输的，必须把Unicode编码进行转码(写文件时要编码，读文件时要解码)，转换成UTF-8或者GBK 得到的b\u0026rsquo;字符串\u0026rsquo;是字节，需要转码 在已经爬取得到网页后进行数据解析时，如果测试正则表达式没有问题但仍无数据时，可以加上re.S之类的re.compile(pattern[, flags])的flag参数 当使用BeautifulSoup的find(参数)参数中出现Python关键字时可以在关键字后面加__(PS: class_ = \u0026quot;值\u0026quot;)  1 2 3  #如下面两种写法一样 find(classs_=\u0026#34;table\u0026#34;) find(attrs={\u0026#34;class\u0026#34;:\u0026#34;table\u0026#34;})   BeautifulSoup中如果想拿到某个属性的值，可以使用get(\u0026ldquo;要获取的属性名\u0026rdquo;)方法  ","permalink":"https://hanson00.github.io/posts/technology/python/%E6%9D%8E%E5%B7%8D%E7%88%AC%E8%99%AB/","summary":"爬虫基础 爬虫基本流程 1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到","title":"李巍爬虫"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Person struct { age int name string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板文件 \troute.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) //配置路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到首页\u0026#34;) }) route.POST(\u0026#34;/post\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到post页面\u0026#34;) }) route.GET(\u0026#34;json1\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, map[string]interface{}{ \u0026#34;json1\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第一个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json2\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;json2\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第2个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json3\u0026#34;, func(c *gin.Context) { tom := \u0026amp;Person{ 18, \u0026#34;tom\u0026#34;, } c.JSON(http.StatusOK, tom) }) route.GET(\u0026#34;xml\u0026#34;, func(c *gin.Context) { c.XML(http.StatusOK, map[string]interface{}{ \u0026#34;xml\u0026#34;: \u0026#34;6666\u0026#34;, }) }) route.GET(\u0026#34;index\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;index.html\u0026#34;, map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;后台的标题\u0026#34;, }) }) route.Run(\u0026#34;:8821\u0026#34;) }   模板渲染 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Aticle struct { Title string Content string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板路径 \t//route.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) \t//如果有多层模板目录/**代表一层目录 \troute.LoadHTMLGlob(\u0026#34;templates/**/*\u0026#34;) //前台路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { //这里的html名称要和渲染的htmldefine后面的名称一致 \tc.HTML(http.StatusOK, \u0026#34;templates/default/beindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/new\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/default/benew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) //后台路由 \troute.GET(\u0026#34;/admin\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;templates/admin/adindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/adminnew\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/admin/adnew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) route.Run(\u0026#34;:8848\u0026#34;) }   ","permalink":"https://hanson00.github.io/posts/technology/golang/01gin/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34;","title":"01gin"},{"content":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看请求头参数如：UA(User-Agent)\nPOST请求注意发送数据，如果是POST请求则要在请求中加入From Data\n数据解析：尽量使用re解析和xpath解析数据\n网页的渲染方式： 使用浏览器开发者工具和查看网页源代码可以看出为什么渲染方式\n 服务器渲染：在服务器那边就直接把数据和html结合在一起，统一返回给客户端(浏览器) 客户端渲染：客户端浏览器第一次请求时只要一个html骨架，待浏览器检查html时再第二次向服务器请求数据，并进行渲染【在网页源代码中看不到数据】|如果遇到客户端渲染的网页则要在抓包工具中(浏览器开发者工具)捕获需要的数据源在对该数据源的URL进行请求抓取  关于网页编码与解码的一些总结   在一般网页有给出自己给出编码字符集时可以观察编码后自行赋值或者自己编写正则表达式自己提取\n电影天堂项目体现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests #1.从电影天堂的首页爬取相关首页的内容 # 2.从相关内容中提取相关下载信息 url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } # response = requests.get(url, headers=header, verify=False) response = requests.get(url, headers=header) #本次实验不需verify也可以获得信息 # 获取网页的编码方式,自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) # print(code) #或者在爬取乱码网页后进行观察后自行赋值（直接自己赋值） response.encoding = code html = response.text #下面两行代码也可解决编码问题 # code = response.encoding # print(response.text.encode(code).decode(\u0026#34;gb2312\u0026#34;)) # print(html) #提取首页中所需内容 redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(html) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) #在提取的首页内容中进一步提取子页所需内容 child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_response = requests.get(child_url, headers=header) child_response.encoding = code # print(child_response.text) child_html = child_response.text child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_html) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) response.close() print(\u0026#34;爬取结束\u0026#34;)     案例2\n1 2 3 4 5 6 7 8 9  import requests heads = {\u0026#34;User_Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} url1 = \u0026#34;https://www.baidu.com\u0026#34; request_data = requests.get(url=url1, headers=heads) code = request_data.encoding print(code) response = request_data.text.encode(code).decode(\u0026#34;utf-8\u0026#34;) print(response) #response = request_data.text 有时可以直接无需转码或者有时候可以使用decode(\u0026#34;gbk\u0026#34;)   requests get方式请求：\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#34;https://www.baidu.com/s?ie=UTF-8\u0026amp;wd=%E5%91%A8%E6%9D%B0%E4%BC%A6\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(url, headers=headers) print(response.text)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import requests if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#39;hello\u0026#39;) # https://movie.douban.com/j/chart/top_list?type=24\u0026amp;interval_id=100%3A90\u0026amp;action=\u0026amp;start=0\u0026amp;limit=20 url = \u0026#39;https://movie.douban.com/j/chart/top_list?\u0026#39; # 封装URL的参数 params = { \u0026#39;type\u0026#39;:24, \u0026#39;interval_id\u0026#39;:\u0026#39;100:90\u0026#39;, \u0026#39;action\u0026#39;:\u0026#39;\u0026#39;, \u0026#39;start\u0026#39;:0, \u0026#39;limit\u0026#39;:20, } headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34; } response = requests.get(url=url, headers=headers, params=params) response.encoding = \u0026#39;utf8\u0026#39; print(response.json())   post方式请求： post请求时需要查看网页的From data所需要的参数然后在填充后传入\n1 2 3 4 5 6 7 8 9 10 11  # post请求：百度翻译 import requests url = \u0026#34;https://fanyi.baidu.com/sug\u0026#34; ins = input(\u0026#34;请输入你要翻译的词\u0026#34;) data = { \u0026#34;kw\u0026#34;: ins } response = requests.post(url, data=data) print(response.json())   数据解析 三种解析方式：\n re解析(最快) bs4解析(便捷)在李巍爬虫学习中 xpath解析  re正则表达式 1 2 3 4 5 6 7 8 9 10 11 12  import re str = \u0026#39;\u0026#39;\u0026#39; \u0026lt;div class=\u0026#34;jay\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;1\u0026#34;\u0026gt;周杰伦\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay2\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;2\u0026#34;\u0026gt;周杰伦2\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay3\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;3\u0026#34;\u0026gt;周杰伦3\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026#39;\u0026#39;\u0026#39; # 使用 (?P\u0026lt;分组名\u0026gt;正则) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;(?P\u0026lt;id\u0026gt;.*?)\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;.*\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;) result = res.finditer(str) for i in result: print(i.group(\u0026#39;id\u0026#39;))   xPath解析   首先导入from lxml import etree etree才包括了xPath解析的功能\nxPath解析XML\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  from lxml import etree xml = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; tree = etree.XML(xml) #生成一个xPath解析的XML对象 # result = tree.xpath(\u0026#34;/book\u0026#34;) #output:[\u0026lt;Element book at 0x183918b2f00\u0026gt;] result = tree.xpath(\u0026#34;/book/name\u0026#34;) #output:[\u0026lt;Element name at 0x21065d142c0\u0026gt;] result = tree.xpath(\u0026#34;/book/name/text()\u0026#34;) #output:[\u0026#39;野花遍地香\u0026#39;] # 把后代(子孙节点的内容拿出来) result = tree.xpath(\u0026#34;/book/author//nick/text()\u0026#34;) #output:[\u0026#39;周大枪\u0026#39;, \u0026#39;周芷若\u0026#39;, \u0026#39;周杰伦\u0026#39;, \u0026#39;蔡依林\u0026#39;, \u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] #*通配符 result = tree.xpath(\u0026#34;/book/author/*/nick/text()\u0026#34;) #output:[\u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] print(result)     xPath解析html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  from lxml import etree tree = etree.parse(\u0026#34;ts.html\u0026#34;)\t# result = tree.xpath(\u0026#34;/html\u0026#34;) #output:[\u0026lt;Element html at 0x1e6e3568880\u0026gt;] result = tree.xpath(\u0026#34;/html/body/ul/li/a\u0026#34;) #output:[\u0026lt;Element a at 0x1f354c57300\u0026gt;, \u0026lt;Element a at 0x1f354c57400\u0026gt;, \u0026lt;Element a at 0x1f354c573c0\u0026gt;] #xPath的索引是从第一个开始 result = tree.xpath(\u0026#34;/html/body/ul/li[1]/a/text()\u0026#34;) #output:[\u0026#39;百度\u0026#39;] #两种提取大炮的方法 result = tree.xpath(\u0026#34;/html/body/ol/li[2]/a/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] #@表示a里面的属性href是dapao的 #[@XXX=XXX] : 属性的筛选 result = tree.xpath(\u0026#34;/html/body/ol/li/a[@href=\u0026#39;dapao\u0026#39;]/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] result = tree.xpath(\u0026#34;/html/body/ol/li\u0026#34;) #output: [\u0026#39;飞机\u0026#39;] #[\u0026#39;大炮\u0026#39;] #[\u0026#39;火车\u0026#39;] for i in result: a = i.xpath(\u0026#34;./a/text()\u0026#34;) print(a) #拿到a里面的属性的值 result = tree.xpath(\u0026#34;/html/body/ol/li/a/@href\u0026#34;) print(result) \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34;\u0026gt;百度\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.sogou.com\u0026#34;\u0026gt;搜狗\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;feiji\u0026#34;\u0026gt;飞机\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;dapao\u0026#34;\u0026gt;大炮\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;huoche\u0026#34;\u0026gt;火车\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;div class=\u0026#34;job\u0026#34;\u0026gt;李嘉诚\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;common\u0026#34;\u0026gt;胡辣汤\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34;   xpath有时候返回的是一个列表注意了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  from lxml import etree import requests if __name__ == \u0026#39;__main__\u0026#39;: url = \u0026#39;https://sc.chinaz.com/jianli/221025527710.htm\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34; } response = requests.get(url=url, headers=headers) response.encoding = \u0026#34;utf-8\u0026#34; html = etree.HTML(response.text) # 下面代码返回的是一个列表注意了 zip_url = html.xpath(\u0026#34;//div[@class=\u0026#39;clearfix mt20 downlist\u0026#39;]//li[1]/a/@href\u0026#34;) # print(type(zip_url)) # zip_url:https://downsc.chinaz.net/Files/DownLoad/jianli/202210/zjianli596.rar response1 = requests.get(url=zip_url[0], headers=headers) with open(\u0026#39;./1.zip\u0026#39;, \u0026#39;wb\u0026#39;) as file: file.write(response1.content) response1.close()   xPath解析猪八戒网 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests from lxml import etree import csv baseurl = \u0026#34;https://zhanjiang.zbj.com/search/f/?kw=saas\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(baseurl, headers=headers) # print(response.text) html = etree.HTML(response.text) #相对解析 #拿到整个大的div的列表 divs = html.xpath(\u0026#34;/html/body/div[6]/div/div/div[2]/div[5]/div[1]/div\u0026#34;) namelist = [] datalist = [] #在整个大的div里面选取要提取的数据 for i in divs: price = i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[1]/span[1]/text()\u0026#34;)[0].strip(\u0026#34;¥\u0026#34;) title = \u0026#34;saas\u0026#34;.join(i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[2]/p/text()\u0026#34;)) com_name = i.xpath(\u0026#34;./div/div/a[1]/div[1]/p/text()\u0026#34;)[1].strip() place = i.xpath(\u0026#34;./div/div/a[1]/div[1]/div/span/text()\u0026#34;)[0] datalist = [title, price, com_name, place] # print(com_name) with open(\u0026#34;猪八戒.csv\u0026#34;, \u0026#34;a+\u0026#34;) as csvfile: zwriter = csv.writer(csvfile, ) zwriter.writerow(datalist) response.close() print(\u0026#34;爬取结束\u0026#34;)   防盗链-梨视频 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  import requests #原始页面的链接，有需要替换的信息 url = \u0026#34;https://www.pearvideo.com/video_1761797\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34;, #防盗链：溯源1-》2-》3一样 \u0026#39;Referer\u0026#39;: \u0026#39;https://www.pearvideo.com/video_1761797\u0026#39; } #获取后面要用于替换的信息 conid = url.split(\u0026#34;_\u0026#34;)[1] #从抓包工具中获取的url里面获取json，目的是要获得假的URL visit_url = f\u0026#34;https://www.pearvideo.com/videoStatus.jsp?contId={conid}\u0026amp;mrd=0.561999819571843\u0026#34; response = requests.get(url=visit_url, headers=header) # print(response.json()) #在抓包工具中的链接里面获得假视频URL和假视频URL的信息用于替换 #\u0026#39;https://video.pearvideo.com/mp4/adshort/20220511/1652607901500-15877599_adpkg-ad_hd.mp4\u0026#39; flase_url = response.json()[\u0026#34;videoInfo\u0026#34;][\u0026#34;videos\u0026#34;][\u0026#34;srcUrl\u0026#34;] systemtime = response.json()[\u0026#39;systemTime\u0026#39;] # print(systemtime) # print(flase_url) realurl = flase_url.replace(systemtime, f\u0026#34;cont-{conid}\u0026#34;) print(realurl) #下载视频 with open(\u0026#34;梨视频.mp4\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(realurl).content) response.close() print(\u0026#34;爬取结束\u0026#34;)   豆瓣小抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests import re import csv url = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } req = requests.get(url, headers=header) response = req.text # print(response) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;hd\u0026#34;\u0026gt;.*?\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(?P\u0026lt;name\u0026gt;.*?)\u0026#39; R\u0026#39;\u0026lt;/span\u0026gt;.*?\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;.*?\u0026lt;br\u0026gt;(?P\u0026lt;year\u0026gt;.*?)\u0026amp;nbsp.*?\u0026#39; R\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(?P\u0026lt;scroe\u0026gt;.*?)\u0026lt;/span\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;span\u0026gt;(?P\u0026lt;people\u0026gt;\\d+)人评价\u0026lt;/span\u0026gt;\u0026#39;, re.S) result = res.finditer(response) with open(\u0026#34;lufeidouban.csv\u0026#34;, \u0026#34;a\u0026#34;) as f: file_writer = csv.writer(f) for i in result: # print(i.group(\u0026#34;name\u0026#34;)) # print(i.group(\u0026#34;year\u0026#34;).strip()) # print(i.group(\u0026#34;scroe\u0026#34;)) # print(i.group(\u0026#34;people\u0026#34;)) #将数据保存成csv文件 dic = i.groupdict() dic[\u0026#34;year\u0026#34;] = dic[\u0026#34;year\u0026#34;].strip() file_writer.writerow(dic.values()) req.close() print(\u0026#34;爬取结束\u0026#34;)    北京新发地抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/13 15:42 # @Author : 软件1194温铭军 # @file : beijing_xinfadi.py # $software : PyCharm import requests import csv #使用post请求时所要添加data请求参数,可以在浏览器开发者工具里面的From Data里面看到 url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } data= { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } response = requests.post(url, headers=headers, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;w\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;,\u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) response.close() print(\u0026#34;爬取结束\u0026#34;)   电影天堂重复代码处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # coding:utf-8 # @Time : 2022/5/13 9:09 # @Author : 软件1194温铭军 # @file : dytt.py # $software : PyCharm # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def htmldata(url): \u0026#34;\u0026#34;\u0026#34; 代替提取网页信息 :param url: :return: \u0026#34;\u0026#34;\u0026#34; response = requests.get(url, headers=header) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) response.encoding = code html = response.text response.close() return html # 或者自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(htmldata(url)) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_data = htmldata(child_url) child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_data) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) print(\u0026#34;爬取结束\u0026#34;)   代理 代理：通过第三方的一个机器去发送请求（去网上找免费代理，或者找）\n综合训练-网易云音乐评论抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132  # coding:utf-8 # @Time : 2022/5/16 10:07 # @file : 网易云音乐.py # $software : PyCharm #爬取网易云音乐评论 # 步骤： # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密 # 3.请求到网易，拿到评论信息 # 加密参数 # params:就是encText # encSecKey:就是encSecKey from Crypto.Cipher import AES import requests import re import json from base64 import b64encode url = \u0026#34;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\u0026#34; #POST请求 headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } #找到了真正的参数，接下来还需要解密过程 data = { \u0026#39;csrf_token\u0026#39;: \u0026#34;\u0026#34;, \u0026#39;cursor\u0026#39;: \u0026#34;-1\u0026#34;, \u0026#39;offset\u0026#39;: \u0026#34;0\u0026#34;, \u0026#39;orderType\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageNo\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageSize\u0026#39;: \u0026#34;20\u0026#34;, \u0026#39;rid\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34;, \u0026#39;threadId\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34; } #处理加密过程 #服务于windows.arXXX的加密过程 e = \u0026#39;010001\u0026#39; f = \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; g = \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; i = \u0026#39;LvjKmGgecBJv6kkF\u0026#39; #手动固定 #如果把i固定那么下面这个函数一定是固定的，由下面的c函数解出 def get_encSecKey(): return \u0026#34;7c435414bc686e49dc26a57ffedbb80b320fd755755c5da8e72f416a61370039aa2d3fa333e54a6e7c9abe7f26faffa69d1721db76cb2e6f17d0393d4cfbc6176590909d027022c4e458aee2123c329b60e1e422beef0f8e39efad5014cbeab022199bc7e6c47a5d0bca5528f6c7946305ae019674309d562c69b39dde9ea429\u0026#34; #字典不能加密，所以下面这个函数默认收到的是字符串 def get_params(data): \u0026#34;\u0026#34;\u0026#34; 就是去还原下面的b的两次加密 h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i) :param data: :return: \u0026#34;\u0026#34;\u0026#34; first = enc_params(data, g) second = enc_params(first, i) return second #返回的就是params #转换成16的长度为加密算法服务 def to_16(data): pad = 16 - len(data) % 16 data += chr(pad) *pad return data #第二个参数的加密算法 def enc_params(data, key): iv = \u0026#39;0102030405060708\u0026#39; data = to_16(data) aes = AES.new(key=key.encode(\u0026#34;utf-8\u0026#34;), IV=iv, mode=AES.MODE_CBC) #创建了一个加密工具 bs = aes.encrypt(data.encode(\u0026#34;utf-8\u0026#34;)) #如果想要返回字符串还需要base64转码 return str(b64encode(bs), \u0026#34;utf-8\u0026#34;) pass \u0026#34;\u0026#34;\u0026#34; function a(a) { var d, e, b = \u0026#34;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\u0026#34;, c = \u0026#34;\u0026#34;; for (d = 0; a \u0026gt; d; d += 1) #循环16次 e = Math.random() * b.length, #产生随机数 e = Math.floor(e), #取整 c += b.charAt(e); return c #返回一个16个字母 } function b(a, b) { a是要加密的内容 var c = CryptoJS.enc.Utf8.parse(b) c和b是一回事 , d = CryptoJS.enc.Utf8.parse(\u0026#34;0102030405060708\u0026#34;) 偏移量 , e = CryptoJS.enc.Utf8.parse(a) e和a相同都是数据 , f = CryptoJS.AES.encrypt(e, c, { e是数据，c是 AES是加密算法自己查的话可以知道下面的东西 iv: d, 偏移量 mode: CryptoJS.mode.CBC 加密模式CBC 看完之后发现少了个密钥所以c(b)是密钥 }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,\u0026#34;\u0026#34;,c), #RSA加密 e = encryptedString(d, a) } function d(d, e, f, g) { d：就是真实数据data,除了数据d是变的之外，其他都是不变的 var h = {}产生一个空对象 e：固定值 \u0026#39;010001\u0026#39; , i = a(16); i是一个16位数的随机字符串 f:超长定值 \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i), i是密钥 #返回的就是加密数据param #如果i固定那么encSeckey就固定 h.encSecKey = c(i, e, f), i是一个16位数的随机字符串，ef是固定的值 #返回的就是加密参数encSecKey return h }windows.a = d加密参数的入口是d encText是通过两次加密得出第二个加密参数 \u0026#34;\u0026#34;\u0026#34; response = requests.post(url, headers=headers, data={ \u0026#34;params\u0026#34;:get_params(json.dumps(data)), \u0026#34;encSecKey\u0026#34;:get_encSecKey() }) print(response.text) # list = response.json()[\u0026#39;data\u0026#39;][\u0026#39;hotComments\u0026#39;] # res = re.compile(r\u0026#34;\u0026#39;content\u0026#39;: \u0026#39;(.*?)\u0026#39;,\u0026#34;) # # print(list[0]) # print(res.findall(str(list))) response.close() print(\u0026#34;OVER!\u0026#34;)   [后续练习] 熟练使用浏览器开发者工具\n [附录] 使用.*?代替变化的内容，使用(.*?)代表需要提取的内容\n记得关闭请求\n","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB/","summary":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看请求头参数如：UA(User-Agent) POST请求注意发送数据，如果是POST请求则要在请","title":"路飞爬虫"},{"content":"Git Learning HEAD是指当前所在分支\ngit branch 显示所在分支\ngit check cat 切换到cat分支\n  git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表示文件已经被安置到暂存区，稍后会与其他文件一起放到存储库) git add \u0026ndash;all全部文件添加到暂存区 git commit -m \u0026ldquo;init commit\u0026rdquo; commit只会把暂存区里面的文件提交到存储库，加粗字体段为提交文件的说明描述一定要添加否则会打开vim编辑器进行再次补充 额外点：二段式：git commit -a -m \u0026ldquo;描述补充信息\u0026rdquo;（一次直接add 和commit） git log 查看日志信息   使用git查询历史记录时的常见问题  git log git log welcome.html 访问特定文件的日志信息 git log \u0026ndash;oneline 显示一行日志 git reflog 可以移动指针次数有所记录方便回调版本 想要查找某个人或者某些人的commit \u0026ndash;git log \u0026ndash;oneline \u0026ndash;author=\u0026ldquo;作者名称\u0026rdquo; 使用\u0026quot;\\|\u0026ldquo;表示或者 git log \u0026ndash;oneline \u0026ndash;grep=\u0026ldquo;关键字信息\u0026rdquo; 在关键字的内容中进行查询 git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; 找到早上九点到十二点的所有commit git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; \u0026ndash;after=\u0026ldquo;2022-01\u0026rdquo; 从2022-01之后的每天的早上九点到十二点的所有commit   在git中删除或者修改文件  直接删除，使用系统直接删除或者使用rm  删除后还要使用git add \u0026ldquo;要删除的文件名称\u0026rdquo; git add 1.txt 移入暂存区 add后还要进行commit操作 git commit -m \u0026ldquo;描述信息\u0026rdquo; 提交到存储库   使用git进行删除 git rm 2.txt 直接完成add操作后面只需使用commit操作即可完成   如果不想git在对文件进行控制可以使用git rm \u0026ldquo;2.txt文件名\u0026rdquo; \u0026ndash;cached 变更文件名  使用系统直接改变名称后 使用git add \u0026ndash;all git commit -m \u0026ldquo;描述信息\u0026rdquo;   或者直接使用git进行改名 git mv filename1 filename2 修改commit描述记录 git commit \u0026ndash;amend -m \u0026ldquo;描述信息\u0026rdquo; 使用amend只能修改最后一条记录 git blame 找出哪段代码谁是修改的 git checkout welcome.html 挽救误删的文件   版本前进后退  git reflog git reset \u0026ndash;hard hash值 前进后退到某一个文档  git diff 文件名 将工作区文件和暂存区文件作比较\ngit diff HEAD 文件名 与本地库做对比\ngit diff 全部文件做比较\n 分支  git branch 获取当前分支 git branch cat 创建cat分支 git branch -d cat 删除cat分支 -D强制删除 git checkout cat 切换分支 git branch -v 查看有什么分支  分支合并 1.先切换到要合并的分支 git merge     git与GitHub   如果是全新的项目创建的话git init\n  git add 1.txt\n  git commit -m \u0026ldquo;git one file\u0026rdquo;\n  接下来要把内容推送到远端的git服务器上\n git remote add origin GitHub服务器地址 origin只是默认的代名词，代指后面的那个服务器地址，可以改名 （起别名，以后可以用这个别名代替远程库地址） git push -u origin master push指令： 例如 远端节点为wmj，选择cat分支推送上去 git push wmj cat  把master分支的内容推送到origin位置 如果远端服务器不存在master分支，会自动创建 如果服务器上存在master分支，则会把master分支的位置指向最新的状态 -u其实就是upstream（上游），其实就是另一个分支的名称而已，在git里，每个分支都可以设置一个上游，它会自动跟踪某个分支 git push -u origin master 会把origin/master设置为本地master的分支的upstream，当下次执行git push时不添加任何指令，git会猜测推送给远端节点origin，并把master分支推上去 git push origin master=git push origin master:master 如果想把分支推上去后进行改名则git push origin master:cat      把推送的内容拉回来更新  git fetch git merge origin/master    推送命令 git pull =git fetch+git merge\n  git pull -rebase 可以使得产生的额外commit取消\n  有时候push推送失败是因为在线版本的内容比本地内容新此时需要 先拉再推 git pull \u0026ndash;rebase\n  看到有趣的项目时单击clone or download 或者使用 git clone git@github.com:\u0026hellip;. 复制下来地址后面还可以加目录名\n与其他开发者互动pull request（待续） ","permalink":"https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/git%E5%AD%A6%E4%B9%A0/","summary":"Git Learning HEAD是指当前所在分支 git branch 显示所在分支 git check cat 切换到cat分支 git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表","title":"Git学习"},{"content":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪 正在学习:  爬虫  🧠 计划学习:\n 数据分析 机器学习 Python量化   ","permalink":"https://hanson00.github.io/about/","summary":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪 正在学习:  爬虫  🧠 计划学习:\n 数据分析 机器学习 Python量化   ","title":""}]
[{"content":"RabbitMQ  在爬取数据的时候有可能需要一些进程间的通信机制，如\n一个进程负责构造爬取请求，另一个进程负责爬取请求\n一个进程爬取数据完毕，通知另一进程处理数据\n 为了降低进程间的耦合度，使用消息中间件来存储和转发消息，实现进程间的通信\n基本使用 RabbitMQ就是一个消息队列，要实现进程间的通信问题，本质上就是生产者-消费者模型，进程1生产者将消息放入消息队列，进程2消费者监听并处理消息队列中的消息。需要关注的点：\n 声明队列：指定参数创建消息队列 生产内容：生产者根据队列的连接信息连接队列，往队列中放入消息 消费内容：消费者根据队列的连接信息连接队列，往队列中取出消息  基础示例  生产者\n 1 2 3 4 5 6 7 8 9 10 11 12  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列 channel.queue_declare(queue=QUEUE_NAME) channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, body=b\u0026#34;123\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) channel = connection.channel() channel.queue_declare(queue=QUEUE_NAME) def callback(ch, method, properties, body): print(f\u0026#34;GET {body}\u0026#34;) channel.basic_consume(queue=\u0026#34;spiders\u0026#34;, auto_ack=True, on_message_callback=callback) channel.start_consuming()   随取随用 使生产者可以自行控制的将消息放入队列，消费者也可以根据自己的能力来获得并处理数据\n 生产者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列 channel.queue_declare(queue=QUEUE_NAME) while True: data = input() channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, body=data.encode()) print(f\u0026#34;Put data is {data}\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import pika QUEUE_NAME = \u0026#34;spiders\u0026#34; connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) channel = connection.channel() while True: input() method_frame, header, body = channel.basic_get( queue=QUEUE_NAME, auto_ack=True ) if body: print(f\u0026#34;Get data {body}\u0026#34;)   优先级队列 只需要在声明队列的时候加上x-max-priority参数来指定最大优先级\n队列持久化 只需要在声明队列的时候加上durable=True参数来开启持久化存储，同时在添加消息的时候要指定pika.BasicProperties对象的delivery_mode=2\n简易运用  生产者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import pika import requests import pickle QUEUE_NAME = \u0026#34;spider2\u0026#34; MAX_PRIORITY = 100 # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() # 声明了一个队列，并且开启持久化存储 channel.queue_declare(queue=QUEUE_NAME, durable=True) for i in range(1, 10): url = f\u0026#34;https://scrape.center/detail/{i}\u0026#34; req = requests.Request(\u0026#39;GET\u0026#39;, url) channel.basic_publish(exchange=\u0026#34;\u0026#34;, routing_key=QUEUE_NAME, properties=pika.BasicProperties(delivery_mode=2), body=pickle.dumps(req)) print(f\u0026#34;Put url is {url}\u0026#34;)    消费者\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pika import requests import pickle QUEUE_NAME = \u0026#34;spider2\u0026#34; MAX_PRIORITY = 100 # 连接RabbitMQ服务 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#34;localhost\u0026#34;)) # 声明了一个频道对象 channel = connection.channel() session = requests.session() def scrape(request): try: response = session.send(request.prepare()) print(f\u0026#34;success {response.url}\u0026#34;) except Exception as e: print(f\u0026#34;Exception if {e}\u0026#34;) while True: method_frame, header, body = channel.basic_get( queue=QUEUE_NAME, auto_ack=True ) if body: request = pickle.loads(body) print(f\u0026#34;GET {body}\u0026#34;) scrape(request)   ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103-3/","summary":"RabbitMQ 在爬取数据的时候有可能需要一些进程间的通信机制，如 一个进程负责构造爬取请求，另一个进程负责爬取请求 一个进程爬取数据完毕，通知另一进程处理数","title":"崔庆才爬虫书阅读摘要03 3"},{"content":"Redis缓存存储 Redis是一个基于内存的、高效的键值型非关系型数据库\n连接Redis 1 2 3 4 5 6  from redis import StrictRedis redis = StrictRedis(host=\u0026#34;localhost\u0026#34;, port=6379, db=0, password=\u0026#34;winhansonserver\u0026#34;) print(redis.get(\u0026#34;a1\u0026#34;)) redis.close()   下面还可以使用ConnectionPool，来连接Redis\n1 2 3 4 5 6 7  import redis from redis import StrictRedis pool = redis.ConnectionPool(host=\u0026#34;localhost\u0026#34;, db=0, password=\u0026#34;winhansonserver\u0026#34;) redis = StrictRedis(connection_pool=pool) print(redis.get(\u0026#34;a1\u0026#34;))   通过源码发现，StrictRedis内就是用参数来构造了一个ConnectionPool。\n并且ConnectionPool还支持通过URL来构建连接。支持的格式有：\n Redis TCP连接  redis://[:password]@host:port/db redis://:winhansonserver@localhost:6379/0   Redis TCP+SSL连接  rediss://[:password]@host:port/db   Redis UNIX socket连接  unix://[:password]@/path/to/socket.sock?db=db    1 2 3 4 5 6 7 8  import redis from redis import StrictRedis url = \u0026#34;redis://:winhansonserver@localhost:6379/0\u0026#34; pool = redis.ConnectionPool.from_url(url) redis = StrictRedis(connection_pool=pool) print(redis.get(\u0026#34;a1\u0026#34;))   在自己使用时可以自行查询的方法 键操作 列表操作 集合操作 集合的元素都是不重复的\n有序集合操作 有序集合比集合多了一个分数字段，利用该字段可以对集合中的数据进行排序。\n散列操作 总结 Redis的便携以及高效，后面我们会使用Redis实现，维护代理池，账号池，ADSL拨号代理池，Scrapy-Redis分布式架构等。\nElasticsearch搜索引擎存储 Elasticsearch是一个开源的全文搜索引擎。可以实现自己的搜索引擎，是一个可以快速存储、搜索和分析海量数据的全文搜索引擎。\nElasticsearch是对Lucene的封装，并且是：\n 一个分布式的实时文档存储库，每个字段都可以被索引和搜索 一个分布式的实时搜索引擎 可以被上百个服务节点拓展，并支持PB级别的结构化和非结构化数据   Elasticsearch相关概念\n  节点和集群  Elasticsearch本质是一个分布式数据库，允许多个服务器协同工作，每台服务器可以运行一个Elasticsearch实列，单个Elasticsearch实列被称为一个节点，多个节点构成一个集群   索引  index，Elasticsearch会索引所有字段，处理后会写入一个反向索引。查询数据的时候直接查找该索引。Elasticsearch的顶层单位就是索引，相当于MySQL和MongoDB等的数据库的概念。注意：索引的名字必须小写   文档  索引里面的单条记录就是文档，许多文档构成索引，最好保持文档的数据结构相同便于搜索的效率   类型  文档可以分组，例如城市的分组，汽车品牌的分组，这种分组就叫做类型，，它是虚拟逻辑的分组，类似于MySQL中的数据表，MongoDB中的集合（Elastic6允许每个索引包含一个类型，在版本7将会开始移除类型）   字段  每个文档都有类似的JSON结构，包含许多字段，每个字段都有对应的值，多个字段组成了文档。类似于MySQL数据表中的字段   es8.X彻底删除了type，es全部是JSON   传统关系型数据库-》database\u0026mdash;\u0026mdash;》Tables\u0026mdash;》Rows \u0026mdash;\u0026mdash;-》Colums\nElasticsearch\u0026mdash;\u0026ndash;》indices（索引）-》Type \u0026mdash;-》Document -》Fields\n 创建索引（相当于数据库） 1 2 3 4 5 6 7 8  from elasticsearch import Elasticsearch # 创建了一个Elasticsearch对象传入参数是连接Elasticsearch的连接 es = Elasticsearch(\u0026#34;http://127.0.0.1:9200/\u0026#34;) # 下面的ignore=400表示如果返回的是400的话忽略这个错误 result = es.indices.create(index=\u0026#34;news\u0026#34;, ignore=400) print(result)\t# 返回的是JSON格式   1 2 3 4 5 6 7 8 9  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # 创建索引（数据库名） result = es.indices.create(index=\u0026#34;news3\u0026#34;) print(result) # output：{\u0026#39;acknowledged\u0026#39;: True, \u0026#39;shards_acknowledged\u0026#39;: True, \u0026#39;index\u0026#39;: \u0026#39;news3\u0026#39;}   删除索引 1 2 3 4 5 6 7 8 9  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # 创建索引（数据库名） result = es.indices.delete(index=\u0026#34;news3\u0026#34;) print(result) # output：{\u0026#39;acknowledged\u0026#39;: True}   插入数据 插入数据有两种方式，index和create方法，这两种方法的区别是使用index时可以不指定id它会自动生成。 create方法内部其实是调用了index方法，是对index方法的封装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} print(result1) result2 = es.index(index=\u0026#34;news3\u0026#34;, document={\u0026#34;title2\u0026#34;:\u0026#34;hello world\u0026#34;}) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;hXNtqYIB7TtztTgDmn0v\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 1, \u0026#39;_primary_term\u0026#39;: 1} print(result2)   更新数据 更新需要指定id和内容。除了使用update更新外还可以使用index更新数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} # print(result1) result = es.update(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, doc={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛很漂亮啊\u0026#34;}) print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 2, \u0026#39;result\u0026#39;: \u0026#39;updated\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 2, \u0026#39;_primary_term\u0026#39;: 1}   更新数据后输出结果还多了一个_version:2每次更新数据都会更新版本号。\n删除数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) # result1 = es.create(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, document={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛漂亮\u0026#34;}) # # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 1, \u0026#39;result\u0026#39;: \u0026#39;created\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 0, \u0026#39;_primary_term\u0026#39;: 1} # print(result1) # result = es.update(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;, doc={\u0026#34;title\u0026#34;:\u0026#34;蓝洁瑛很漂亮啊\u0026#34;}) # print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 2, \u0026#39;result\u0026#39;: \u0026#39;updated\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 2, \u0026#39;_primary_term\u0026#39;: 1} result = es.delete(index=\u0026#34;news3\u0026#34;, id=\u0026#34;1\u0026#34;) print(result) # {\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;_version\u0026#39;: 3, \u0026#39;result\u0026#39;: \u0026#39;deleted\u0026#39;, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 2, \u0026#39;successful\u0026#39;: 1, \u0026#39;failed\u0026#39;: 0}, \u0026#39;_seq_no\u0026#39;: 3, \u0026#39;_primary_term\u0026#39;: 1}   删除数据后_version的版本再次增加①。\n查询数据 由于查询数据的方法多种多样请参考es查询语法参考。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  from elasticsearch import Elasticsearch es = Elasticsearch(hosts=\u0026#34;http://127.0.0.1:9200/\u0026#34;).options( request_timeout=15, ignore_status=[400, 404] ) properties = { \u0026#34;title\u0026#34;: {\u0026#39;type\u0026#39;: \u0026#39;text\u0026#39;} } # es.indices.delete(index=\u0026#34;news\u0026#34;) result = es.indices.put_mapping(index=\u0026#39;news3\u0026#39;, properties=properties) print(result) # datas = [ # { # \u0026#34;title\u0026#34;: \u0026#34;人之初性本善\u0026#34;, # }, # { # \u0026#34;title\u0026#34;: \u0026#34;中国越发强大，深感骄傲\u0026#34; # }, # { # \u0026#34;title\u0026#34;: \u0026#34;义不容辞\u0026#34; # } # ] # for data in datas: # es.index(index=\u0026#34;news3\u0026#34;, document=data) # # result = es.search(index=\u0026#34;news3\u0026#34;) # print(result) query = { \u0026#34;match\u0026#34;:{ \u0026#34;title\u0026#34;:\u0026#34;骄傲\u0026#34; } } result = es.search(index=\u0026#34;news3\u0026#34;, query=query) print(result) {\u0026#39;acknowledged\u0026#39;: True} # {\u0026#39;took\u0026#39;: 6, \u0026#39;timed_out\u0026#39;: False, \u0026#39;_shards\u0026#39;: {\u0026#39;total\u0026#39;: 1, \u0026#39;successful\u0026#39;: 1, \u0026#39;skipped\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0}, \u0026#39;hits\u0026#39;: {\u0026#39;total\u0026#39;: {\u0026#39;value\u0026#39;: 1, \u0026#39;relation\u0026#39;: \u0026#39;eq\u0026#39;}, \u0026#39;max_score\u0026#39;: 1.6285465, \u0026#39;hits\u0026#39;: [{\u0026#39;_index\u0026#39;: \u0026#39;news3\u0026#39;, \u0026#39;_type\u0026#39;: \u0026#39;_doc\u0026#39;, \u0026#39;_id\u0026#39;: \u0026#39;iHOLqYIB7TtztTgDHH20\u0026#39;, \u0026#39;_score\u0026#39;: 1.6285465, \u0026#39;_source\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;中国越发强大，深感骄傲\u0026#39;}}]}}   插眼：由于7.X版本和8.X版本方法变动太大，后续深入再记录学习\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103-2/","summary":"Redis缓存存储 Redis是一个基于内存的、高效的键值型非关系型数据库 连接Redis 1 2 3 4 5 6 from redis import StrictRedis redis = StrictRedis(host=\u0026#34;localhost\u0026#34;, port=6379, db=0, password=\u0026#34;winhansonserver\u0026#34;) print(redis.get(\u0026#34;a1\u0026#34;)) redis.close() 下面还可以使用Co","title":"崔庆才爬虫书阅读摘要03 2"},{"content":"MySQL(关系型数据库) MySQL的学习将以代码的形式来展开，并辅以少量文字解释。\nsql语句使用格式化符%s来构造，避免使用+的拼接字符串、对于数据的插入、更新、删除操作都需要db的commit方法才行，如果执行失败则使用rollback执行回调。标准写法如下：\n1 2 3 4 5  try: cursor.execute(sql, (带传入的值)) db.commit() except: db.rollback()   下面给出所使用的数据表的生成代码：\n1 2 3 4 5 6  CREATETABLE`students`(`id`varchar(255)NOTNULL,`name`varchar(255)NOTNULL,`age`int(11)NOTNULL,PRIMARYKEY(`id`))ENGINE=InnoDBDEFAULTCHARSET=utf8mb4;  插入语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import pymysql person1 = {\u0026#34;id\u0026#34;:\u0026#34;2022812\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;Jhon\u0026#34;, \u0026#34;age\u0026#34;:18} # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = \u0026#34;insert into students(id, name, age) values(%s, %s, %s)\u0026#34; try: cursor.execute(sql, (person1[\u0026#34;id\u0026#34;], person1[\u0026#34;name\u0026#34;], person1[\u0026#34;age\u0026#34;])) db.commit() except: db.rollback() db.close()   对上述代码进行优化如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import pymysql data = {\u0026#34;id\u0026#34;:\u0026#34;202281203\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;Saliy\u0026#34;, \u0026#34;age\u0026#34;:18} table = \u0026#34;students\u0026#34; keys = \u0026#34;, \u0026#34;.join(data.keys())\t# output：\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34; values = \u0026#34;, \u0026#34;.join([\u0026#39;%s\u0026#39;] * len(data)) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;insert into {table}({keys}) values({values})\u0026#34; try: cursor.execute(sql, tuple(data.values())) print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   实现了动态SQL语句的实现，sql中的列名就是通过传入数据字典的键名来表示，excute方法的第一个参数是sql变量，第二个参数是传入data的值所构造的元组实现。\n更新语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import pymysql data = {\u0026#34;id\u0026#34;:\u0026#34;202281204\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;, \u0026#34;age\u0026#34;:118} table = \u0026#34;students\u0026#34; keys = \u0026#34;, \u0026#34;.join(data.keys()) values = \u0026#34;, \u0026#34;.join([\u0026#39;%s\u0026#39;] * len(data)) # print(values) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() # ON DUPLICATE KEY UPDATE是如果主键已经存在就会执行更新，否则就是插入 sql = f\u0026#34;insert into {table}({keys}) values ({values}) ON DUPLICATE KEY UPDATE \u0026#34; update = \u0026#34;,\u0026#34;.join([\u0026#34;{key}= %s\u0026#34;.format(key=key) for key in data]) sql += update try: if cursor.execute(sql, tuple(data.values())*2): print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   删除语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pymysql condition = \u0026#34;age \u0026gt; 100\u0026#34; table = \u0026#34;students\u0026#34; # print(values) # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;delete from {table}where {condition}\u0026#34; try: if cursor.execute(sql): print(\u0026#34;Successful\u0026#34;) db.commit() except: print(\u0026#34;Failed\u0026#34;) db.rollback() db.close()   查询语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import pymysql table = \u0026#34;students\u0026#34; condition = \u0026#34;age \u0026lt; 100\u0026#34; # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;select * from {table}where {condition}\u0026#34; try: cursor.execute(sql) print(\u0026#34;查询的数据条数为：\u0026#34;, cursor.rowcount) one = cursor.fetchone() print(\u0026#34;第一条：\u0026#34;, one) alldata = cursor.fetchall() print(alldata) print(\u0026#34;--------\u0026#34;) for i in alldata: print(i) except: print(\u0026#34;ERROR\u0026#34;) db.close()    查询语句比较推荐下面的写法可以减小开销（当数据体量很大时）\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  import pymysql table = \u0026#34;students\u0026#34; condition = \u0026#34;age \u0026lt; 100\u0026#34; # 声明了一个MySQL连接对象 db db = pymysql.connect(host=\u0026#34;localhost\u0026#34;, # MySQL运行的host（即IP） user=\u0026#34;root\u0026#34;, # 用户名 password=\u0026#34;1234\u0026#34;, # 密码 database=\u0026#34;spiders\u0026#34;) # 所连接的数据库 # 获得MySQL的操作游标，可以利用游标执行sql语句 cursor = db.cursor() sql = f\u0026#34;select * from {table}where {condition}\u0026#34; try: cursor.execute(sql) print(\u0026#34;查询的数据条数为：\u0026#34;, cursor.rowcount) one = cursor.fetchone() while one: print(\u0026#34;Row\u0026#34;, one) one = cursor.fetchone() except: print(\u0026#34;ERROR\u0026#34;) db.close()   MongoDB MongoDB中的集合类似于关系数据库中的表。 可能由于更新后某些语法失效，具体可以查看官方文档\n插入语句 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import pymongo # 创建mongodb连接对象 client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) # client = pymongo.MongoClient(\u0026#39;mongodb://localhost:27017/\u0026#39;) 该语句相当于上面的语句作用相同 # 连接数据库 db = client[\u0026#34;test\u0026#34;] # db = client.test 作用与上面的语句相同 # 指定集合（相当于关系数据库里面的表） collection = db[\u0026#34;students\u0026#34;] # collection = db.students student1 = { \u0026#34;id\u0026#34;:202281301, \u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;, \u0026#34;age\u0026#34;:18, \u0026#34;gender\u0026#34;:\u0026#34;man\u0026#34; } result = collection.insert_one(student1) print(result) print(result.inserted_id)   在MongoDB中每条数据都有一个_id属性作为唯一标识，如果没有显式指明该属性，那么MongoDB会自动产生一个object类型的_id属性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import pymongo # 创建mongodb连接对象 client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) # client = pymongo.MongoClient(\u0026#39;mongodb://localhost:27017/\u0026#39;) 该语句相当于上面的语句作用相同 # 连接数据库 db = client[\u0026#34;test\u0026#34;] # db = client.test 作用与上面的语句相同 # 指定集合（相当于关系数据库里面的表） collection = db[\u0026#34;students\u0026#34;] # collection = db.students student1 = { \u0026#34;id\u0026#34;:202281302, \u0026#34;name\u0026#34;:\u0026#34;jocker\u0026#34;, \u0026#34;age\u0026#34;:22, \u0026#34;gender\u0026#34;:\u0026#34;man\u0026#34; } student2 = { \u0026#34;id\u0026#34;:202281303, \u0026#34;name\u0026#34;:\u0026#34;sarah\u0026#34;, \u0026#34;age\u0026#34;:23, \u0026#34;gender\u0026#34;:\u0026#34;woman\u0026#34; } result = collection.insert_many([student1, student2]) print(result)   查询语句 1 2 3 4 5 6 7 8 9 10  import pymongo client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] result = collection.find_one({\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;}) print(result) # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;), \u0026#39;id\u0026#39;: 202281301, \u0026#39;name\u0026#39;: \u0026#39;tom\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;gender\u0026#39;: \u0026#39;man\u0026#39;} print(result[\u0026#34;id\u0026#34;])   还可以使用_id属性来查询不过需要使用bson库里面的Objectid：\n1 2 3 4 5 6 7 8 9 10  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] result = collection.find_one({\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;)}) print(result) # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;62f700cc3f5cb4db631a52fa\u0026#39;), \u0026#39;id\u0026#39;: 202281301, \u0026#39;name\u0026#39;: \u0026#39;tom\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;gender\u0026#39;: \u0026#39;man\u0026#39;} print(result[\u0026#34;id\u0026#34;])      符号 含义 例子     $lt 小于（less than） result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$lt\u0026quot;:50}})   $gt 大于（greater than） result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$gt\u0026quot;:50}})   $lte 小于等于 同上   $gte 大于等于 同上   $ne 不等于（not equal to） 同上   $in 在范围内 result = collection.find({\u0026ldquo;age\u0026rdquo;:{\u0026quot;$in\u0026quot;:[18, 23]}})   $nin 不在范围内 同上   $regex 匹配正则表达式 语法同上   $exists 属性是否存在 同上   $text 文本查询 {\u0026quot;$text\u0026quot;:{\u0026quot;$search\u0026quot;:\u0026ldquo;hanson\u0026rdquo;}}    等等还有需要功能符号方法\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 查询多个语句相当于一个生成器 result = collection.find({\u0026#34;age\u0026#34;:{\u0026#34;$in\u0026#34;:[18, 23]}}) for i in result: print(i[\u0026#34;name\u0026#34;])   计数 统计查询结果包含多少条数据使用count方法\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 查询多个语句相当于一个生成器 result = collection.estimated_document_count() result1 = collection.count_documents({\u0026#34;age\u0026#34;:{\u0026#34;$lt\u0026#34;:22}}) print(result1) print(result)   偏移 可能只想获取某几个元素就是用skip方法，还可以使用limit指定想要的结果个数\n1 2 3 4 5 6 7 8 9 10 11 12  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] # 表示忽略前两个元素获得第三个及之后 result = collection.find().sort(\u0026#34;name\u0026#34;,pymongo.ASCENDING).skip(2) # print(result) for i in result: print(i[\u0026#34;name\u0026#34;])   更新 1 2 3 4 5 6 7 8 9 10 11 12 13 14  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] condition = {\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;} # 查询要更新的数据 student = collection.find_one(condition) # 修改数据 student[\u0026#34;age\u0026#34;] = 100 # 调用update_one方法将 原条件 和 修改后的数据传入 result = collection.update_one(condition, {\u0026#34;$set\u0026#34;:student}) print(result)   更新后的结果可以调用matched_count和modified_count属性获得匹配的数据条数和影响的数据条数、\n删除 1 2 3 4 5 6 7 8 9 10  import pymongo from bson.objectid import ObjectId client = pymongo.MongoClient(host=\u0026#34;localhost\u0026#34;) db = client[\u0026#34;test\u0026#34;] collection = db[\u0026#34;students\u0026#34;] condition = {\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;} result = collection.delete_one(condition) print(result) print(result.deleted_count)   MongoDB除了提供上面的方法还提供有一些组合方法例如find_one_and_delete和find_one_and_update\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8103/","summary":"MySQL(关系型数据库) MySQL的学习将以代码的形式来展开，并辅以少量文字解释。 sql语句使用格式化符%s来构造，避免使用+的拼接字符串","title":"崔庆才爬虫书阅读摘要03"},{"content":"对爬虫书的第三章摘录 xpath xpath属性多值的匹配 当某个节点的某个属性拥有多个值的时候该如何匹配，使用contains方法\n1 2 3 4 5 6 7 8 9 10 11  from lxml import etree tree = etree.parse(\u0026#34;../../ts.html\u0026#34;) alls = tree.xpath(\u0026#34;/html/body/ol/li/a[@href=\u0026#39;http://www.baidu.com\u0026#39;]/text()\u0026#34;) print(alls) # 下面两个语句均可打印 # alls2 = tree.xpath(\u0026#34;//li/a[contains(@href, \u0026#39;666\u0026#39;)]/text()\u0026#34;) alls2 = tree.xpath(\u0026#34;//li/a[contains(@href, \u0026#39;http://www.baidu.com\u0026#39;)]/text()\u0026#34;) print(alls2)   上面使用contains方法给第一个参数传入属性名称，第二个参数传入属性值（属性里面包含的值）。\nxpath的多属性匹配 1 2 3 4 5 6 7 8 9 10 11 12  from lxml import etree html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com 666\u0026#34; id=\u0026#34;baidu\u0026#34;\u0026gt;百度1\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com 666\u0026#34; id=\u0026#34;baidu2\u0026#34;\u0026gt;百度2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34; id=\u0026#34;guge\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026#34;\u0026#34;\u0026#34; tree = etree.HTML(html) alls = tree.xpath(\u0026#39;//li/a[contains(@href, \u0026#34;666\u0026#34;) and @id=\u0026#34;baidu2\u0026#34;]/text()\u0026#39;) print(alls)\t# [\u0026#39;百度2\u0026#39;]   xpath详细方法参考：\nhttps://www.cnblogs.com/mxjhaima/p/13775844.html\nBeautifulSoup  BeautifulSoup是Python的一个HTML或XML解析库，可以将输入的文档自动转换成Unicode编码，将输出的文档转换成UTF-8编码，除非文档没有指定的编码方式，不然无需指定编码方式。\n 对html 进行解析时，Beautiful Soup 支持解析器的选取，通常来说在选择解析器的时候需要记住两个点，一个是解析时间，另一个是兼容性。\nBeautiful Soup 支持的解析器有四种：\nhtml.parser，lxml，lxml-xml，html5lib。\n这四种解析器的优点和缺点，文档中下面的表很容易看清：\n在BeautifulSoup 4.0以前，大部分使用的是html.parser ，现在更多使用的是lxml。\n 基础代码演示如下：\n soup.Tagname.string：获取标签里面的内容 soup.Tagename.attrs[\u0026ldquo;属性名\u0026rdquo;]：获取属性的值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 将要解析的字符串传入BeautifulSoup对象，该对象会自动补全不标准的HTML字符串 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.prettify()) # 将要解析的字符串以标准的缩进格式输出，只是缩进 print(soup.title.string) # output：The Dormouse\u0026#39;s story print(soup.p.attrs[\u0026#34;class\u0026#34;]) # output：[\u0026#39;title\u0026#39;]   节点选择器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p) # \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;   bs4.element.Tag即上面soup.标签名的形式只能匹配第一个符合的节点后面复合的节点会被忽略。\n提取信息  获取名称 利用name属性即可获取  1  print(soup.title.name) #output：title    获取属性 一个节点可能有多个属性，可以通过选中该节点的元素后调用attrs来获取属性  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34; id=\u0026#34;这是第一个P节点id\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p.attrs)\t# {\u0026#39;class\u0026#39;: [\u0026#39;title\u0026#39;], \u0026#39;id\u0026#39;: \u0026#39;这是第一个P节点id\u0026#39;} print(soup.p.attrs[\u0026#39;class\u0026#39;])\t# [\u0026#39;title\u0026#39;] print(soup.p.attrs[\u0026#39;id\u0026#39;])\t# 这是第一个P节点id print(soup.a.attrs[\u0026#39;id\u0026#39;])\t# link1 print(soup.a.attrs[\u0026#39;class\u0026#39;])\t# [\u0026#39;sister\u0026#39;]   **注意：**返回的属性值有的是字符串有的是字符串组成的列表，如果属性的值是唯一的就返回字符串，否则返回列表。\n 嵌套选择 即在节点里面再继续.Tagname即可接着往下选择节点  关联选择 有时候在选择的过程中不能一步到位选择到想要的节点就要先选中某一个节点然后再以它为基准再次选择子节点、父节点、兄弟节点等等。\n方法选择器  find_all，查询所有符合条件的元素 find_all(self, name=None, attrs={}, recursive=True, text=None,limit=None, **kwargs)\n 使用该方法仍然可以嵌套继续调用。\n name：使用标签名来查询元素 attrs：传入一些属性作为字典来查询 text：还可以根据节点的文本来进行匹配（可以传入字符串也可以传入正则表达式）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import re from bs4 import BeautifulSoup html_doc = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34; id=\u0026#34;这是第一个P节点id\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) result = soup.find_all(name=\u0026#34;a\u0026#34;) result2 = soup.find_all(attrs={\u0026#34;id\u0026#34;:\u0026#34;link1\u0026#34;}) result3 = soup.find_all(text=re.compile(r\u0026#34;.*The.*\u0026#34;)) print(result) print(result2) print(result3) print(result[0].text) # 获取文本内容    find方法只会匹配第一个符合条件的元素节点\n pyquery 一个CCS选择器的解析库\nparsel 一个可以穿插XPath和CSS选择器的解析库，同时支持XPath和CSS解析，并支持对内容进行提取和修改，同时也糅合了正则表达式的提取功能，灵活且强大。\n基本用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick\u0026#34;) print(oneobj) twoobj = selector.css(\u0026#34;#10086\u0026#34;) print(twoobj)   输出截图如下：\n提取文本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick\u0026#34;) for i in oneobj: print(i.get()) # 使用text()才可以提取文本内容否则还是返回节点对象内容 print(\u0026#34;加上text()\u0026#34; + i.xpath(\u0026#34;.//text()\u0026#34;).get()) twoobj = selector.css(\u0026#34;#10086\u0026#34;) # *用来提取所有子节点（包括纯文本节点）提取文本则加上::text threeobj = selector.css(\u0026#34;#10010 *::text\u0026#34;) print(twoobj.xpath(\u0026#34;.//text()\u0026#34;).get()) print(threeobj.get())   提取属性 提取属性的方法基本上和上面的大体相同\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # coding:utf-8 # @Time : 2022/8/11 16:54 # @Author : 软件1194温铭军 # @file : parsel01.py # $software : PyCharm from parsel import Selector xmls = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; # 创建一个Selector对象，并传入字符串，然后创建了一个Selector对象 selector = Selector(text=xmls) oneobj = selector.xpath(\u0026#34;//nick/@id\u0026#34;) for i in oneobj: print(i.get()) twoobj = selector.css(\u0026#34;#10086 *::attr(id)\u0026#34;) # *用来提取所有子节点（包括纯文本节点）提取文本则加上::text threeobj = selector.css(\u0026#34;#10010 *::text\u0026#34;) print(twoobj.get()) print(threeobj.get())   正则提取 正则提取以及更多的用法可以参见parsel的官方文档在这里不过多介绍\n","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8102/","summary":"对爬虫书的第三章摘录 xpath xpath属性多值的匹配 当某个节点的某个属性拥有多个值的时候该如何匹配，使用contains方法 1 2 3 4 5 6 7 8 9 10 11","title":"崔庆才爬虫书阅读摘要02"},{"content":"对爬虫书的第二章摘录 urllib的高级用法  urllib的一些基本用法已经可以满足绝大部分的使用，但有时候又需要一些更高级的操作（Cookie处理，代理设置，登录问题等），需要使用更高级的用法实现。\n 介绍urllib.request模块里面比较重要的两个类：BaseHandler类、OpenerDirector类也可以称为Opener。我们之前使用的urlopen方法就是urllib库为我们提供的一个Opener。\n为什么要引入这两个类，就是为了完成一些更高要求的请求功能。可以利用Handler类来构建Opener类。\n验证示例（登录需要认证的网站） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # coding:utf-8 # @Time : 2022/8/8 8:03 # @file : urllib01.py # $software : PyCharm from urllib.request import HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm, build_opener from urllib.error import URLError \u0026#34;\u0026#34;\u0026#34; urlopen的高级使用方法1 访问一些网站时遇到认证窗口 url：https://ssr3.scrape.center/ \u0026#34;\u0026#34;\u0026#34; url = \u0026#34;https://ssr3.scrape.center/\u0026#34; username = \u0026#34;admin\u0026#34; password = \u0026#34;admin\u0026#34; # 建立一个处理验证的Handler类 p = HTTPPasswordMgrWithDefaultRealm()\t# 具有默认领域的 HTTP 密码管理器 p.add_password(realm=None, uri=url, user=username, passwd=password) auth_handler = HTTPBasicAuthHandler(p)\t# HTTP 基本身份验证处理程序 # 通过刚刚建立的Handler类来构建一个Opener opener = build_opener(auth_handler) try: result = opener.open(url) html = result.read().decode(\u0026#34;utf8\u0026#34;) print(html) except URLError as e: print(e.errno)   代理 则通过使用from urllib.request import ProxyHandler来构建一个处理代理的Handler。 步骤还是同上先创建一个Handler处理对象然后再通过Handler对象构建一个Opener对象。\n示例|片段代码：\n1 2 3 4 5 6  from urllib.request import ProxyHandler,build_opener proxy = ProxyHandler({ \u0026#39;http\u0026#39;:XXXX, \u0026#39;https\u0026#39;:XXXX }) opener = build_opener(proxy)   urllib-Cookie Cookie部分的内容用到时了解即可\n处理异常 urllib库中的error模块定义了由request模块产生的异常。由request模块产生的异常都由urllib中的error模块捕获。\n URLError：时error异常模块的基类，由request模块产生的异常都可以由该模块来捕获处理 HTTPError：专门处理HTTP请求错误，例如认证失败等  示例|片段代码：\n1 2 3 4 5 6 7  from urllib import error, request url = \u0026#34;XXXXX\u0026#34; try: response = request.urlopeen(url=url) except error.URLError as e: print(e.reason)   解析链接（parse模块） urllib.parse.urlparse：实现URL的识别和分段urlparse(url, scheme='', allow_fragments=True)\n1 2 3 4 5 6 7 8  import urllib.parse url = \u0026#39;https://www.baidu.com/s?ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;tn=88093251_47_hao_pg\u0026amp;wd=4399\u0026#39; result = urllib.parse.urlparse(url=url) print(result) # output:ParseResult(scheme=\u0026#39;https\u0026#39;, netloc=\u0026#39;www.baidu.com\u0026#39;, path=\u0026#39;/s\u0026#39;, # params=\u0026#39;\u0026#39;, query=\u0026#39;ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;tn=88093251_47_hao_pg\u0026amp;wd=4399\u0026#39;, # fragment=\u0026#39;\u0026#39;)   urllib.parse.urlunparse：用于构造（拼接）URL。这个方法接受的参数是一个可迭代对象，长度必须为6\n1 2 3 4 5  import urllib.parse url2 = [\u0026#34;https\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;index.html\u0026#34;, \u0026#34;user\u0026#34;, \u0026#34;a=6\u0026#34;, \u0026#34;comment\u0026#34;] print(urllib.parse.urlunparse(url2)) # output: https://www.baidu.com/index.html;user?a=6#comment   和上面两个方法类似的方法是：urlsplit和urlunsplit只不过这两个方法不再单独解析params这部分，所以指定长度为5\nurllib.parse.urlencode：该方法在构造GET请求参数的时候非常有用\n1 2 3 4 5 6 7 8 9  import urllib.parse params = { \u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;:18 } baseurl = \u0026#34;https://www.baidu.com\u0026#34; url = baseurl + urllib.parse.urlencode(params) print(url) # output: https://www.baidu.comname=hanson\u0026amp;age=18   urllib.parse.urljoin：上面的urlunparse和urlparse两个方法可以完成对链接的合并，但有长度和参数限制。所以可以使用更方便的urljoin来代替。 该方法的参数：我们提供一个基础的链接来作为第一个参数，将新的链接作为第二个参数，该方法会分析基础链接（第一个参数）然后对新链接缺失部分进行补充，最后返回结果。\n1 2 3 4 5 6  # 电影天堂案例 import urllib.parse parturl = \u0026#34;/html/gndy/dyzz/20220806/62861.html\u0026#34; baseurl = \u0026#39;https://m.dytt8.net/index2.htm\u0026#39; print(urllib.parse.urljoin(baseurl, parturl)) # https://m.dytt8.net/html/gndy/dyzz/20220806/62861.html   在爬虫爬到一些网页的子url时可以使用该方法进行方便快捷的拼接。\n反序列化 urllib.parse.parse_qs：将一串GET请求参数转换回字典\n1 2 3 4  import urllib.parse baseurl = \u0026#34;https://www.baidu.com/s?ie=UTF-8\u0026amp;wd=4399\u0026#34; print(urllib.parse.parse_qs(baseurl)) #output: {\u0026#39;https://www.baidu.com/s?ie\u0026#39;: [\u0026#39;UTF-8\u0026#39;], \u0026#39;wd\u0026#39;: [\u0026#39;4399\u0026#39;]}   urllib.parse.parse_qsl：将一串GET请求参数转换回元组组成的列表\nurllib.parse.quote：将内容转化成URL编码的格式，并且会自动将中文字符转换成URL编码\nurllib.parse.unquote：将URL编码转换成原来的样子，并且还原成中文字符\nrequests的高级用法 先插入一些requests的基础代码\n1 2 3 4 5 6 7 8 9 10 11  import requests url = \u0026#39;http://httpbin.org/get\u0026#39; params = { \u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;:18 } r = requests.get(url=url, params=params) print(r.text) print(r.url) # http://httpbin.org/get?name=hanson\u0026amp;age=18   抓取二进制数据（音频|视频|图片）等等 图片，视频，音频这些文件本质上就是一堆的二进制码组成，由于有特定的保存格式和对应的解析方式才可以让我们看到不同。\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#39;https://scrape.center/favicon.ico\u0026#39; response = requests.get(url=url) with open(url.split(\u0026#39;/\u0026#39;)[-1], \u0026#34;wb\u0026#34;) as f: f.write(response.content) print(\u0026#34;OVER\u0026#34;)   response.content，打印出来会发现前面是带有b\u0026rsquo;XXX\u0026rsquo;的bytes类型的数据。\n文件上传 1 2 3 4 5 6 7  import requests url = \u0026#39;http://httpbin.org/post\u0026#39; files = {\u0026#34;file\u0026#34;: open(\u0026#34;../../favicon.ico\u0026#34;, \u0026#34;rb\u0026#34;)} r = requests.post(url=url, files=files) print(r.text)   上述代码输出如下图：\nCookie设置 写一个简单的示例来展开获取Cookie的流程\n1 2 3 4 5 6 7 8 9 10  import requests url = \u0026#39;https://www.baidu.com\u0026#39; response = requests.get(url=url) cookies = response.cookies for k, v in cookies.items(): print(k,v)   session的维持 第一次如果使用了post方法登录了某个网站，再用get想获取登录后的网站源代码，这相当于打开了两个不同的浏览器，登录信息不会保留。\n解决该方法主要是维持同一个session（会话），要利用到下面说的session对象。\nrequests.session()相当于requests对象，二者的GET请求都要通过get。\n1 2 3 4 5 6 7 8 9 10 11 12  import requests url = \u0026#39;https://www.baidu.com\u0026#39; session = requests.session() # 这步相当于requests response = session.get(url=url) cookies = response.cookies for k, v in cookies.items(): print(k,v)   SSL证书验证 某些网站可能没有设置好HTTPS证书，或者HTTPS证书不被CA机构认可，就有可能出现SSL证书错误的提示，只需要使用varify=False不去验证证书即可\n简易代码repsonse = requests.get(url=url, varify=False)\ntime超时设置 语法基本和urllib的语法相同较为简单，这里不做过多的介绍\n身份验证 前面urllib中提到的个别会用身份认证的网站，以下展开requests的用法\n 方法1较为繁琐的方法\n 1 2 3 4 5 6 7 8 9 10  import requests from requests.auth import HTTPBasicAuth \u0026#34;\u0026#34;\u0026#34; 身份认证，较为繁琐的方法 \u0026#34;\u0026#34;\u0026#34; url = \u0026#39;https://ssr3.scrape.center/\u0026#39; response = requests.get(url=url, auth=HTTPBasicAuth(\u0026#34;admin\u0026#34;, \u0026#34;admin\u0026#34;)) print(response.status_code)    方法2较为简单的方法\n直接传入一个元组给auth会默认调用HTTPBasicAuth这个方法\n 1 2 3 4 5 6 7 8 9 10  import requests \u0026#34;\u0026#34;\u0026#34; 身份认证，较为繁琐的方法 \u0026#34;\u0026#34;\u0026#34; url = \u0026#39;https://ssr3.scrape.center/\u0026#39; response = requests.get(url=url, auth=(\u0026#34;admin\u0026#34;, \u0026#34;admin\u0026#34;)) print(response.status_code)   代理设置 代理设置也较为简单\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#39;XXXXXXXXX\u0026#39; proxy = { \u0026#39;http\u0026#39;:\u0026#39;XXXX\u0026#39;, \u0026#39;https\u0026#39;:\u0026#39;XXXX\u0026#39; } response = requests.get(url=url, proxies=proxy)   httpx 前面的urllib和requests可以爬取绝大多数的网站，但是对于使用HTTP2.0协议的网站，目前（2022.8）urllib和requests是不支持爬取的。所以对于使用了HTTP2.0的网站可以使用httpx库来进行爬取。\n 注意：httpx默认使用的是HTTP1.1，需要我们手动声明才能使用HTTP2.0，代码如下\n 1 2 3 4 5 6 7  import httpx url = \u0026#34;https://spa16.scrape.center/\u0026#34; client = httpx.Client(http2=True) # 开启对HTTP2.0的支持《==》可以理解为声明了一个HTTP2.0的requests对象 response = client.get(url=url) print(response.text)   其余的用法基本和requests相同。\nhttpx支持异步请求\n可以了解下re的sub用法 ","permalink":"https://hanson00.github.io/posts/technology/python/%E5%B4%94%E5%BA%86%E6%89%8D%E7%88%AC%E8%99%AB%E4%B9%A6%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%8101/","summary":"对爬虫书的第二章摘录 urllib的高级用法 urllib的一些基本用法已经可以满足绝大部分的使用，但有时候又需要一些更高级的操作（Cookie","title":"崔庆才爬虫书阅读摘要01"},{"content":"浏览器在分析JS中的作用 xhr属于AJAX请求\n想找到对应加密解密函数的实现可以在找到后,把鼠标放上去然后定位(找来源)\nJS中方法是在前面定义,函数是在后面\n不能够回溯的js方法，一般是js的内置方法\n方法:\n函数:\nfilter过滤 可以在filter中输入部分url地址，对所有的url地址起到一定的过滤效果\n对人人网分析  模拟登录前的抓包情况如下\n  模拟登录后的抓包\n  想要搜索某文件里面的关键字可以在search中搜索\n JS解析  通过initiator定位js文件  从上面标记的initator中点击就进入到下图（initiator就是触发前面的文件的来源）\n也可以在点击该文件后的initiator中进入\n通过search关键字搜索定位js文件  在使用search搜索时会遇到以下弊端：\n  如果网页的数据是加密数据是无法搜索的\n  页面的数据是js生成的，异步加载的数据等\n  通过元素绑定的事件监听函数（比如浏览器的登录click等）可以通过EventListeners再次查到js文件来源\n  JS调试  关键字 XHR断点 路径 启动器引导  示例：\n 找到要分析的路径Request URL: https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\n然后复制后面的路径weapi/comment/resource/comments/get?csrf_token=\n  接着点击Sources(源代码)在左侧的XHR/fetch Breakpoints中粘贴刚刚的路径\n  接着刷新网页进行JS调试\n ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%ABjs%E5%88%86%E6%9E%90%E6%8A%80%E5%B7%A7/","summary":"浏览器在分析JS中的作用 xhr属于AJAX请求 想找到对应加密解密函数的实现可以在找到后,把鼠标放上去然后定位(找来源) JS中方法是在前面定义","title":"爬虫js分析技巧"},{"content":"Scrapy学习 使用Scrapy创建爬虫项目使用如下步骤：  创建爬虫项目scrapy startproject 项目名字(项目名字不允许使用数字开头，不能包含中文) 创建爬虫文件，在spiders文件夹中创建爬虫文件  先进入到要创建爬虫文件的目录cd 项目名字\\项目名字\\spiders 创建爬虫文件scrapy genspider 爬虫文件的名字 要爬取的网页（要爬取的网页一般不写http只写www）   运行爬虫代码scrapy crawl 爬虫名字   scrapy项目结构 项目名字\n​\t项目名字\n​\tspiders文件（存储的是爬虫文件）\n​\tinit\n​\t自定义的爬虫文件 核心功能文件\n​\tinit\n​\titmes\t定义数据结构的地方爬取的数据包含哪些，是一个继承自scrapy.Item的类，通俗的说要下载哪些数据\n​\tmiddlewares\t中间件\t代理\n​\tpipelines\t管道 里面只有一个类用来处理下载的数据，默认值是300优先级，值越小优先级越高（1-1000）\n​\tsettings\t配置文件robots协议、UA等配置\n response的属性和方法  response.text\t获取的是响应字符串 response.body\t获取的是二进制数据 response.xpath\t可以直接通过xpath方法来解析response中的内容\nresponse.extract()\t提取seletor对象的data属性值，现在一般使用下面的get()\nresponse.extract_first()\t提取seletor列表的第一个数据下面，现在一般使用getall()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import scrapy class TcSpider(scrapy.Spider): name = \u0026#39;tc\u0026#39; allowed_domains = [\u0026#39;https://hanson00.github.io/posts/technology/python/python-json/\u0026#39;] start_urls = [\u0026#39;https://hanson00.github.io/posts/technology/python/python-json/\u0026#39;] def parse(self, response, **kwargs): # 获取网页字符串 # content = response.text # 二进制数据 # content = response.body # content = response.xpath(\u0026#39;//*[@id=\u0026#34;top\u0026#34;]/main/article/header/div[1]/a[4]/text()\u0026#39;) # print(\u0026#34;=====================================\u0026#34;) # print(content.extract()) content = response.xpath(\u0026#39;//*[@id=\u0026#34;top\u0026#34;]/main/article/header/div[1]/a[4]\u0026#39;).get() print(\u0026#34;=====================================\u0026#34;) print(content)    管道的使用  现在pipelines中定义自己的功能管道 在settings中开启管道 写管道功能  settings数据库配置  导入settingsfrom scrapy.utils.project import get_project_settings 在管道文件中配置数据库连接参数  settings = get_project_settings() self.host = settings['DB_HOST']，参考在settings中的配置等等。    scrapy架构  引擎 下载器 spiders 调度器 管道  官网架构概述图如下：\n自己总结版如下：\n 引擎向spiders要url 引擎在得到url后，将要爬取的url给调度器 调度器将url生成对象放入指定的队列中 调度器从队列中出队一个请求 引擎将请求交给下载器进行处理 下载器发送请求到互联网获取数据 下载器将response数据返回给引擎 引擎将数据再次给到spiders spiders通过xpath解析数据，得到url或者数据 spiders将数据或者url返回给引擎 引擎判断是数据还是url，是数据则交给管道处理，是url则交给调度器处理(即重复上诉逻辑)  日志 一般推荐直接在settings文件中写入LOG_FILE = log.log来解决执行时调式显示过多。\n项目学习 当当网项目  爬取数据（spiders爬虫文件） 定义数据结构（items） 开启管道（settings） 下载数据（pipeline）  项目目录：\n 记得在settings中开启管道\n items.py文件如下： 定义要下载的数据的数据结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Define here the models for your scraped items # # See documentation in: # https://docs.scrapy.org/en/latest/topics/items.html import scrapy class ScrapyDangdangItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 就是下载的数据有哪些 src_pic = scrapy.Field() name = scrapy.Field() price = scrapy.Field()   spiders.dang.py文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  import scrapy from ..items import ScrapyDangdangItem class DangSpider(scrapy.Spider): name = \u0026#39;dang\u0026#39; # allowed_domains在多页下载中可以只写域名（category.dangdang.com）或者可以不写 allowed_domains = [\u0026#39;category.dangdang.com\u0026#39;] start_urls = [\u0026#39;http://category.dangdang.com/cp01.01.07.00.00.00.html\u0026#39;] page = 1 def parse(self, response, **kwargs): print(\u0026#34;=============================\u0026#34;) li_list = response.xpath(\u0026#39;//ul[@id=\u0026#34;component_59\u0026#34;]/li\u0026#39;) for i in li_list: src_pic = i.xpath(\u0026#39;.//img/@data-original\u0026#39;).extract_first() name = i.xpath(\u0026#39;./p[@name=\u0026#34;title\u0026#34;]/a/@title\u0026#39;).extract_first() price = i.xpath(\u0026#39;./p[@class=\u0026#34;price\u0026#34;]/span[1]/text()\u0026#39;).extract_first() if src_pic: src_pic = src_pic else: src_pic = i.xpath(\u0026#39;.//img/@src\u0026#39;).extract_first() # print(src_pic, name, price) # 开始管道封装下载数据 # 导入items的数据结构 book = ScrapyDangdangItem(src_pic=src_pic, name=name, price=price) # 获取一个book就将一个book交给pipeline yield book if self.page \u0026lt; 100: self.page = self.page + 1 more_url = f\u0026#34;http://category.dangdang.com/pg{self.page}-cp01.01.07.00.00.00.html\u0026#34; # scrapy.Request就是scrapy的get请求，还可以通过Request中的mete传递参数， # 并用response.meta[\u0026#39;参数名\u0026#39;]接受，callback就是要执行的那个函数 yield scrapy.Request(url=more_url, callback=self.parse)   pipeline文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  # Define your item pipelines here # # Don\u0026#39;t forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter import requests # 如果想要使用管道，就要在setting中开启管道 class ScrapyDangdangPipeline: # 该方法会在爬虫文件开始前就执行 def open_spider(self, spider): self.f = open(\u0026#34;book.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) def process_item(self, item, spider): #item就是yield后面的book对象 # 如果单单在这里使用文件的读写会使文件读写过于频繁 # with open(\u0026#39;book.json\u0026#39;, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: # f.write(str(item)) self.f.write(str(item)) return item # 该方法会在爬虫文件结束时执行 def close_spider(self, spider): self.f.close() # 多条管道的开启 # 1.首先创建一个管道类 # 2.在settings中开启管道 class DangDangDownloadPipeline: def process_item(self, item, spider): #item就是yield后面的book对象 url = \u0026#34;http:\u0026#34; + item.get(\u0026#34;src_pic\u0026#34;) name = \u0026#34;./books/\u0026#34; + item.get(\u0026#34;name\u0026#34;) + \u0026#34;.jpg\u0026#34; response = requests.get(url=url) with open(name, \u0026#34;wb\u0026#34;) as f: f.write(response.content)   在settings打开管道\n1 2 3 4 5  ITEM_PIPELINES = { # 管道可以有很多个，有优先级，优先级是1-1000，越低越高 \u0026#39;scrapy_dangdang.pipelines.ScrapyDangdangPipeline\u0026#39;: 300, \u0026#39;scrapy_dangdang.pipelines.DangDangDownloadPipeline\u0026#39;:301, }   # scrapy.Request就是scrapy的get请求，还可以通过Request中的mete传递参数，并用response.meta['参数名']接受\nCrawlSpider-读书网  继承自scrapy.Spider CrawlSpider可以自定义规则，再解析html内容的时候可以根据链接规则提取出指定的链接，然后载器向这些链接发送请求 提取链接  allow = ()，\t正则表达式，提取符合正则表达式的链接 restrict_xpaths = (), 提取符合xpath规则的链接 restrict_css = (), 提取符合选择器规则的链接     rules = ( Rule(LinkExtractor(allow=r'/book/1158_\\d+.html'), callback=\u0026lsquo;parse_item\u0026rsquo;, follow=False), ) 的\tfollwo如果为True则表示为是否按照链接提取规则一直跟进提取\n 使用步骤：\n 创建项目scrapy startproject 项目名 进入到spiders后执行scrapy genspider -t crawl 爬虫文件名 爬取链接  spiders.read.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from ..items import ScrapyReadbookItem class ReadSpider(CrawlSpider): name = \u0026#39;read\u0026#39; allowed_domains = [\u0026#39;www.dushu.com\u0026#39;] start_urls = [\u0026#39;https://www.dushu.com/book/1158_1.html\u0026#39;] rules = ( Rule(LinkExtractor(allow=r\u0026#39;/book/1158_\\d+.html\u0026#39;), callback=\u0026#39;parse_item\u0026#39;, follow=False), ) def parse_item(self, response): name = response.xpath(\u0026#39;//div[@class=\u0026#34;book-info\u0026#34;]/h3/a/@title\u0026#39;).getall() book = ScrapyReadbookItem(name=name) yield book   POST请求 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import scrapy import json class TestpostSpider(scrapy.Spider): name = \u0026#39;testpost\u0026#39; # POST请求 如果没有参数那么请求就没有意义所以start_urls也没有用了导致需要执行的parse方法无效 allowed_domains = [\u0026#39;fanyi.baidu.com\u0026#39;] # start_urls = [\u0026#39;https://fanyi.baidu.com/sug\u0026#39;] # # def parse(self, response, **kwargs): # pass def start_requests(self): url = \u0026#39;https://fanyi.baidu.com/sug\u0026#39; data = { \u0026#39;kw\u0026#39;:\u0026#39;apple\u0026#39; } yield scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second) def parse_second(self, response): content = response.text res = json.loads(content) print(res)   items.py\n1 2 3 4 5 6 7 8 9 10 11 12  # Define here the models for your scraped items # # See documentation in: # https://docs.scrapy.org/en/latest/topics/items.html import scrapy class ScrapyReadbookItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() name = scrapy.Field()   pipeline.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # Define your item pipelines here # # Don\u0026#39;t forget to add your pipeline to the ITEM_PIPELINES setting # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html # useful for handling different item types with a single interface from itemadapter import ItemAdapter class ScrapyReadbookPipeline: def open_spider(self, spider): self.f = open(\u0026#34;book.json\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) def process_item(self, item, spider): self.f.write(str(item)) return item def close_spider(self, spider): self.f.close()   注意： 如果请求的接口是html为结尾的是不需要加\\的。\n在开发爬虫项目中如果使用crawlspider时一定要注意爬取的start_urls与后续要爬取的url之间的规律与规则。\n","permalink":"https://hanson00.github.io/posts/technology/python/scrapy%E5%AD%A6%E4%B9%A01.0/","summary":"Scrapy学习 使用Scrapy创建爬虫项目使用如下步骤： 创建爬虫项目scrapy startproject 项目名字(项目名字不允许使用数字开头，不能包含中文) 创建","title":"Scrapy学习1.0"},{"content":"汽车之家项目概述 首先对要爬取的网页代码进行分析https://you.autohome.com.cn/index/searchkeyword?keyword=%E9%A9%AC%E5%B0%94%E4%BB%A3%E5%A4%AB 在查看网页源代码后发现没有要的数据就对该页面进行抓包如下：\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  import requests import re from concurrent.futures import ThreadPoolExecutor url0 = \u0026#39;https://you.autohome.com.cn\u0026#39; # 保存下载图片 def download_pic(urlpic): response3 = requests.get(urlpic) file_path = urlpic.split(\u0026#34;/\u0026#34;)[-1] with open(f\u0026#34;pic/{file_path}\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(response3.content) def main(): # 1.在原始的列表帖子页面获取 url = \u0026#34;https://you.autohome.com.cn/summary/getsearchresultlist\u0026#34; params1 = { \u0026#39;ps\u0026#39;: 20, \u0026#39;pg\u0026#39;: 0, \u0026#39;type\u0026#39;: 4, \u0026#39;q\u0026#39;: \u0026#39;马尔代夫\u0026#39;, \u0026#39;dataType\u0026#39;: \u0026#39;4,9\u0026#39;, \u0026#39;_\u0026#39;: 1659220150512, } response = requests.get(url=url,params=params1) dic1 = response.json() list_blog_place = response.json()[\u0026#39;result\u0026#39;][\u0026#39;hitlist\u0026#39;] # 获得每个文章的部分url列表 # print(dic1) pic_re = re.compile(\u0026#39;{\u0026#34;imgurl\u0026#34;:\u0026#34;(?P\u0026lt;pic\u0026gt;.*?)\u0026#34;\u0026#39;) for i in list_blog_place: url4 = i[\u0026#39;url\u0026#39;] # print(url4) realurl = url0 + url4 response2 = requests.get(url=realurl) pichtml = response2.text # print(response2.text) allpic = pic_re.finditer(pichtml) piclist = [] for i in allpic: # print(i.group(\u0026#39;pic\u0026#39;)) piclist.append(i.group(\u0026#39;pic\u0026#39;)) # 下载图片 with ThreadPoolExecutor(10) as t: for pic in piclist: t.submit(download_pic, pic) response.close() if __name__ == \u0026#39;__main__\u0026#39;: main()   使用from urllib.parse import urljoin 对URL进行方便的拼接，大大方便拼接完整url，在m3u8中非常适用。替换小部分即可\n1 2 3 4 5 6 7 8 9 10 11 12  for i in list_blog_place: url4 = i[\u0026#39;url\u0026#39;] # print(url4) realurl = urljoin(url, url4) response2 = requests.get(url=realurl) pichtml = response2.text # print(response2.text) allpic = pic_re.finditer(pichtml) piclist = [] for i in allpic: # print(i.group(\u0026#39;pic\u0026#39;)) piclist.append(i.group(\u0026#39;pic\u0026#39;))   ","permalink":"https://hanson00.github.io/posts/technology/python/%E7%88%AC%E8%99%AB-%E6%B1%BD%E8%BD%A6%E4%B9%8B%E5%AE%B6/","summary":"汽车之家项目概述 首先对要爬取的网页代码进行分析https://you.autohome.com.cn/index/searchkeyword","title":"爬虫 汽车之家"},{"content":"json模块 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。下面给出json模块的常用方法\n json.dump()：将Python数据对象以JSON格式数据流写入文件 json.load()：解析包含JSON数据的文件为Python对象 json.dumps()：将Python数据对象转换成JSON格式的字符串 json.loads()：将包含JSON的字符串、字节以及字节数组解析成为Python对象  生成的json字符串一定是双引号引导的。\n Python中的json模块的使用方法\n    方法 功能     json.dump(obj,fp) 将Python数据类型转换并保存到json格式的文件里   json.dumps(s) 将Python数据类型转换成json格式的字符串   json.load(fp) 将json格式的文件读取数据并转换成Python的类型   json.loads(s) 将json格式的字符串转换成Python类型的字符串     dump和dumps方法\n 1 2 3 4 5 6 7 8 9 10 11 12  import json person = {\u0026#34;name\u0026#34;: \u0026#34;hanson\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;tel\u0026#34;:[123, 321]} print(type(person)) print(person) # 将字典转换成json格式的字符串,intent指定缩进，不指定也行 j = json.dumps(person, indent=2) print(type(j)) # \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; print(j) with open(\u0026#34;../saves/data.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(person, f)    loads方法\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # coding:utf-8 # @Time : 2022/8/11 20:39 # @Author : 软件1194温铭军 # @file : json01.py # $software : PyCharm import json person = {\u0026#34;name\u0026#34;: \u0026#34;杰克\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;tel\u0026#34;:[123, 321]} # 将字典转换成json格式的字符串,intent指定缩进，不指定也行,ensure_ascii显示中文 j = json.dumps(person, indent=2, ensure_ascii=False) print(j) # 将json string 转换 python对象 pythonObj = json.loads(j) print(type(pythonObj)) # output: \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; print(pythonObj)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import json dic = { \u0026#34;name\u0026#34; : \u0026#34;饿饿饿\u0026#34;, \u0026#34;age\u0026#34;:[ { \u0026#34;shuah\u0026#34;:\u0026#34;sodjsijd\u0026#34; } ] } with open(\u0026#34;22.txt\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(dic, f) # 将上述json数据写入文件 with open(\u0026#34;22.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: print(f.read()) # output:{\u0026#34;name\u0026#34;: \u0026#34;\\u997f\\u997f\\u997f\u0026#34;, \u0026#34;age\u0026#34;: [{\u0026#34;shuah\u0026#34;: \u0026#34;sodjsijd\u0026#34;}]} print(\u0026#34;--------------------------------\u0026#34;) with open(\u0026#34;22.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: print(json.load(f)) # output:{\u0026#39;name\u0026#39;: \u0026#39;饿饿饿\u0026#39;, \u0026#39;age\u0026#39;: [{\u0026#39;shuah\u0026#39;: \u0026#39;sodjsijd\u0026#39;}]} print(\u0026#34;OVER\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-json/","summary":"json模块 JSON（JavaScript Object Notation）是一种轻量级的数据交换格式。下面给出json模块的常用方法 json.dump(","title":"Python Json"},{"content":"Python Flask的项目实战 项目概述：使用Flask将爬虫爬取到的数据结合echarts可视化的展示到页面(Python+Flask+Echarts)，使用jQuery和AJAX实现数据交互与局部刷新\n效果展示如下：\n项目目录如下：\n爬取并存储疫情数据 数据来源：腾讯疫情实时数据\n 获取疫情数据\nget_tenxun_nowdate_data()：将返回爬取到的腾讯疫情数据\nupdate_details()|insert_history()|update_history()：对数据进行数据库的存储操作\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166  # @file : spiders1.py # $software : PyCharm import re import time import pymysql import requests import time import traceback from configsql import get_conn, close_conn def get_tenxund_history_ata_and_all(): \u0026#34;\u0026#34;\u0026#34; 第一个函数先返回历史数据 :return:history[历史数据], nowdate_data[省级数据] \u0026#34;\u0026#34;\u0026#34; history = {} url = \u0026#34;https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=chinaDayListNew,chinaDayAddListNew\u0026amp;limit=30\u0026#34; response = requests.get(url=url) data_resp = response.json() for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;chinaDayListNew\u0026#34;]: ds = \u0026#34;2022.\u0026#34; + i[\u0026#34;date\u0026#34;] tup = time.strptime(ds, \u0026#34;%Y.%m.%d\u0026#34;) ds = time.strftime(\u0026#34;%Y-%m-%d\u0026#34;, tup) confirm = i[\u0026#34;confirm\u0026#34;] suspect = i[\u0026#34;suspect\u0026#34;] heal = i[\u0026#34;heal\u0026#34;] dead = i[\u0026#34;dead\u0026#34;] history[ds] = {\u0026#34;confirm\u0026#34;: confirm, \u0026#34;suspect\u0026#34;: suspect, \u0026#34;heal\u0026#34;: heal, \u0026#34;dead\u0026#34;: dead} for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;chinaDayAddListNew\u0026#34;]: ds = \u0026#34;2022.\u0026#34; + i[\u0026#34;date\u0026#34;] tup = time.strptime(ds, \u0026#34;%Y.%m.%d\u0026#34;) ds = time.strftime(\u0026#34;%Y-%m-%d\u0026#34;, tup) confirm = i[\u0026#34;confirm\u0026#34;] suspect = i[\u0026#34;suspect\u0026#34;] heal = i[\u0026#34;heal\u0026#34;] dead = i[\u0026#34;dead\u0026#34;] history[ds].update({\u0026#34;confirm_add\u0026#34;: confirm, \u0026#34;suspect_add\u0026#34;: suspect, \u0026#34;heal_add\u0026#34;: heal, \u0026#34;dead_add\u0026#34;: dead}) nowdate_data = get_tenxun_nowdate_data() return history, nowdate_data def get_tenxun_nowdate_data(): details = [] # 当日详细数据 url = \u0026#34;https://api.inews.qq.com/newsqa/v1/query/inner/publish/modules/list?modules=localCityNCOVDataList,diseaseh5Shelf\u0026#34; data = { \u0026#34;modules\u0026#34;: \u0026#34;localCityNCOVDataList,diseaseh5Shelf\u0026#34; } response = requests.post(url=url) data_resp = response.json() update_time = data_resp[\u0026#34;data\u0026#34;][\u0026#34;diseaseh5Shelf\u0026#34;][\u0026#34;lastUpdateTime\u0026#34;] for i in data_resp[\u0026#34;data\u0026#34;][\u0026#34;diseaseh5Shelf\u0026#34;][\u0026#34;areaTree\u0026#34;][0][\u0026#34;children\u0026#34;]: province = i[\u0026#34;name\u0026#34;] # 省名 # print(province) for city_info in i[\u0026#34;children\u0026#34;]: # if city_info[\u0026#34;name\u0026#34;] == \u0026#34;地区待确认\u0026#34;: # city_info[\u0026#34;name\u0026#34;] = province # print(city_info[\u0026#34;name\u0026#34;]) city = city_info[\u0026#34;name\u0026#34;] confirm = city_info[\u0026#34;total\u0026#34;][\u0026#34;confirm\u0026#34;] confirm_add = city_info[\u0026#34;total\u0026#34;][\u0026#34;continueDayZeroLocalConfirmAdd\u0026#34;] heal = city_info[\u0026#34;total\u0026#34;][\u0026#34;heal\u0026#34;] dead = city_info[\u0026#34;total\u0026#34;][\u0026#34;dead\u0026#34;] details.append([update_time, province, city, confirm, confirm_add, heal, dead]) return details def update_details(): cursor = None conn = None try: li = get_tenxund_history_ata_and_all()[1]\t# 获取上面爬取的数据 conn, cursor = get_conn()\t# 获取游标对象 sql = \u0026#34;insert into details(update_time, province, city, confirm, confirm_add, heal, dead) \u0026#34; \\ \u0026#34;values(%s, %s, %s, %s, %s, %s, %s)\u0026#34; # 下面两句是获取最新的时间，并判断是否要更新数据 sql_query = \u0026#34;select %s=(select update_time from details order by id desc limit 1)\u0026#34; cursor.execute(sql_query, li[0][0]) if not cursor.fetchone()[0]: print(f\u0026#34;{time.asctime()}开始更新数据\u0026#34;) for item in li: cursor.execute(sql, item) conn.commit() print(f\u0026#34;{time.asctime()}数据更新完毕\u0026#34;) else: print(f\u0026#34;{time.asctime()}已经是最新数据了\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def insert_history(): cursor = None conn = None try: dic = get_tenxund_history_ata_and_all()[0] print(f\u0026#34;{time.asctime()}开始插入历史数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into history values(%s, %s, %s, %s, %s, %s, %s, %s, %s)\u0026#34; for k, v in dic.items(): cursor.execute(sql, [k, v.get(\u0026#34;confirm\u0026#34;), v.get(\u0026#34;confirm_add\u0026#34;), v.get(\u0026#34;suspect\u0026#34;), v.get(\u0026#34;suspect_add\u0026#34;), v.get(\u0026#34;heal\u0026#34;), v.get(\u0026#34;heal_add\u0026#34;), v.get(\u0026#34;dead\u0026#34;), v.get(\u0026#34;dead_add\u0026#34;)]) conn.commit() print(f\u0026#34;{time.asctime()}插入历史数据完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def update_history(): cursor = None conn = None try: dic = get_tenxund_history_ata_and_all()[0] print(f\u0026#34;{time.asctime()}开始更新历史数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into history values(%s, %s, %s, %s, %s, %s, %s, %s, %s)\u0026#34; sql_query = \u0026#34;select confirm from history where ds=%s\u0026#34; for k, v in dic.items(): if not cursor.execute(sql_query, k): cursor.execute(sql, [k, v.get(\u0026#34;confirm\u0026#34;), v.get(\u0026#34;confirm_add\u0026#34;), v.get(\u0026#34;suspect\u0026#34;), v.get(\u0026#34;suspect_add\u0026#34;), v.get(\u0026#34;heal\u0026#34;), v.get(\u0026#34;heal_add\u0026#34;), v.get(\u0026#34;dead\u0026#34;), v.get(\u0026#34;dead_add\u0026#34;)]) conn.commit() print(f\u0026#34;{time.asctime()}插入历史数据完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) def get_baidu_hot(): url = \u0026#34;https://top.baidu.com/board?tab=realtime\u0026#34; response = requests.get(url=url) # print(response.text) reword = re.compile(\u0026#39;\u0026#34;word\u0026#34;:\u0026#34;(?P\u0026lt;content\u0026gt;.*?)\u0026#34;\u0026#39;) result = reword.finditer(response.text) # for i in result: # print(i.group(\u0026#34;content\u0026#34;)) content = [i.group(\u0026#34;content\u0026#34;) for i in result] # print(content) return content response.close() def update_baidu_hot(): cursor = None conn = None try: content = get_baidu_hot() print(f\u0026#34;{time.asctime()}开始更新热搜数据\u0026#34;) conn, cursor = get_conn() sql = \u0026#34;insert into hotsearch(dt, content) values(%s, %s)\u0026#34; ts = time.strftime(\u0026#34;%Y-%m-%d%X\u0026#34;) for i in content: cursor.execute(sql, (ts, i)) conn.commit() print(f\u0026#34;{time.asctime()}热搜数据更新完毕\u0026#34;) except: traceback.print_exc() finally: close_conn(conn, cursor) if __name__ == \u0026#39;__main__\u0026#39;: insert_history() update_details() update_history() update_baidu_hot()    存储疫情数据库的配置\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # @file : configsql.py # $software : PyCharm import pymysql def get_conn(): # 打开数据库连接 db = pymysql.connect(host=\u0026#39;localhost\u0026#39;, user=\u0026#39;root\u0026#39;, password=\u0026#39;1234\u0026#39;, database=\u0026#39;cov\u0026#39;) # 创建一个游标对象 cursor cursor = db.cursor() return db, cursor def close_conn(conn, cursor): if cursor: cursor.close() if conn: conn.close()   Flask业务逻辑的编写  右上角的时间显示的代码\n 后台返回时间\n1 2 3 4 5 6 7 8  # @file : utils.py # $software : PyCharm import time import pymysql def gettime(): time_str = time.strftime(\u0026#34;%Y{}%m{}%d{}%X\u0026#34;) return time_str.format(\u0026#34;年\u0026#34;,\u0026#34;月\u0026#34;,\u0026#34;日\u0026#34;)   controller.js\n1 2 3 4 5 6 7 8 9 10 11  function gettime() { $.ajax({ url: \u0026#34;/time\u0026#34;, timeout: 10000, success: function (data) { $(\u0026#34;#time\u0026#34;).html(data) }, error: function (xhr, type, errorThrown) { } }) }   main.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;疫情监控\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;css/main.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/echarts.min.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/china.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;title\u0026#34;\u0026gt;全国疫情实时跟踪\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;time\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;l1\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;l2\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;c1\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;num\u0026#34;\u0026gt;\u0026lt;h1\u0026gt;--------\u0026lt;/h1\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计确诊\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;剩余疑似\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计治愈\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;txt\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;累计死亡\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;c2\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;r1\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;r2\u0026#34;\u0026gt;\u0026lt;h3\u0026gt;在家办公，减少出门次数，勤洗手多通风，尽自己的最大努力为这场战疫献出自己的一份力。中国加油!\u0026lt;/h3\u0026gt;\u0026lt;br\u0026gt; \u0026lt;h3\u0026gt;拼尽全力抗击疫情，医护人员是最美逆行者!为自己加油，让我们一起并肩作战，让疫情早日消散!\u0026lt;/h3\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_center.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/controller.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_left1.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_left2.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/ec_right1.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   controller.js完整代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90  function gettime() { $.ajax({ url: \u0026#34;/time\u0026#34;, timeout: 10000, success: function (data) { $(\u0026#34;#time\u0026#34;).html(data) }, error: function (xhr, type, errorThrown) { } }) } function getc1data() { $.ajax({ url: \u0026#34;/c1\u0026#34;, success: function (data) { $(\u0026#34;.num h1\u0026#34;).eq(0).text(data.confirm); $(\u0026#34;.num h1\u0026#34;).eq(1).text(data.suspect); $(\u0026#34;.num h1\u0026#34;).eq(2).text(data.heal); $(\u0026#34;.num h1\u0026#34;).eq(3).text(data.dead); }, error: function (xhr, type, errorThrown) { } }) } function getc2data() { $.ajax({ url: \u0026#34;/c2\u0026#34;, success: function (data) { optionMap.series[0].data = data.data ec_center.setOption(optionMap) }, error: function (xhr, type, errorThrown) { } }) } function getl1data() { $.ajax({ url: \u0026#34;/l1\u0026#34;, success: function (data) { ec_left1_Option.xAxis[0].data = data.day ec_left1_Option.series[0].data = data.confirm ec_left1_Option.series[1].data = data.suspect ec_left1_Option.series[2].data = data.heal ec_left1_Option.series[3].data = data.dead ec_left1.setOption(ec_left1_Option) }, error: function (xhr, type, errorThrown) { } }) } function getl2data() { $.ajax({ url: \u0026#34;/l2\u0026#34;, success: function (data) { ec_left2_Option.xAxis[0].data = data.day ec_left2_Option.series[0].data = data.confirm_add ec_left2_Option.series[1].data = data.suspect_add ec_left2.setOption(ec_left2_Option) }, error: function (xhr, type, errorThrown) { } }) } function get_r1_data(){ $.ajax({ url:\u0026#34;/r1\u0026#34;, success: function(data) { ec_right1_Option.xAxis.data=data.city; ec_right1_Option.series[0].data=data.confirm; ec_right1.setOption(ec_right1_Option); } }) } getc1data() getl1data() getl2data() getc2data() get_r1_data() setInterval(gettime, 1000) setInterval(getc1data, 1000*10) setInterval(getc2data, 10000*10) setInterval(getl1data, 10000*10) setInterval(getl2data, 10000*10) setInterval(get_r1_data, 10000*10)   Echarts语法简明  初始化echarts实例  1 2  //初始化echarts实例 var ec_center = echarts.init(document.getElementById(\u0026#34;c2\u0026#34;),\u0026#34;dark\u0026#34;);    找到你想要的数据可视化模板代码，或者自己编写\n  使用制定的配置项和数据显示图表\n1 2  //使用制定的配置项和数据显示图表 ec_center.setOption(optionMap);     例如：\nec_center.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  //初始化echarts实例 var ec_center = echarts.init(document.getElementById(\u0026#34;c2\u0026#34;),\u0026#34;dark\u0026#34;); var mydata = [] var optionMap = { title: { text: \u0026#39;\u0026#39;, subtext: \u0026#39;\u0026#39;, x: \u0026#39;left\u0026#39; }, tooltip: { trigger: \u0026#39;item\u0026#39; }, //左侧小导航图标  visualMap: { show: true, x: \u0026#39;left\u0026#39;, y: \u0026#39;bottom\u0026#39;, textStyle: { fontSize: 8 }, splitList: [{ start: 1, end: 9 }, { start: 10, end: 99 }, { start: 100, end: 999 }, { start: 1000, end: 9999 }, { start: 10000 } ], color: [\u0026#39;#8A3310\u0026#39;,\u0026#39;#C64918\u0026#39;, \u0026#39;#E55B25\u0026#39;,\u0026#39;#F2AD92\u0026#39;, \u0026#39;#F9DCD1\u0026#39;] }, //配置属性  series: [{ name: \u0026#39;累积确诊人数\u0026#39;, type: \u0026#39;map\u0026#39;, mapType: \u0026#39;china\u0026#39;, roam: false, itemStyle: { normal: { borderWidth: .5, borderColor: \u0026#39;#009fe8\u0026#39;, areaColor: \u0026#39;#ffefd5\u0026#39; }, emphasis: { borderWidth: .5, borderColor: \u0026#39;#4b0082\u0026#39;, areaColor: \u0026#39;#fff\u0026#39; } }, label: { normal: { show: true, //省份名称  fontSize: 8 }, emphasis: { show: true, fontSize: 8 } }, data: mydata //数据  }] }; //使用制定的配置项和数据显示图表  ec_center.setOption(optionMap);   ec_left1.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  var ec_left1 = echarts.init(document.getElementById(\u0026#39;l1\u0026#39;), \u0026#34;dark\u0026#34;); var ec_left1_Option = { title: { text: \u0026#34;全国累计趋势\u0026#34;, textStyle: { //color:\u0026#39;white\u0026#39;,  }, left: \u0026#39;left\u0026#39;, }, tooltip: { trigger: \u0026#39;axis\u0026#39;, axisPointer: { type: \u0026#39;line\u0026#39;, lineStyle: { color: \u0026#39;#7171C6\u0026#39; } }, }, legend: { data: [\u0026#34;累计确诊\u0026#34;, \u0026#34;现有疑似\u0026#34;, \u0026#34;累积治愈\u0026#34;, \u0026#34;累计死亡\u0026#34;], left: \u0026#34;right\u0026#34; }, //图形位置  grid: { left: \u0026#39;4%\u0026#39;, right: \u0026#39;6%\u0026#39;, bottom: \u0026#39;4%\u0026#39;, top: 50, containLabel: true }, xAxis: [{ type: \u0026#39;category\u0026#39;, data: [] // \u0026#39;01.24\u0026#39;, \u0026#39;01.25\u0026#39;, \u0026#39;01.26\u0026#39;  }], yAxis: [{ type: \u0026#39;value\u0026#39;, axisLabel: { show: true, color: \u0026#39;white\u0026#39;, fontSize: 12, formatter: function(value) { if (value \u0026gt;= 1000) { value = value / 1000 + \u0026#39;k\u0026#39;; } return value; } }, //y轴线设置显示  axisLine: { show: true }, //与x轴平行的线样式  splitLine: { show: true, lineStyle: { color: \u0026#39;#17273B\u0026#39;, width: 1, type: \u0026#39;solid\u0026#39;, } } }], series: [{ name: \u0026#34;累计确诊\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] // 260, 406, 529  }, { name: \u0026#34;现有疑似\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: []}, { name: \u0026#34;累积治愈\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] },{ name: \u0026#34;累计死亡\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }] }; ec_left1.setOption(ec_left1_Option)   ec_left2.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  var ec_left2 = echarts.init(document.getElementById(\u0026#39;l2\u0026#39;), \u0026#34;dark\u0026#34;); var ec_left2_Option = { tooltip: { trigger: \u0026#39;axis\u0026#39;, //指示器  axisPointer: { type: \u0026#39;line\u0026#39;, lineStyle: { color: \u0026#39;#7171C6\u0026#39; } }, }, legend: { data: [\u0026#39;新增确诊\u0026#39;, \u0026#39;新增疑似\u0026#39;], left: \u0026#34;right\u0026#34; }, //标题样式  title: { text: \u0026#34;全国新增趋势\u0026#34;, textStyle: { color:\u0026#39;yellow\u0026#39;, fontSize: 16 }, left: \u0026#39;left\u0026#39; }, //图形位置  grid: { left: \u0026#39;4%\u0026#39;, right: \u0026#39;6%\u0026#39;, bottom: \u0026#39;4%\u0026#39;, top: 50, containLabel: true }, xAxis: [{ type: \u0026#39;category\u0026#39;, //x轴坐标点开始与结束点位置都不在最边缘  // boundaryGap : true,  data: [] }], yAxis: [{ type: \u0026#39;value\u0026#39;, //y轴字体设置  //y轴线设置显示  axisLine: { show: true }, axisLabel: { show: true, color: \u0026#39;white\u0026#39;, fontSize: 12, formatter: function(value) { if (value \u0026gt;= 1000) { value = value / 1000 + \u0026#39;k\u0026#39;; } return value; } }, //与x轴平行的线样式  splitLine: { show: true, lineStyle: { color: \u0026#39;#17273B\u0026#39;, width: 1, type: \u0026#39;solid\u0026#39;, } } }], series: [{ name: \u0026#34;新增确诊\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }, { name: \u0026#34;新增疑似\u0026#34;, type: \u0026#39;line\u0026#39;, smooth: true, data: [] }] }; ec_left2.setOption(ec_left2_Option)   ec_right1.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  var ec_right1=echarts.init(document.getElementById(\u0026#39;r1\u0026#39;),\u0026#34;dark\u0026#34;); var ec_right1_Option={ title:{ text:\u0026#34;非湖北地区城市确诊TOP5\u0026#34;, textStyle:{ color:\u0026#39;white\u0026#39;, }, left:\u0026#39;left\u0026#39;, }, color:[\u0026#39;#3398DB\u0026#39;], tooltip:{ trigger:\u0026#39;axis\u0026#39;, axisPointer:{ type:\u0026#39;shadow\u0026#39;, } }, xAxis:{ type:\u0026#39;category\u0026#39;, data:[] }, yAxis:{ type:\u0026#39;value\u0026#39;, }, series:[{ data:[], type:\u0026#39;bar\u0026#39;, barMaxWidth:\u0026#34;50%\u0026#34; }] }; ec_right1.setOption(ec_right1_Option)   ","permalink":"https://hanson00.github.io/posts/technology/python/flask%E7%96%AB%E6%83%85%E5%AE%9E%E6%88%98/","summary":"Python Flask的项目实战 项目概述：使用Flask将爬虫爬取到的数据结合echarts可视化的展示到页面(Python+Flask+Echart","title":"Flask疫情实战"},{"content":"Python编码与爬虫 Python3在内存中使用的是Unicode编码方式存储的，所以不能直接存储和传输，必须要转换成其他编码进行存储和传输。 Python的编码解码过程：源文件===》encode(编码方式)===》decode(解码方式) 。即字符串通过编码转换成字节码(str-\u0026gt;bytes)，字节码通过解码转换成字符串(bytes-\u0026gt;str) 如果在爬虫遇到返回文件不正确时，建议使用encode和decode的方式来处理文本。\n我们在使用requests的时候Python会自动猜测源文件的编码方式，然后根据猜测的编码方式将源文件转换成Unicode编码，而爬虫中出现乱码也就是Python猜测编码方式错误，所以我们需要手动来指定编码方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import sys import locale # 返回当前系统所使用的默认字符编码 print(sys.getdefaultencoding()) # 返回用于转换Unicode文件名至系统文件名所使用的编码 print(sys.getfilesystemencoding()) # 获取默认的区域设置并返回元祖(语言, 编码) print(locale.getdefaultlocale()) # 返回用户设定的文本数据编码 # 文档提到this function only returns a guess print(locale.getpreferredencoding())   网页一般会告诉你是什么编码方式，而Python默认是使用UTF-8解码，例如我们可以指定编码字符集response.encoding = \u0026quot;GBK\u0026quot;\n  一般可以使用res.text.encode(res.encoding).decode(\u0026quot;GBK\u0026quot;)或者res.text.encode(res.encoding).decode(\u0026quot;UTF-8\u0026quot;)可以解决编码问题   1 2 3 4 5 6 7 8 9 10 11 12 13 14  import requests url = \u0026#39;https://www.dy2018.com/\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\u0026#34; } res = requests.get(url=url, headers=headers) print(res.encoding) # res.encoding = \u0026#34;gb2312\u0026#34; print(res.text.encode(res.encoding).decode(\u0026#34;GBK\u0026#34;)) res.close()    使用res.encoding = \u0026quot;指定的编码方式\u0026quot;再打印即可   1 2 3 4 5 6 7 8 9 10 11 12 13  import requests url = \u0026#39;https://www.dy2018.com/\u0026#39; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\u0026#34; } res = requests.get(url=url, headers=headers) res.encoding = \u0026#34;gb2312\u0026#34; print(res.text) res.close()   ","permalink":"https://hanson00.github.io/posts/technology/python/python%E7%BC%96%E7%A0%81%E6%B5%85%E8%B0%88/","summary":"Python编码与爬虫 Python3在内存中使用的是Unicode编码方式存储的，所以不能直接存储和传输，必须要转换成其他编码进行存储和传输","title":"Python编码浅谈"},{"content":"Python-Flask学习  微框架、简洁、只做它需要做的，给开发者提供了很大的扩展性。 Flask和相应的插件写得很好，如果实在找不到相应插件，就动手造轮子。 开发效率非常高，比如使用SQLAlchemy的ORM操作数据库可以节省开发者大量书写sql的时间。 默认的Jinija2模板引擎替换成其他模板引擎都是非常方便的。 开发项目时的命名尽量避开Python自带的命名   一般普通开发者的项目结构如下： 外置数据库配置文件是为了防止导包循环\n配置文件 官方配置文档：https://dormousehole.readthedocs.io/en/latest/config.html\n配置项文件里面都是变量名大写\n视图函数(路由函数)转URL 知道视图函数(路由函数)从而寻找对应的URL\n使用url_for(函数名,对应的URL实现规则)实现\n页面跳转和重定向  参数传递的两种方式：\n 作为url的组成部分如/book/1 查询字符串如/book?id=1使用request.args.get获取参数   1 2 3 4 5 6 7 8 9  from flask import redirect @app.route(\u0026#34;/profile\u0026#34;) def tttsss(): uid = request.args.get(\u0026#34;id\u0026#34;) if uid: return \u0026#34;用户验证成功\u0026#34; else: return redirect(url_for(\u0026#34;index\u0026#34;))   模板的使用 使用render_template来对模板进行使用from flask import render_template，如果想传递变量到模板则通过变量定义为字典以关键字参数的方式传递render_template(\u0026quot;about.html\u0026quot;, **context)\n知识点：\n  模板过滤器\n  if语句\n  1 2 3 4 5  {% if age \u0026gt;= 18 %} \u0026lt;div\u0026gt;成年\u0026lt;/div\u0026gt; {% if age \u0026lt; 18 %} \u0026lt;div\u0026gt;没成年\u0026lt;/div\u0026gt; {% endif %}       for语句\n  1 2 3 4  @app.route(\u0026#34;/about\u0026#34;) def about(): context = {\u0026#34;username\u0026#34;:\u0026#34;温铭军\u0026#34;} return render_template(\u0026#34;about.html\u0026#34;, **context)    静态文件的使用\n  1  \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#34;static\u0026#34;, filename=\u0026#34;css/about.css\u0026#34;) }}\u0026#34;\u0026gt;     1 2 3 4 5  在蓝图中使用静态文件需要使用蓝图名+.+static来引用 \u0026lt;link href=\u0026#34;{{ url_for(\u0026#39;admin.static\u0026#39;,filename=\u0026#39;about.css\u0026#39;) }}\u0026#34;\u0026gt;       模板的继承 base.html\n1 2 3 4 5 6 7 8 9 10 11  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;{%block title %}{% endblock %}\u0026lt;/title\u0026gt; {% block head %}{% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block body %}{% endblock %} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   about.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  {% extends \u0026#34;base.html\u0026#34; %} {% block title %} 我是关于页面，要善用block+tab生成句子 {% endblock %} {% block head %} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#34;static\u0026#34;, filename=\u0026#34;css/about.css\u0026#34;) }}\u0026#34;\u0026gt; {% endblock %} {% block body %} 这里是{{ username|length }} \u0026lt;h1\u0026gt;温铭军爱学习\u0026lt;/h1\u0026gt; {% endblock %}   蓝图 app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from flask import Flask from apps.book import bp as bookbp from apps.course import bp as coursebp from apps.user import bp as userbp app = Flask(__name__) # 注册蓝图 app.register_blueprint(bookbp) app.register_blueprint(coursebp) app.register_blueprint(userbp) @app.route(\u0026#34;/\u0026#34;) def index(): return \u0026#34;hello\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   1 2 3 4 5 6 7 8 9 10 11 12 13  # coding:utf-8 # @Time : 2022/7/13 20:22 # @Author : 软件1194温铭军 # @file : course.py # $software : PyCharm from flask import Blueprint # 类似于创建app bp = Blueprint(\u0026#34;course\u0026#34;, __name__, url_prefix=\u0026#34;/course\u0026#34;) @bp.route(\u0026#34;/list\u0026#34;) def courselist(): return \u0026#34;课程列表\u0026#34;   Flask连接数据库 1.0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   数据库ORM映射  一对多\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[\u0026#34;SQLALCHEMY_TRACK_MODIFICATIONS\u0026#34;] = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) class User(db.Model): __tablename__ = \u0026#34;user\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) username = db.Column(db.String(20), nullable=False) # 定义ORM模型 class Aticle(db.Model): __tablename__ = \u0026#34;article\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) title = db.Column(db.String(60), nullable=False) content = db.Column(db.Text, nullable=False) # 一对多 # 外键的数据类型要看所引用的字段类型，db.ForeignKey(\u0026#34;表名.字段名\u0026#34;) # 外键是属于数据库层面，不推荐在ORM中直接使用 author_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # ORM绑定关系db.relationship(\u0026#34;所创建的数据库类模型字段class 名\u0026#34;) # 第一个参数是模型的名字，backref代表反向引用代表对方访问我的时候的名称字段 author = db.relationship(\u0026#34;User\u0026#34;, backref=\u0026#34;aticles\u0026#34;) # 将模型映射到数据库 db.drop_all() db.create_all() @app.route(\u0026#34;/onetomany\u0026#34;) def otm(): article = Aticle(title=\u0026#34;555\u0026#34;, content=\u0026#34;文章\u0026#34;) user = User(username=\u0026#34;温铭军\u0026#34;) article.author = user db.session.add(article) db.session.commit() print(user.aticles)\t#上面的反向引用名backref return \u0026#34;数据保存成功otm\u0026#34; @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; @app.route(\u0026#34;/article\u0026#34;) def addarticle(): # # 添加数据 # # 创建数据对象 # article = Aticle(title=\u0026#34;温铭军爱学习\u0026#34;, content=\u0026#34;6666\u0026#34;) # db.session.add(article) # db.session.commit() # return \u0026#34;数据添加成功\u0026#34; # # 查询数据 # article = Aticle.query.filter_by(id=1)[0] # print(article.title) # return \u0026#34;数据查询完成\u0026#34; # # 修改数据 # article = Aticle.query.filter_by(id=1)[0] # article.content = \u0026#34;修改后真的强\u0026#34; # db.session.commit() # return \u0026#34;数据修改成功\u0026#34; # 删除数据 Aticle.query.filter_by(id=1).delete() db.session.commit() return \u0026#34;数据删除成功\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()    一对一表的实现\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112  from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[\u0026#34;SQLALCHEMY_TRACK_MODIFICATIONS\u0026#34;] = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;stuflask\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; app.config[\u0026#34;SQLALCHEMY_DATABASE_URI\u0026#34;] = DB_URI db = SQLAlchemy(app) class UserExtension(db.Model): __tablename__ = \u0026#34;user_extension\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) school = db.Column(db.String(20)) user_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # 一对一引用 db.backref在反向引用时需要传递一些其他参数否则不需要使用 # uselist=False 表示在代表反向引用时使用的是一个对象不是一个列表 user = db.relationship(\u0026#34;User\u0026#34;, backref=db.backref(\u0026#34;extension\u0026#34;, uselist=False)) class User(db.Model): __tablename__ = \u0026#34;user\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) username = db.Column(db.String(20), nullable=False) # 定义ORM模型 class Aticle(db.Model): __tablename__ = \u0026#34;article\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) title = db.Column(db.String(60), nullable=False) content = db.Column(db.Text, nullable=False) # 一对多 # 外键的数据类型要看所引用的字段类型，db.ForeignKey(\u0026#34;表名.字段名\u0026#34;) # 外键是属于数据库层面，不推荐在ORM中直接使用 author_id = db.Column(db.Integer, db.ForeignKey(\u0026#34;user.id\u0026#34;)) # ORM绑定关系db.relationship(\u0026#34;所创建的数据库类模型字段class 名\u0026#34;) # 第一个参数是模型的名字，backref代表反向引用代表对方访问我的时候的名称字段 author = db.relationship(\u0026#34;User\u0026#34;, backref=\u0026#34;aticles\u0026#34;) # 将模型映射到数据库 db.drop_all() db.create_all() @app.route(\u0026#34;/onetomany\u0026#34;) def otm(): article1 = Aticle(title=\u0026#34;555\u0026#34;, content=\u0026#34;文章\u0026#34;) article2 = Aticle(title=\u0026#34;2222\u0026#34;, content=\u0026#34;文章222\u0026#34;) user = User(username=\u0026#34;温铭军\u0026#34;) article1.author = user article2.author = user db.session.add(article1) db.session.add(article2) db.session.commit() # 反向引用 print(user.aticles) return \u0026#34;数据保存成功otm\u0026#34; @app.route(\u0026#34;/onetoone\u0026#34;) def oto(): user = User(username=\u0026#34;wmj\u0026#34;) extension = UserExtension(school=\u0026#34;北京大学\u0026#34;) user.extension = extension db.session.add(user) db.session.commit() return \u0026#34;一对一\u0026#34; @app.route(\u0026#34;/\u0026#34;) def index(): #写一个测试代码验证是否成功 engine = db.get_engine() with engine.connect() as conn: result = conn.execute(\u0026#34;select 1\u0026#34;) print(result.fetchone()) return \u0026#34;hello\u0026#34; @app.route(\u0026#34;/article\u0026#34;) def addarticle(): # # 添加数据 # # 创建数据对象 # article = Aticle(title=\u0026#34;温铭军爱学习\u0026#34;, content=\u0026#34;6666\u0026#34;) # db.session.add(article) #添加数据到数据库 # db.session.commit() # return \u0026#34;数据添加成功\u0026#34; # # 查询数据 # article = Aticle.query.filter_by(id=1)[0] # print(article.title) # return \u0026#34;数据查询完成\u0026#34; # # 修改数据 # article = Aticle.query.filter_by(id=1)[0] # article.content = \u0026#34;修改后真的强\u0026#34; # db.session.commit() # return \u0026#34;数据修改成功\u0026#34; # 删除数据 Aticle.query.filter_by(id=1).delete() db.session.commit() return \u0026#34;数据删除成功\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()   灵活管理ORM与数据库表 不用使用db.create_all()、db.drop_all(),使用前面语句的弊端模型新增或删除字段，只会在第一次生成表的时候处理\n1  from flask_migrate import Migrate   1  migrate = Migrate(app, db)    flask db init flask db migrate -m \u0026ldquo;first commit\u0026rdquo; flask db upgrade  初始化后，后面更新数据库后只需要执行后面两句\nflask中使用cookie和session  cookies：在Flask中操作cookie是通过response对象来操作，可以在response回之前，通过response.set_cookie来设置，这个方法有以下几个参数需要注意：  key：设置的cookie的key。 value：key对应的value。 max_age：改cookie的过期时间，如果不设置，则浏览器关闭后就会自动过期。 expires：过期时间，应该是一个datetime类型。 domain：该cookie在哪个域名中有效。一般设置子域名，比如cms.example.com。 path：该cookie在哪个路径下有效。   session：Flask中的session是通过from flask import session。然后添加值key和value进去即可。并且，Flask中的session机制是将session信息加密，然后存储在cookie中。专业术语叫做client side session。  Flask-WTF表单验证 flask表单验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  from flask import Flask,request,render_template from exts import db from flask_migrate import Migrate import config1 from models import Aticle, User, UserExtension from froms import LoginForm #导入表单 app = Flask(__name__) app.config.from_object(config1) # 绑定app db.init_app(app) migrate = Migrate(app, db) @app.route(\u0026#34;/login\u0026#34;, methods=[\u0026#34;POST\u0026#34;, \u0026#34;GET\u0026#34;]) def login(): if request.method == \u0026#34;GET\u0026#34;: return render_template(\u0026#34;login.html\u0026#34;) else: form = LoginForm(request.form) if form.validate(): return \u0026#34;登录成功\u0026#34; else: return \u0026#34;邮箱或密码失败\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run()    validators:验证器\n 1 2 3 4 5 6 7 8 9 10 11  # coding:utf-8 # @Time : 2022/7/14 12:04 # @Author : 软件1194温铭军 # @file : froms.py # $software : PyCharm import wtforms from wtforms.validators import length, email class LoginForm(wtforms.Form): email = wtforms.StringField(validators=[length(min=5, max=20), email()]) password = wtforms.StringField(validators=[length(min=6, max=15)])   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;登录\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;form action=\u0026#34;/login\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;邮箱：\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; name=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;密码：\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;提交\u0026#34;\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   项目重构例子 app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  from flask import Flask, url_for import config # 导入配置文件 from exts import db,mail from blueprints.qa import bp as qa_bp from blueprints.user import bp as user_bp from flask_migrate import Migrate # from models import EmailModel app = Flask(__name__) # 绑定config配置 app.config.from_object(config) # 初始化，绑定数据库 db.init_app(app) mail.init_app(app) migrate = Migrate(app, db) app.register_blueprint(qa_bp) app.register_blueprint(user_bp) if __name__ == \u0026#39;__main__\u0026#39;: app.run()   models.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # coding:utf-8 # @Time : 2022/7/14 22:46 # @Author : 软件1194温铭军 # @file : models.py # $software : PyCharm from exts import db from datetime import datetime class EmailModel(db.Model): __tablename__ = \u0026#34;emailmodel\u0026#34; id = db.Column(db.Integer, primary_key=True, autoincrement=True) email = db.Column(db.String(20), nullable=False, unique=True) captcha = db.Column(db.String(10), nullable=False) nowtimes = db.Column(db.DateTime, default=datetime.now)   1 2 3 4 5 6 7 8 9  # coding:utf-8 # @Time : 2022/7/14 13:33 # @Author : 软件1194温铭军 # @file : exts.py # $software : PyCharm from flask_sqlalchemy import SQLAlchemy from flask_mail import Mail db = SQLAlchemy() mail = Mail()   config1.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # coding:utf-8 # @Time : 2022/7/12 10:38 # @Author : 软件1194温铭军 # @file : config.py # $software : PyCharm JSON_AS_ASCII = True # 数据库的配置变量 HOSTNAME = \u0026#39;127.0.0.1\u0026#39; PORT = \u0026#39;3306\u0026#39; DATABASE = \u0026#39;flaskpj\u0026#39; USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;1234\u0026#39; DB_URI = f\u0026#39;mysql+pymysql://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{DATABASE}\u0026#39; SQLALCHEMY_DATABASE_URI = DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = True SECRET_KEY = \u0026#34;hanson2000\u0026#34; # 邮箱配置 QQ MAIL_SERVER = \u0026#34;smtp.qq.com\u0026#34; MAIL_PORT = 465 MAIL_USE_TLS = False MAIL_USE_SSL = True MAIL_DEBUG = True # 正式上线用False MAIL_USERNAME = \u0026#34;954952920@qq.com\u0026#34; MAIL_PASSWORD = \u0026#34;bmyrgvagcitvbfdf\u0026#34; MAIL_DEFAULT_SENDER = \u0026#34;954952920@qq.com\u0026#34;   Flask的小点 1  from flask import jsonify\t# 将列表数据转换成json数据返回   如果有一个列表，想要在路由函数中返回该数据则可以用return jsonify(该列表)\n1 2 3 4 5 6 7 8 9  #如果不想定制子路径来传递参数，也可以通过传统的?=的形式来传递参数，例如：/article?id=xxx，这种情况下，可以通过request.args.get(\u0026#39;id\u0026#39;)来获取id的值。如果是post方法，则可以通过request.form.get(\u0026#39;id\u0026#39;)来进行获取。 @app.route(\u0026#34;/book/\u0026lt;int:bookid\u0026gt;\u0026#34;, methods=[\u0026#34;POST\u0026#34;,\u0026#34;GET\u0026#34;]) def onebook(bookid): for boo in books1: if bookid == boo[\u0026#34;id\u0026#34;]: return boo return f\u0026#34;{bookid}is not found\u0026#34;   1 2 3 4 5 6 7  @app.route(\u0026#34;/profile\u0026#34;) def tttsss(): uid = request.args.get(\u0026#34;id\u0026#34;) if uid: return \u0026#34;用户验证成功\u0026#34; else: return redirect(url_for(\u0026#34;index\u0026#34;))   ","permalink":"https://hanson00.github.io/posts/technology/python/flask/","summary":"Python-Flask学习 微框架、简洁、只做它需要做的，给开发者提供了很大的扩展性。 Flask和相应的插件写得很好，如果实在找不到相应插件","title":"Python-Flask"},{"content":"Python-PEP8代码风格指南摘要  PEP8代码风格指南仅仅只是提供建议，最重要是保持与项目内部保持一致性，有时在遇到与代码风格不适用的地方要根据自己的最佳判断进行决定。\n下面给出一些可以忽略代码风格的一些理由：\n 应用代码风格指南时会使代码可读性降低时，不应使用 与历史代码(周围代码)的代码风格不一致时   代码布局 缩进 续行应该使用 Python 的隐式行在括号、方括号和大括号内连接，或者使用悬挂缩进 垂直对齐包裹的元素。使用悬挂缩进时，应考虑以下事项；第一行不应该有任何参数，并且应该使用进一步的缩进来清楚地将自己区分为续行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # 正确的用法: # 与开始分隔符对齐。 foo = long_function_name(var_one, var_two, var_three, var_four) # 添加 4 个空格（额外的缩进级别）以将参数与函数体区分开来。 def long_function_name( var_one, var_two, var_three, var_four): print(var_one) # 悬挂缩进应该增加一个层次。 foo = long_function_name( var_one, var_two, var_three, var_four) # 悬挂缩进*可能*缩进到 4 个空格以外。 foo = long_function_name( var_one, var_two, var_three, var_four)   空格是首选的缩进方法 最大行长  将所有行限制为最多 79 个字符。\n对于结构限制较少的长文本块（文档字符串或注释），行长应限制为 72 个字符。\n包装长行的首选方法是在括号、方括号和大括号内使用 Python 的隐含行继续。通过将表达式括在括号中，可以将长行分成多行。这些应该优先使用反斜杠来继续行。\n 在二元运算符之前换行 1 2 3 4 5 6  #正确的： income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest)   import语句 1 2 3 4  #正确的： import os import sys from flask import Flask,jsonity,request    导入总是放在文件的顶部，就在任何模块注释和文档字符串之后，模块全局变量和常量之前。\n进口应按以下顺序分组：\n 标准库导入。 相关第三方进口。 本地应用程序/库特定的导入。   表达式和语句中的空格 在下面的情况应该避免多余的空格：\n 紧接在圆括号，方括号或大括号内：   1 2 3 4 5  # 正确的: spam(ham[1], {eggs: 2}) # 错误的: spam( ham[ 1 ], { eggs: 2 } )    在结尾的逗号和后面的右括号之间 紧接在逗号、分号或冒号之前 在切片中   1 2 3 4 5 6  # 正确的: ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:] ham[lower:upper], ham[lower:upper:], ham[lower::step] ham[lower+offset : upper+offset] ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)] ham[lower + offset : upper + offset]     始终在这些二元运算符的两边加上一个空格：赋值 ( =)、扩充赋值 (+=等-= )、比较 ( ==, \u0026lt;, \u0026gt;, !=, \u0026lt;\u0026gt;, \u0026lt;=, \u0026gt;=, in, , , ) 布尔值 ( , , )。\n  如果使用具有不同优先级的运算符，请考虑在具有最低优先级的运算符周围添加空格。使用您自己的判断；但是，永远不要使用多个空格，并且在二元运算符的两侧总是有相同数量的空格：\n 1 2 3 4 5 6  # 正确的: i = i + 1 submitted += 1 x = x*2 - 1 hypot2 = x*x + y*y c = (a+b) * (a-b)      函数注释应该使用冒号的常规规则，并且-\u0026gt;如果存在，箭头周围总是有空格。\n 1 2  # 正确的: def munge() -\u0026gt; PosInt: ...      =当用于指示关键字参数或用于指示未 注释函数参数的默认值时，请勿在符号周围使用空格：\n 1 2 3  # 正确的: def complex(real, imag=0.0): return magic(r=real, i=imag)      注释 与代码相矛盾的注释比没有注释更糟糕。当代码更改时，请始终优先更新评论！\n块注释通常由一个或多个由完整句子组成的段落组成，每个句子都以句点结尾。\n 块注释通常适用于跟随它们的代码，并且缩进到与该代码相同的级别。块注释的每一行都以一个#和一个空格开头。\n谨慎使用内联注释。内联注释是与语句在同一行上的注释。内联注释应与语句至少间隔两个空格。它们应该以 # 和一个空格开头。内联注释是不必要的，如果它们陈述明显的话，实际上会分散注意力。\n 命名约定  Python 库的命名约定有点混乱，所以我们永远不会完全一致——尽管如此，这里是目前推荐的命名标准。应该按照这些标准编写新的模块和包（包括第三方框架），但如果现有库具有不同的风格，则首选内部一致性。\n 至高原则  作为 API 的公共部分对用户可见的名称应遵循反映使用而非实现的约定。\n 描述性:命名样式 通常区分以下命名样式：\n  b（单个小写字母）\n  B（单个大写字母）\n  lowercase\n  lower_case_with_underscores\n  UPPERCASE\n  UPPER_CASE_WITH_UNDERSCORES\n  CapitalizedWords（或 CapWords 或 CamelCase——因其字母的凹凸外观而得名）。这有时也称为StudlyCaps。\n注意：在 CapWords 中使用首字母缩写词时，首字母缩写词的所有字母大写。因此 HTTPServerError 比 HttpServerError 好。\n  mixedCase（与 CapitalizedWords 的不同之处在于初始小写字符！）\n  Capitalized_Words_With_Underscores（丑陋的！）\n  还有一种使用简短的唯一前缀将相关名称组合在一起的风格。这在 Python 中使用不多，但为了完整性而提及。其项传统上具有st_mode、 st_size等名称st_mtime。（这样做是助于程序员熟悉它。）\n规定性:命名约定  注意避免的命名   切勿使用字符“l”（小写字母 el）、“O”（大写字母 oh）或“I”（大写字母 eye）作为单字符变量名。\n在某些字体中，这些字符与数字 1 和 0 无法区分。当想使用“l”时，请改用“L”。\n 包和模块   模块应该有简短的全小写名称。如果提高可读性，可以在模块名称中使用下划线。Python 包也应该有简短的全小写名称，尽管不鼓励使用下划线。\n当用 C 或 C++ 编写的扩展模块附带提供更高级别（例如更面向对象）接口的 Python 模块时，C/C++ 模块具有前导下划线（例如_socket）。\n 类名   类名一般首字母大写\n 函数名和变量名   函数名称应为小写，必要时用下划线分隔单词以提高可读性。\n变量名遵循与函数名相同的约定。\n 函数和方法参数   始终self用于实例方法的第一个参数。\n始终cls用于类方法的第一个参数。\n 方法名称和实例变量   使用函数命名规则：小写单词，必要时用下划线分隔以提高可读性。\n 常数   常量通常在模块级别定义，并以全大写字母书写，并用下划线分隔单词。\n 编程建议 None的建议 1 2  # 正确的: if foo is not None:   1 2  # 错误的: if not foo is None:   函数返回 在返回语句中保持一致。函数中的所有 return 语句都应该返回一个表达式，或者它们都不应该返回。如果任何 return 语句返回一个表达式，则任何没有返回值的 return 语句都应该明确地将 this 声明为，并且明确的 return 语句应该出现在函数的末尾（如果可访问）：return None\n1 2 3 4 5 6 7 8 9 10 11 12  # Correct: def foo(x): if x \u0026gt;= 0: return math.sqrt(x) else: return None def bar(x): if x \u0026lt; 0: return None return math.sqrt(x)     对象类型比较应始终使用 isinstance() 而不是直接比较类型：\n1 2  # 正确的: if isinstance(obj, int):   1 2  # 错误的: if type(obj) is type(1):     对于序列（字符串、列表、元组），使用空序列为假的事实：\n1 2 3  # 正确的: if not seq: if seq:   1 2 3  # 错误的: if len(seq): if not len(seq):     编程注释建议待续\u0026hellip;. 注意：Python 中没有属性是真正私有的。 ","permalink":"https://hanson00.github.io/posts/technology/python/python_pep8/","summary":"Python-PEP8代码风格指南摘要 PEP8代码风格指南仅仅只是提供建议，最重要是保持与项目内部保持一致性，有时在遇到与代码风格不适用的地","title":"Python_PEP8"},{"content":"Python数组篇 关于如何使用Python统计多维数组的行和列的长度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 创建多维数组的方法 # row ：行，col：列 row = 3 col = 4 list1 = [[0] * col for row in range(row)] print(list1) # 分别统计多维数组的行和列的长度 # 创建了一个4行3列的数组 alist = [[8, 8, 8], [8, 8, 8], [8, 8, 8], [8, 8, 8]] # 在打印时如果 len(数组名) 这就是打印多维数组的行的长度 # 使用 len(数组名[0]) 就是打印列的长度 for row in range(len(alist)): print(row, end= \u0026#34; \u0026#34;) #output：0 1 2 3 print(\u0026#34;\\n-------------\u0026#34;) for col in range(len(alist[0])): print(col, end= \u0026#34; \u0026#34;) #output：0 1 2   ","permalink":"https://hanson00.github.io/posts/technology/python/python%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B901/","summary":"Python数组篇 关于如何使用Python统计多维数组的行和列的长度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 创建多维数组的方法 # row ：行，c","title":"Python小知识点01"},{"content":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下\n UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence\n 尝试1：在不断尝试了很多种网页编码问题后仍然没有解决\n之后让别的小伙伴尝试了同样的代码，别人可以运行但自己无法运行，问题关键在于解决编码和解码问题。通常这种问题是由于编译器编码出现问题\n解决方法： 针对编码问题，主要从两个方面进行出发，其一是网页编码，其二是Pycharm编码，顺利解决这个小bug。\n解决步骤如下图：\nselenium浏览器打开后闪退  selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 97 Current browser version is 103.0.5060.114 with binary path C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n  selenium.common.exceptions.SessionNotCreatedException：消息：会话未创建：此版本的 ChromeDriver 仅支持 Chrome 版本 97 当前浏览器版本为 103.0.5060.114\n **解决方法：**重新安装浏览器驱动即可解决\n问题：关于Scrapy框架 问题： Signature of method \u0026lsquo;QiubaiSpider.parse()\u0026rsquo; does not match signature of base method in class \u0026lsquo;Spider\u0026rsquo;\n问题描述： 方法 \u0026lsquo;BaiduSpider.parse()\u0026rsquo; 的签名与类 \u0026lsquo;Spider\u0026rsquo; 中基方法的签名不匹配\n问题状态： 已解决\n解决方法： 将Scrapy生成的parse()方法的参数后面添加**kwargs即可 即将方法修改为\n1 2  def parse(self, response, **kwargs): pass   ","permalink":"https://hanson00.github.io/posts/thinking/%E7%88%AC%E8%99%AB%E8%A7%81%E9%97%BB%E5%BD%95/","summary":"解决爬虫报错：UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 报错如下 UnicodeEncodeError: \u0026lsquo;gbk\u0026rsquo; codec can\u0026rsquo;t encode character \u0026lsquo;\\xa0\u0026rsquo; in position 7067: illegal multibyte sequence 尝试1：在不断尝试了很多种网页","title":"爬虫见闻录"},{"content":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档\npandas官方文档\n以下代码是简单的为单元格赋值操作\n1 2 3 4 5 6 7 8 9 10 11 12 13  import win32com.client from win32com.client import Dispatch xlapp = win32com.client.Dispatch(\u0026#34;Excel.Application\u0026#34;) #创建一个Excel程序 xlapp.DisplayAlerts = False #取消弹窗 xlapp.Visible = True #显示Excel wb = xlapp.Workbooks.Add() #添加工作簿 wb.Worksheets.Item(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A1\u0026#34;).Value = 2000 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).Range(\u0026#34;A2\u0026#34;).Value = 100 #为单元格赋值 wb.Worksheets(\u0026#34;Sheet1\u0026#34;).cells(1,3).value = 200 #为单元格赋值 wb.SaveAs(r\u0026#34;C:\\Users\\Lenovo\\Desktop\\jb\\123.xlsx\u0026#34;) #保存工作簿 wb.Close() #关闭工作簿 xlapp.quit() #关闭Excel程序 print(\u0026#34;正常结束\u0026#34;)   1 2 3 4 5 6 7 8 9 10 11 12  from win32com.client import Dispatch # 创建Excel程序 xlapp = Dispatch(\u0026#34;Excel.Application\u0026#34;) # 视为可见 xlapp.Visible = True # 警告信息 xlapp.DisplayAlerts = True # 添加工作簿 wb = xlapp.Workbooks.Add() ws = wb.Sheets(\u0026#34;Sheet1\u0026#34;)\t#或者ws = wb.Sheets.Add()后面这个为新添加一个Sheet # ws2 = wb.Sheets.Add() ws.Range(\u0026#34;A1\u0026#34;).Value = 100    Python\u0026ndash;Word python-docx文档\n Python\u0026ndash;PowerPoint python-pptx文档\n Python\u0026ndash;PDF ","permalink":"https://hanson00.github.io/posts/technology/python/python-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC/","summary":"Python\u0026ndash;自动化办公 Python\u0026ndash;Excel xlwings官方文档 pandas官方文档 以下代码是简单的为单元格赋值操作 1 2 3 4 5 6 7 8 9 10 11 12 13 import","title":"Python 自动化办公"},{"content":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求]\n蛋白质 优质蛋白质\n 鱼类 蛋类 精瘦肉 漆皮肌肉  劣质蛋白质\n 牛腩 较肥的肉  淀粉（锻炼开始和结束可以和蛋白质一起吃） 优质淀粉\n 黑麦面包 红薯 全谷物面包 麸皮食品  劣质淀粉\n 土豆 高糖谷类 白米饭 白面包 干果不要吃太多  饮料 优质\n 花草茶 矿物质水 水 茶  健身周期计划 想要塑造肌肉就要摄入大量的热量\n健身-减脂周期：  先进行6个月的锻炼，同时摄入更多的热量 减脂阶段：在锻炼的途中减脂，控制热量的摄入，全程减脂 在稍微增加热量，增加肌肉 在稍微减少热量，减脂  ","permalink":"https://hanson00.github.io/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%86%E4%BA%AB/%E9%94%BB%E7%82%BC%E5%88%86%E4%BA%AB/","summary":"锻炼 怎么吃？ 体重（磅为单位）×11=基本热量需求[不吃东西不运动的热量需求] 蛋白质 优质蛋白质 鱼类 蛋类 精瘦肉 漆皮肌肉 劣质蛋白质 牛腩 较肥的肉 淀粉","title":"锻炼分享"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\ncsv模块实现了 CSV 格式表单数据的读写，提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;温铭军\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 温铭军,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;温铭军\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E6%A8%A1%E5%9D%97/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv模块"},{"content":"Pycharm 常用快捷键  连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ctrl+Alt+T\u0026ndash;添加Try/catch Alt+Shift+上下键\u0026ndash;上下移动 Ctrl+Shift+Enter\u0026ndash;补全代码 Ctrl+R\u0026ndash;替换 Ctrl+Shift+I\u0026ndash;快速查看方法的实现内容 Shift+F1\u0026ndash;查看API文档 Ctrl+Alt+H\u0026ndash;查看那里调用了方法  ","permalink":"https://hanson00.github.io/posts/technology/python/pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/","summary":"Pycharm 常用快捷键 连续按两次Shift\u0026ndash;查看资源文件 Ctrl+D\u0026ndash;复制一行 Ctrl+Alt+L\u0026ndash;格式化代码 Ct","title":"Pycharm快捷键"},{"content":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解\n Servlet快速入门  浏览器使用URL访问时所发生的事情  Servlet的方法  根据请求方式的不同进行处理 ServletRequest转换成HttpServletReque才能获取是什么球球方式\n Servlet的运行过程 Servlet程序是由WEB服务器调用，web服务器收到客户端的Servlet访问请求后： ①Web服务器首先检查是否已经装载并创建了该Servlet的实例对象。如果是，则直接执行第④步，否则，执行第②步。 ②装载并创建该Servlet的一个实例对象。 ③调用Servlet实例对象的init()方法。 ④创建一个用于封装HTTP请求消息的HttpServletRequest对象和一个代表HTTP响应消息的HttpServletResponse对象，然后调用Servlet的service()方法并将请求和响应对象作为参数传递进去。 ⑤WEB应用程序被停止或重新启动之前，Servlet引擎将卸载Servlet，并在卸载之前调用Servlet的destroy()方法。\n Request(请求)和Response(响应) request对象里有很多请求数据，可以通过该对象获取请求数据\nresponse返回设置的响应数据\n1.Tomcat需要解析请求数据，封装为request对象，并创建request对象传递到service方法中。\n SqlSessionFactory的优化 1.代码重复每次都要创建一个SqlSessionFactory\n使用静态代码块进行优化\n","permalink":"https://hanson00.github.io/posts/technology/java/04servlet/","summary":"Servlet 新版本的Servel的配置方式使用注解，以前使用XML配置方式，Servle要想被访问就要添加访问注解 Servlet快速入门 浏览器使用UR","title":"04Servlet"},{"content":"MyBatis(持久层/数据访问层)持久层的优秀框架  Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;x.x.x\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   2.编写Mybatis核心配置文件（mybatis-config.xml文件的命名）\u0026ndash;\u0026gt;==替换连接信息，解决硬编码问题==（官网）\n上面的是需要修改的 上面的mappers是指定sql的映射文件的\n\u0026lt;mapper resource=\u0026ldquo;这一段字符串是要替换的sql映射文件(sql语句的文件)的地址\u0026rdquo;\n 3.编写SQL映射文件\u0026ndash;》统一管理sql语句(官网也有)，，命名规则是要操作的表名Mapper.xml\n    编码\n 定义pojo类 加载核心配置文件，获取SQL session factory对象（官网）   获取sqlsession对象，执行sql语句（下面的Mapper代理开发可以替换）   释放资源    Mapper代理开发 1.定义与SQL映射文件同名的Mapper接口，并放在同一目录下2.设置namespace为全限名3.接口的方法名是要与id的名称相同，并且返回值要与编码相同\nMabatis的具体使用(三步走:编写接口方法，编写sql，执行方法)  编写接口方法Mapper接口 生成相应的sql映射文件  2.步骤的图    执行方法\n 数据库的列名和实体类的名称对应不上的时候使用 除了主键外的起别名用\u0026lt;==result== \u0026gt;\n\u0026lt;==result== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n\u0026lt;==id== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n 主键起别名用\u0026lt;==id== \u0026gt;\n\u0026lt;==id== column=\u0026ldquo;数据库的列名\u0026rdquo; property=\u0026ldquo;实体类对应映射的名\u0026rdquo;\u0026gt;\n 上面图片的resultMap替换了原先的resultType\n void selectById(int id);\n占位符使用 #{}\n不同的接收参数的方式\n多条件查询(使用if实现)，\n单条件查询(类似于switch语句)\n增删改操作后需要提交事务\n添加-主键返回需要使用useGeneratedKeys和keyProperty\n批量删除接受的数组是要使用注解名的，否则传递不进去 separator分隔符\n","permalink":"https://hanson00.github.io/posts/technology/java/03mybatis/","summary":"MyBatis(持久层/数据访问层)持久层的优秀框架 Mybatis2.0 1.创建模块导入坐标（官网）,除了mybatis的必要包外，还要导入其他所要的依赖包 1","title":"03MyBatis"},{"content":"maven  提供了一套标准化的项目结构\n提供了一套标准化的项目构建流程（编译，测试，打包，发布）\n提供了一套完整的依赖管理机制\n ","permalink":"https://hanson00.github.io/posts/technology/java/03maven/","summary":"maven 提供了一套标准化的项目结构 提供了一套标准化的项目构建流程（编译，测试，打包，发布） 提供了一套完整的依赖管理机制","title":"03maven"},{"content":"JDBC   注册驱动Class.forName()\n  获取连接DriverManager.getConnecti\n1 2 3 4  String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);     定义sql操作语句\n1  String sql=\u0026#34;update stu set math=10 where id=8\u0026#34;;     获取执行sql的对象Statement XXX = conn.createStatement();\n1  Statement stmt=conn.createStatement();     执行sql\n  处理结果\n  释放资源\n DriverManager  DriverManager(驱动管理类)：  注册驱动DriverManager.registerDriver 获取数据库连接    它下面的registerDriver()注册驱动 getCconnection()获取连接,语法jdbc:mysql://ip地址:端口号/数据库名称【jdbc:mysql://127.0.0.1:3306/DB1】\n  1 2 3 4 5  //获取连接 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password);    Connection   Connection(数据库连接对象):\n 获取执行sql的对象  普通执行sql对象 Statement createStatement() 预编译SQL的执行sql,防止sql注入 PreparedStatement preareStatement(sql) 执行存储过程的对象 CallbleStatement prepareCall(sql)     int executeUpdate(sql):这个在下面(即参考回滚事务代码)\nResultSet executeQuery(sql):执行DQL语句，返回值是ResultSet结果集对象\n  管理事务  开启事务：setAutoCommit(boolean autoCommit) 提交事务commit() 回滚事务rollback()    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  try { //开启事务  conn.setAutoCommit(false); int count1=stmt.executeUpdate(sql1);//受影响的行数  //处理受影响的结果  System.out.println(count1); int count2=stmt.executeUpdate(sql2);//受影响的行数  //处理受影响的结果  System.out.println(count2); //提交事务  conn.commit(); } catch (Exception throwables) { //回滚事务  conn.rollback(); throwables.printStackTrace(); }      Statement   Statement作用:\n  执行sql语句\n  int executeUpdate(sql):执行DML、DDL语句【DDL操作数据库，DML对数据进行增删改查】 返回值:1.DML语句受影响的行数2.DDL语句执行后，执行成功也可能返回0\n  ResultSet executeQuery(sql)DQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  //定义sql  String sql = \u0026#34;select * from stu\u0026#34;; //获取执行sql对象,获取Statement对象  Statement stmt = conn.createStatement(); //5.执行sql  ResultSet rs = stmt.executeQuery(sql); //6.处理结果，遍历所有数据  while (rs.next()){ //获取数据  int id = rs.getInt(1); String name = rs.getString(2); double money = rs.getDouble(3); //输出  System.out.println(id); System.out.println(name); System.out.println(money); } //7.释放资源  rs.close(); stmt.close(); conn.close(); }         PreparedStatement作用\n 预防sql注入    ","permalink":"https://hanson00.github.io/posts/technology/java/02jdbc/","summary":"JDBC 注册驱动Class.forName() 获取连接DriverManager.getConnecti 1 2 3 4 String url=\u0026#34;jdbc:mysql://127.0.0.1:3306/DB1\u0026#34;; String username=\u0026#34;root\u0026#34;; String password=\u0026#34;1234\u0026#34;; Connection conn=DriverManager.getConnection(url,username,password); 定义sql操作语句","title":"02JDBC"},{"content":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )\n show databases; 查询本地数据库\n  create database 数据库名称; 创建数据库\n  一般采用该语句用于创建数据库\ncreate database if not exists 数据库名称; \n  drop database 数据库名称; 删除数据库\n  一般采用该语句用于删除数据库\ndrop database if exists 数据库名称;\n  use 数据库名称; 使用数据库\n  select database(); 查看当前使用的数据库\n  表操作：\n show tables; 查看当前数据库的表名称\n  desc 表名称; 查询表结构\n  1 2 3 4 5 6 7 8 9 10  --创建表 create table 表名( 列名1,数据类型， 列名2,数据类型， 列名3,数据类型， 列名n,数据类型)；     drop table 表名; 删除表\n  drop table if exists 表名; 删除表并判断是否存在\n  alter table 表名 rename 新表名 修改表名\n  alter table 表名 add 新列名 数据类型; 添加一列\n  alter table 表名 modify 列名 新的数据类型; 修改数据类型\n  alter table 表名 change 列名 新的数据类型; 修改列名和数据类型\n  alter table 表名 drop 列名; 删除列\n  insert into 表名(列名1,列名2,...) values(值1,值2,值3); 给指定的列添加数据\n  insert into 表名 values(值1,值2,值3,...); 给全部列添加数据\n  insert into 表名 values(值1,值2,值3,...),(值1,值2,值3,...),(值1,值2,值3,...); 批量添加数据\n  ==查询语法DQL详细学习见后面自学==\n 条件查询 排序查询 聚合函数 分组查询 分页查询 约束实现 多表查询中的内连接，外连接 多表查询的子查询 事务   ","permalink":"https://hanson00.github.io/posts/technology/java/01mysql/","summary":"MySQL的学习 1.命令行的基本命令 数据库操作：通过命令行登录MySQL使用命令:mysql -uroot -p1234 ( -uMySQL用户名,-pMySQL密码 )","title":"01mysql"},{"content":"  安装selenium 安装浏览器驱动   1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   切换到子窗口  功能代码如下：\n1 2 3 4 5 6  browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0])    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   selenium拖动滚动条  使用JavaScript代码实现拖动具体使用window.scrollTo()方法，通过x，y坐标进行滑动。  语法scrollTo(xpos, ypos)这两个参数都是必须的。 两个坐标位置参考浏览器左上角，参数是偏移左上角的位置 纵向滚动条最后是使用设置height = 0，与document.body.scrollHeight前后位置调换来实现    1 2 3 4 5 6 7 8 9 10 11 12 13 14  from selenium import webdriver import time url = \u0026#34;https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/git%E5%AD%A6%E4%B9%A0/\u0026#34; chrome = webdriver.Chrome() chrome.get(url=url) time.sleep(1) js = \u0026#34;window.scrollTo(0, document.body.scrollHeight)\u0026#34; chrome.execute_script(js) time.sleep(2) chrome.quit()   使用ActionChains()实现滚动条的拖拽  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time from selenium.webdriver.common.action_chains import ActionChains #鼠标操作导包,事件链 #防止被认出是爬虫程序，chrome版本88以上适用 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--disable-blink-features=AutomationControlled\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://kyfw.12306.cn/otn/resources/login.html\u0026#39; browser.get(url) time.sleep(1) #输入账号密码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-userName\u0026#34;]\u0026#39;).send_keys(\u0026#39;你的账号\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-password\u0026#34;]\u0026#39;).send_keys(\u0026#39;密码\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-login\u0026#34;]\u0026#39;).click() time.sleep(2) #拖拽目标按钮 btn = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1_n1z\u0026#34;]\u0026#39;) #找到目标拖拽验证码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1__scale_text\u0026#34;]/span\u0026#39;) #拖拽移动 ActionChains(browser).drag_and_drop_by_offset(btn, 300, 0).perform() #横推拽   execute_script(“document.documentElement.scrollTop= 位置”)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  from selenium import webdriver import time driver = webdriver.Chrome() # 访问网址 driver.get(\u0026#39;https://www.baidu.com/s?ie=utf-8\u0026amp;f=8\u0026amp;rsv_bp=1\u0026amp;rsv_idx=1\u0026amp;tn=baidu\u0026amp;wd=%E5%86%AC%E5%A5%A5%E4%BC%9A\u0026amp;fenlei=256\u0026amp;rsv_pq=c61031d20000416d\u0026amp;rsv_t=34329gsBNwKJFQ%2Bmyc957YqRGPTtQjrdzsvJ%2Fpfwl4Dta6B4YxTZjnzxVE8\u0026amp;rqlang=cn\u0026amp;rsv_enter=1\u0026amp;rsv_dl=tb\u0026amp;rsv_sug3=13\u0026amp;rsv_sug1=15\u0026amp;rsv_sug7=101\u0026amp;rsv_sug2=0\u0026amp;rsv_btype=i\u0026amp;inputT=4248\u0026amp;rsv_sug4=4635\u0026#39;) # 等待3秒 time.sleep(3) # 滚动到底部 driver.execute_script(\u0026#34;document.documentElement.scrollTop=10000\u0026#34;) # 等待1秒 time.sleep(1) # 关闭所有页面 driver.quit()   定位元素实现滑动  一般步骤：\n 获取元素定位通过tagetbtn = find_element() 再滚动到定位的目标位置chrome.execute_script(“arguments[0].scrollIntoView()”, targetbtn)  打开多个窗口 使用window.open(url)\n1 2 3 4 5 6 7 8 9 10 11 12  from selenium import webdriver import time chrome = webdriver.Chrome() chrome.maximize_window() chrome.get(\u0026#34;http://www.baidu.com\u0026#34;) time.sleep(2) js1 = \u0026#39;window.open(\u0026#34;https://www.sogou.com\u0026#34;)\u0026#39; chrome.execute_script(js1) time.sleep(2) chrome.quit()   不同frame间的转换 当网页代码出现无法定位获取的元素时，并且发现iframe的代码时，要进行iframe的定位与切换\n1 2  frame_place = chrome.find_element(\u0026#34;iframe的位置\u0026#34;) chrome.switch_to_frame(frame_place)   Selenium的等待 Selenium的等待有以下三种：\n 强制等待time.sleep()等到网页加载完成再等待指定的秒数才能继续往下执行。 隐式等待，在指定等待时间到来前加载完了，直接继续往下执行，如果等待了指定的时间后还未成功打开网页就直接继续往下执行。 显示等待，最为灵活。两个人去玩，其中一人到了目的地，另一人告诉他不来了，他就一个人进去。   隐式等待判断网页是否加载完，但可能网页上某个JavaScript脚本加载速度特别慢，而JavaScript脚本只是个限制脚本，不影响网页的正常浏览，但仍然需要等待全部页面加载完成。\n显示等待只需要判断某个特定元素是否加载出来就可以了。\n 隐式等待如下：隐式等待设置一次就会在整个程序中都起作用\n1 2 3 4 5 6 7 8 9 10 11 12  from selenium import webdriver import time from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys chrome = webdriver.Chrome() url = \u0026#34;https://www.python.org/downloads/\u0026#34; chrome.implicitly_wait(10) chrome.get(url=url) print(chrome.find_element(By.LINK_TEXT,\u0026#34;About\u0026#34;).get_attribute()) chrome.quit()   显示等待：下面列出显示等待所需的库\n from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from selenium import webdriver from selenium.webdriver.support import expected_conditions from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.common.by import By url = \u0026#39;https://www.csdn.net/\u0026#39; chrome = webdriver.Chrome() # 设置隐式等待，等待时间为隐式等待和显示等待时间长的那个 chrome.implicitly_wait(10) chrome.get(url=url) try: WebDriverWait(chrome, 20, 0.5).until(expected_conditions.presence_of_element_located((By.LINK_TEXT, u\u0026#39;博客\u0026#39;))) finally: print(chrome.find_element(By.LINK_TEXT, \u0026#34;博客\u0026#34;).get_attribute(\u0026#39;href\u0026#39;)) chrome.quit()   WebDriverWait()可选择的参数有四个，一般情况下只用前三个，第一个参数是设置要打开的浏览器，第二个参数是设置超时时间，第三个是设置检查频率。\n后面一部分的until(method,message=\u0026quot;\u0026quot;)，message返回的结果是0，即失败后要返回的消息。\n向expected_conditions.presence_of_element_located((元组))\n上面的代码可以同等替换为：\n1 2 3 4 5 6 7  # 定位条件 place = (By.LINK_TEXT, u\u0026#39;博客\u0026#39;) try: WebDriverWait(chrome, 20, 0.5).until(expected_conditions.presence_of_element_located(place)) finally: print(chrome.find_element(By.LINK_TEXT, \u0026#34;博客\u0026#34;).get_attribute(\u0026#39;href\u0026#39;)) chrome.quit()   项目实战 拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   暂时补充： selenium键盘操作和鼠标操作\n","permalink":"https://hanson00.github.io/posts/technology/python/python-selenium/","summary":"安装selenium 安装浏览器驱动 1 2 3 4 5 6 7 8 from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrom","title":"Python Selenium"},{"content":"Python并发与爬虫 进程与线程 每个主进程中至少都会有一个线程，当你创建一个新的线程时，就相当于有两个员工(线程)在工作具体的工作顺序由CPU决定，主线程会继续往下面执行，子线程也会自己执行自己的任务\n在Python中一般不创建多进程，因为进程消耗的内存资源较大\n线程执行案例 t.start()，当前线程准备就绪，等待CPU调度 t.join()等待当前任务执行完成，当前任务执行完成之后才会往后面执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  # import threading # # # 线程如果想传参一定要通过arg=(元组的形式进行传递) # def func(name): # for i in range(1000): # print(name, i) # # if __name__ == \u0026#34;__main__\u0026#34;: # t1 = threading.Thread(target=func, args=(\u0026#34;Rose\u0026#34;,) ) # t1.start() # t2 = threading.Thread(target=func, args=(\u0026#34;你太牛\u0026#34;,) ) # t2.start() # for i in range(1000): # print(\u0026#34;main\u0026#34;, i) from threading import Thread class Mythread(Thread): def run(self): #固定写法 for i in range(1000): print(self.name, i) if __name__ == \u0026#34;__main__\u0026#34;: t1 = Mythread() t1.start() t2 = Mythread() t2.start() for i in range(1000): print(\u0026#34;main\u0026#34;, i) #Thread-2 main984  #Thread-2867  #985main  #Thread-2 868 #main986 #Thread-2 869 #main 870 #main987 #Thread-2 988 871 #Thread-2  #main989  #872 #Thread-2 main 873990 #Thread-2 #main 991 #Thread-2 992 #874Thread-2 # main 875993 #Thread-2 main994 #Thread-2 995 876   进程执行案例 1 2 3 4 5 6 7 8 9 10 11  from multiprocessing import Process def func(): for i in range(100000): print(\u0026#34;函数里\u0026#34;, i) if __name__ == \u0026#39;__main__\u0026#39;: t = Process(target=func) t.start() for i in range(100000): print(\u0026#34;main\u0026#34;, i)    线程池：一次性创建一批线程，然后我们用户把任务分配给线程池，然后线程池分配任务给线程池里面的线程\n1 2 3 4 5 6 7 8 9 10 11  from concurrent.futures import ThreadPoolExecutor def func(name): for i in range(100): print(name, i) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(50) as t: for i in range(10): t.submit(func, name=f\u0026#34;线程{i}\u0026#34;) print(\u0026#34;OVER\u0026#34;)    北京新发地爬虫与多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  # coding:utf-8 # @Time : 2022/5/17 14:41 # @Author : 软件1194温铭军 # @file : 北京新发地多线程.py # $software : PyCharm import requests from concurrent.futures import ThreadPoolExecutor import csv url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def getdata(data): url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; response = requests.post(url, headers=header, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;, \u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) if __name__ == \u0026#39;__main__\u0026#39;: with ThreadPoolExecutor(20) as t: for i in range(1, 60): data = { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: f\u0026#39;{i}\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } t.submit(getdata, data) print(\u0026#34;OVER\u0026#34;)   协程(多任务异步协程) 协程是在单线程的基础上操作（多线程协作可以充分地利用CPU）\n 切换条件：当程序遇到IO操作时可以选择切换到其他任务(计算类型的任务)上\n在微观上一个任务一个任务进行切换，切换条件是遇到IO操作(单线程的多个任务来回切换运行)\n在宏观上就是多个任务共同运行\n 会使程序陷入阻塞状态的有：IO操作和requests.get()在网络请求返回前都会处于阻塞状态\n 协程分配的任务是由人来调度，线程池是由系统来调度的\n 异步任务和同步任务看是否需要等待某一操作\n在编写协程代码时要注意：await(挂起)要写到async的函数里面，放在协程对象里面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #协程的编写 import time import asyncio async def func1(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱我们\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作,time.sleep()是串形同步操作，使得异步中断了 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱你\u0026#34;) async def func2(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱22\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(3) print(\u0026#34;这是第二次我爱22\u0026#34;) async def func3(): #async:将该函数定义成异步协程函数 print(\u0026#34;我爱33\u0026#34;) # time.sleep(3) #当前线程处于阻塞状态，CPU不为我工作 await asyncio.sleep(4) #改成这个异步操作代码就可以实现协程的异步处理 print(\u0026#34;这是第二次我33\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: #input()也会让CPU处于阻塞状态 #requests.get()也会使程序处于阻塞状态 #当程序处于IO操作时程序会陷入阻塞状态 #协程：当程序遇到IO操作时，可以选择性的切换到其他任务 #如果执行函数就会得到一个协程对象 #协程函数的运行需要asyncio模块的支持 t1 = time.time() f1 = func1() f2 = func2() f3 = func3() task = [f1, f2, f3] #一次性启动多个任务（协程） #多任务同时启动就需要asyncio.wait,把多个任务交给asyncio.wait(),而启动就需要asyncio.run() asyncio.run(asyncio.wait(task)) t2 = time.time() print(t2-t1)   上面代码是同步操作的协程\n下面代码是加入异步的协程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import asyncio import time async def func1(): print(\u0026#34;我是第一\u0026#34;) await asyncio.sleep(3) print(\u0026#34;我是第一\u0026#34;) async def func2(): print(\u0026#34;我是第2\u0026#34;) await asyncio.sleep(2) print(\u0026#34;我是第2\u0026#34;) async def func3(): print(\u0026#34;我是第3\u0026#34;) await asyncio.sleep(4) print(\u0026#34;我是第3\u0026#34;) async def main(): task = [] task = [asyncio.create_task(func1()), asyncio.create_task(func2()), asyncio.create_task(func3())] await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1) #output:我是第一 # 我是第2 # 我是第3 # 我是第2 # 我是第一 # 我是第3 # 4.009669780731201   使用异步协程爬取图片\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # coding:utf-8 # @Time : 2022/5/18 15:41 # @Author : 软件1194温铭军 # @file : aiohttp_umei.py # $software : PyCharm #aiohttp异步爬取优美图库 # requests.get()该操作是同步操作，如果想要 # 使用异步操作的话，就得使用aiohttp import asyncio import aiohttp import time urls = [ \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/zkc0inje5x0.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/ufmo0xczdbg.jpg\u0026#34;, \u0026#34;http://kr.shanghai-jiuxin.com/file/mm/20211130/15dujfems5v.jpg\u0026#34; ] async def downsave(url): name = url.split(\u0026#34;/\u0026#34;)[-1] async with aiohttp.ClientSession() as sessions: # 这部的操作就相当于 \u0026lt;==\u0026gt; requests async with sessions.get(url) as s: with open(f\u0026#34;{name}.jpg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(await s.content.read()) print(\u0026#34;OVER\u0026#34;) async def main(): task = [] for url in urls: task.append(downsave(url)) await asyncio.wait(task) if __name__ == \u0026#39;__main__\u0026#39;: t1 = time.time() asyncio.run(main()) t2 = time.time() print(t2-t1)    个人对于协程的理解 asyncio.run()是用来运行最高层级的入口点\n","permalink":"https://hanson00.github.io/posts/technology/python/python%E5%B9%B6%E5%8F%91%E4%B8%8E%E7%88%AC%E8%99%AB/","summary":"Python并发与爬虫 进程与线程 每个主进程中至少都会有一个线程，当你创建一个新的线程时，就相当于有两个员工(线程)在工作具体的工作顺序由CP","title":"Python并发与爬虫"},{"content":"Python-csv模块的学习  引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。\n 为什么会出现：由于当时不同程序读写数据时会出现细微的差别，这些多个来源的数据文件的格式差别让程序难以高效地处理数据\n==csv模块实现了 CSV 格式表单数据的读写，==提供了“以兼容 Excel 的方式输出数据文件”或“读取 Excel 输出的数据文件”的功能。\ncsv模块的常见用法 csv.reader()与csv.writer() csv.reader(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n 返回一个reader对象，该对象会逐行遍历csvfile。 csvfile可以是任何支持iterator协议的对象，并且每次调用的时候都会调用__next__方法返回字符串，文件对象和列表对象都适用。 如果csvfile是文件对象，则打开它时应该使用newline=\u0026quot;\u0026quot;，对于后面两个参数可以见Python标准库有详细介绍\n  第二个参数dialect，其实代指的是dialect这一大类，\n有如下可选参数： delimiter(中译:界定符)：用于分隔字段的单字符，默认为',' doublequote：当该属性为True时，双写引号字符。如果该属性为 Flase，则在 引号字符 的前面放置 转义符。默认值为 True quotechar：用于包住有特殊字符的字段，特殊字段有定界符,引号字符,换行符,默认为'\u0026quot;' ，如用引号分隔的字符串名\u0026quot;李 寻欢\u0026quot;，则在输出的时候用选中的符号代替引号 skipinitialspace：如果为True，则在输入错误的csv时抛出Error异常，默认为False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as csvfile: read_obj = csv.reader(csvfile, ) #返回一个read对象 for i in read_obj: print(i) # output: # [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] # [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] # [\u0026#39;二奶\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;物理\u0026#39;] # [\u0026#39;三奶\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;1198\u0026#39;]    csv.writer(csvfile, dialect=\u0026lsquo;excel\u0026rsquo;, **fmtparams)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # coding:utf-8 # @Time : 2022/5/14 11:50 # @file : csv_reader.py # $software : PyCharm \u0026#34;\u0026#34;\u0026#34; 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; import csv with open(\u0026#34;../2.CSV\u0026#34;, \u0026#34;w+\u0026#34;) as csvfile: writer_obj = csv.writer(csvfile, ) #返回一个read对象 writer_obj.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) print(\u0026#34;OVER\u0026#34;) # output: # Spam,Lovely Spam,Wonderful Spam   1 2 3 4 5 6 7 8 9 10 11  import csv with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) spamwriter.writerow([\u0026#39;李铭记\u0026#39;, \u0026#39;大修个\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;,\u0026#39;司法部 是你\u0026#39;]) # output: # Spam Spam Spam Spam Spam |Baked Beans| # Spam |Lovely Spam| |Wonderful Spam| # 李铭记 大修个 |Wonderful Spam| |司法部 是你|   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  #csv.reader()方法和csv.writer()方法 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;r\u0026#34;) as cs: result = csv.reader(cs) for i in result: print(i) \u0026#34;\u0026#34;\u0026#34; output: [\u0026#39;姓名\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;班级\u0026#39;] [\u0026#39;君不愁\u0026#39;, \u0026#39;22\u0026#39;, \u0026#39;软件1194\u0026#39;] 等等将csv文件按这种格式输出完 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;eggs.csv\u0026#39;, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;) as csvfile: # 参数delimiter是为每个读入数据之间添加值， # 参数quotechar(数据分隔符)是在每个插入的数据中间添加 # 参数具体详情请参照csv模块中的变种与格式参数 spamwriter = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;, quotechar=\u0026#39;|\u0026#39;, quoting=csv.QUOTE_MINIMAL) spamwriter.writerow([\u0026#39;Spam\u0026#39;] * 5 + [\u0026#39;Baked Beans\u0026#39;]) spamwriter.writerow([\u0026#39;Spam\u0026#39;, \u0026#39;Lovely Spam\u0026#39;, \u0026#39;Wonderful Spam\u0026#39;]) \u0026#34;\u0026#34;\u0026#34; outputeggs.csv文件: Spam Spam Spam Spam Spam |Baked Beans| Spam |Lovely Spam| |Wonderful Spam| \u0026#34;\u0026#34;\u0026#34;   csv.DictReader() csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)以字典的输出方式来读取文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  #csv.DictReader方法测试 import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # csv.DictReader(f, fieldnames=None, restkey=None, restval=None, dialect=\u0026#39;excel\u0026#39;, *args, **kwds)) # f 指要操作的文件|| fieldnames 是所要指定的字典的键名，如果不给出则默认使用csv文件的第一行来作为键名\\ # 当存储的数据没有列名的时候使用fieldnames对列名进行赋值 # 当存储的数据有列名的时候可以不使用fieldname \u0026#34;\u0026#34;\u0026#34; 使用fieldnames后输出为： {\u0026#39;name\u0026#39;: \u0026#39;姓名\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;年龄\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;班级\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;../1.CSV\u0026#34;, ) as cs: result = csv.DictReader(cs, fieldnames=[\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;class\u0026#34;]) for row in result: print(row) \u0026#34;\u0026#34;\u0026#34; 不使用fieldname输出为： {\u0026#39;姓名\u0026#39;: \u0026#39;雷军\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;软件1194\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷二奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;18\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;物理\u0026#39;} {\u0026#39;姓名\u0026#39;: \u0026#39;雷三奶\u0026#39;, \u0026#39;年龄\u0026#39;: \u0026#39;33\u0026#39;, \u0026#39;班级\u0026#39;: \u0026#39;1198\u0026#39;} \u0026#34;\u0026#34;\u0026#34;   csv.DictWriter() csv.DictWriter(f, fieldnames, restval='', extrasaction='raise', *dialect='excel', **args, ***kwds)\nf为文件名\nfieldnames是由键的组成的值作为序列传参\nextrasaction 用于指定要执行的操作。 如果将其设置为默认值 \u0026lsquo;raise\u0026rsquo;，则会引发 ValueError 如果将其设置为 \u0026lsquo;ignore\u0026rsquo;，则字典中的其他键值将被忽略\n（传入的字典的键不够时）如果传递给writerow()方法的键缺少fieldnames中的键时，默认会使用可选参数extrasaction写入，默认值 'raise'，则会引发 ValueError。 如果将其设置为 'ignore'，则字典中的其他键值将被忽略\n（（没有指定fieldnames时）在缺少fieldnames参数时，默认会用restval用于指定值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import csv \u0026#34;\u0026#34;\u0026#34;该csv文件内容如下 姓名,年龄,班级 君不愁,22,软件1194 二奶,18,物理 三奶,33,1198 \u0026#34;\u0026#34;\u0026#34; # newline是每次写入完后，再次迭代写入时所要插入的值,默认是换行 with open(\u0026#34;../1.CSV\u0026#34;, \u0026#34;a\u0026#34;, newline=\u0026#39;\u0026#39;) as cs: loadwriter = csv.DictWriter(cs,extrasaction=\u0026#34;ignore\u0026#34;, fieldnames=[\u0026#34;姓名\u0026#34;, \u0026#34;年龄\u0026#34;, \u0026#34;班级\u0026#34;]) loadwriter.writeheader() #将参数fieldnames写入表头 loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;类四奶\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;16\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) loadwriter.writerow({\u0026#34;姓名\u0026#34;:\u0026#34;君不愁\u0026#34;, \u0026#34;年龄\u0026#34;:\u0026#34;22\u0026#34;, \u0026#34;班级\u0026#34;:\u0026#34;软件1194\u0026#34;}) print(\u0026#34;csv.DictWriter写入完毕\u0026#34;)   ","permalink":"https://hanson00.github.io/posts/technology/python/python-csv%E5%AD%A6%E4%B9%A0/","summary":"Python-csv模块的学习 引言：CSV (Comma Separated Values)逗号分隔值格式是电子表格和数据库的常见输入输出文件格式。 为什么会出现：由于当时不","title":"Python Csv学习"},{"content":"爬虫路飞进阶 视频网站工作原理   用户上传视频到视频网站\n  视频网站对视频进行转码(4K,2K,1080P,标清等)\n  对视频进行切片处理（个人理解不知道对错，分布式存储视频片段） ==================被切分的视频片段，当用户想拖到后面的片段播放时就直接播放后面被切分的一小段片段，节省流量资源\n  切分后的视频的排序问题，需要一个文件1.记录视频播放顺序2.视频的存放路径基本上会直接存放在M3U文件(固定格式的文本)如json，txt，M3U文件通过utf-8编码后的名字是M3U8\n 怎么爬取？\n 找到m3u8文件（通过各种手段） 通过m3u8下载视频文件 将下载到的视频文件合并成一个视频文件     selenium的视频学习 tips：\n 所打开的浏览器的左上角的浏览器受到控制提示可能后面会遇到反爬 如果遇到ajax的页面(局部刷新)所要操作时需要等待否则大概率报错 当代码只爬取到了部分数据就报错了一般是爬取太快所导致 有些视频网站会有iframe，必须先要拿到iframe(iframe = find_element()找到iframe)然后把视角切换到iframe(web.switch_to.window(iframe窗口))才能拿到数据，切换回原页面使用web.switch_to.default_content()  先粗细定位整体在精细定位局部\n如果你的程序chrome被检测到了是爬虫程序，一般分为两种情况解决chrome版本号小于88和大于88（浏览器控制台输入window.navigator.webdriver查看）  关于selenium的一些常见用法 最好能记住下面的案例所要导入的模块，多看\n1 2 3 4 5 6 7 8 9 10 11 12  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.page_source browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER)   selenium的微信读书小案例 1 2 3 4 5 6 7 8  from selenium import webdriver #使用selenium获取网页的基本操作 url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; browser = webdriver.Chrome() #声明模拟的浏览器是Chrome browser.maximize_window() #最大化浏览器 browser.get(url) #模拟浏览器发送请求 browser.quit() #关闭模拟浏览器   1 2 3 4 5 6 7 8 9 10 11 12 13 14  #如果希望访问网页时不弹出浏览器窗口 #使用无界面浏览器是为了在某些爬取工作时不弹出浏览器窗口来影响自己的操作 #比如在获取网页源代码后仍然需要后续操作时 from selenium import webdriver url = \u0026#34;http://finance.sina.com.cn/realstock/company/sh000001/nc.shtml\u0026#34; chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) #声明模拟的浏览器是Chrome browser.get(url) #控制浏览器发起请求，访问网址 # data = browser.page_source #获取网页真正的源代码(经过渲染之后的源代码) # print(data) # print(browser.title) #获取浏览页面的标题   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from selenium import webdriver import re #使用无界模式访问,并获取网页源代码 sin = input(\u0026#34;请输入你要查找的新闻:\u0026#34;) url = f\u0026#34;https://search.sina.com.cn/news?q={sin}\u0026amp;c=news\u0026amp;from=index\u0026#34; # #无界模式 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#34;--headless\u0026#34;) browser = webdriver.Chrome(options=chrome_opt) browser.get(url) # data = browser.page_source browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[1]\u0026#39;).send_keys(\u0026#34;阿里巴巴\u0026#34;) browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;tabc02\u0026#34;]/form/div/input[4]\u0026#39;).click() retitle = re.compile(r\u0026#39;\u0026lt;div class=\u0026#34;box-result clearfix\u0026#34; data-sudaclick=\u0026#34;.*?\u0026#34;\u0026gt;.*?\u0026lt;a href=\u0026#34;(?P\u0026lt;url\u0026gt;.*?)\u0026#34;.*?\u0026gt;(?P\u0026lt;title\u0026gt;.*?)\u0026lt;/a\u0026gt;\u0026#39;, re.S) result = retitle.finditer(browser.page_source) for i in result: title = i.group(\u0026#34;title\u0026#34;).replace(\u0026#39;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026#39;,\u0026#34;\u0026#34;).replace(\u0026#39;\u0026lt;/font\u0026gt;\u0026#39;, \u0026#34;\u0026#34;) url = i.group(\u0026#34;url\u0026#34;)   拉钩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) #上面//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1] for li in all_list: # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a jobname = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).text salary = li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[2]/span\u0026#39;).text print(jobname, salary)   验证码  自己写图像识别 选择互联网上成熟的验证码破解工具  下拉框案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # coding:utf-8 # @Time : 2022/5/21 10:46 # @Author : 软件1194温铭军 # @file : 无头浏览器.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.select import Select #无头浏览器代码 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--headless\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://www.endata.com.cn/BoxOffice/BO/Year/index.html\u0026#39; browser.get(url) sel_el = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;OptionDate\u0026#34;]\u0026#39;) #选择下拉框的提取 sel = Select(sel_el) #将下拉框包装成select对象 #让浏览器调整选项 for i in range(len(sel.options)): sel.select_by_index(i) #拿到每一个下拉框的选项 time.sleep(1) table = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;TableList\u0026#34;]\u0026#39;) print(table.text) time.sleep(1)   窗口切换 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # coding:utf-8 # @Time : 2022/5/20 9:32 # @Author : 软件1194温铭军 # @file : lagou.py # $software : PyCharm import time from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys url = \u0026#34;https://www.lagou.com/\u0026#34; browser = webdriver.Chrome() browser.get(url) time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;changeCityBox\u0026#34;]/p[1]/a\u0026#39;).click() time.sleep(1) browser.find_element(By.XPATH,\u0026#39;//*[@id=\u0026#34;search_input\u0026#34;]\u0026#39;).send_keys(\u0026#34;爬虫\u0026#34;, Keys.ENTER) time.sleep(1) #接下来提取公司名称，岗位名称，工作地点 # 查找存放数据的位置，进行数据提取 #进行列表全部数据的粗提取 all_list = browser.find_elements(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div\u0026#39;) for li in all_list: li.find_element(By.XPATH, value=\u0026#39;./div[1]/div[1]/div[1]/a\u0026#39;).click() # //*[@id=\u0026#34;jobList\u0026#34;]/div[1]/div[1]/div[1]/div[1]/div[1]/a time.sleep(1) browser.switch_to.window(browser.window_handles[-1]) #切换窗口，将它置为最后一个窗口 time.sleep(1) datajob = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;job_detail\u0026#34;]/dd[2]\u0026#39;).text print(datajob) browser.close() browser.switch_to.window(browser.window_handles[0]) time.sleep(1)   12306 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/21 11:41 # @Author : 软件1194温铭军 # @file : 12306.py # $software : PyCharm #12306账号：a954952920 #密码：hansonwmj2000 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time from selenium.webdriver.common.action_chains import ActionChains #鼠标操作导包,事件链 #防止被认出是爬虫程序，chrome版本88以上适用 chrome_opt = webdriver.ChromeOptions() chrome_opt.add_argument(\u0026#39;--disable-blink-features=AutomationControlled\u0026#39;) browser = webdriver.Chrome(options=chrome_opt) url = \u0026#39;https://kyfw.12306.cn/otn/resources/login.html\u0026#39; browser.get(url) time.sleep(1) #输入账号密码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-userName\u0026#34;]\u0026#39;).send_keys(\u0026#39;你的账号\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-password\u0026#34;]\u0026#39;).send_keys(\u0026#39;密码\u0026#39;) browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;J-login\u0026#34;]\u0026#39;).click() time.sleep(2) #拖拽目标按钮 btn = browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1_n1z\u0026#34;]\u0026#39;) #找到目标拖拽验证码 browser.find_element(By.XPATH, value=\u0026#39;//*[@id=\u0026#34;nc_1__scale_text\u0026#34;]/span\u0026#39;) #拖拽移动 ActionChains(browser).drag_and_drop_by_offset(btn, 300, 0).perform() #横推拽   ","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%E7%88%AC%E8%99%AB/","summary":"爬虫路飞进阶 视频网站工作原理 用户上传视频到视频网站 视频网站对视频进行转码(4K,2K,1080P,标清等) 对视频进行切片处理（个人理解不知道","title":"路飞爬虫进阶爬虫"},{"content":"爬虫基础 爬虫基本流程\n1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到本地，爬取完的URL放到已爬取队列 3.分析网页内容，找出网页里面关心的URL链接和内容，继续执行第二步\n 获取网页 提取信息 保存数据 自动化程序  爬虫如果需要模拟则把该网站下面的所有请求信息封装(例如UA)然后发送\nHTTP请求方法 请求，是由客户端向服务器发出一般分为4部分内容：请求方法（request method）、请求的网址（request url）、请求头（request Headers）、请求体（request Body）\n   1 GET 请求指定的页面信息，并返回实体主体。     2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头   3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。   4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。   5 DELETE 请求服务器删除指定的页面。   6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。   7 OPTIONS 允许客户端查看服务器的性能。   8 TRACE 回显服务器收到的请求，主要用于测试或诊断。   9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。    请求头 请求头，用来说明服务器要使用的附加信息，比较重要的信息有，cookie、Refer、User-Agent\n响应 响应，由服务器端返回给客户端，分为响应状态码（Response Status Code）、响应头（Response headers）、响应体（Response Body）\n响应头 包含了服务器对请求的应答信息\n 小知识  Host:域名。表示请求的服务器网址   爬虫学习 urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)打开一个网页并把获取该URL的网页对象 data 必须是一个对象，用于给出要发送到服务器的附加数据，若不需要发送数据则为 None，data支持的对象类型包括字节串、类文件对象和可遍历的类字节串对象\nurllib.request.urlopen()  一个简单的get请求爬取\n 1 2 3 4 5 6 7 8 9 10 11  import urllib.request import urllib.parse #解析器 import urllib.error #Get请求 response = urllib.request.urlopen(\u0026#34;http://www.baidu.com\u0026#34;) print(response.read().decode(\u0026#34;UTF-8\u0026#34;))\t# response是\u0026lt;http.client.HTTPResponse object at 0x000002DC28A404C0\u0026gt; url = \u0026#34;http://httpbin.org/get\u0026#34; response2 = urllib.request.urlopen(url=url) print(response2.read().decode(\u0026#34;UTF-8\u0026#34;))    一个简单的post请求(post请求需要提交个表单信息)，需要提供一个封装的数据\n 1 2 3 4 5  #post请求 需要封装数据 http://httpbin.org/post url = \u0026#34;http://httpbin.org/post\u0026#34; data = bytes(urllib.parse.urlencode({\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;}), encoding = \u0026#34;utf-8\u0026#34;)\t#把数据变成二进制格式 response = urllib.request.urlopen(url=url, data=data) print(response.read().decode(\u0026#34;utf-8\u0026#34;))   urllib.request.Request() urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) 上面的urlopen过于简单直接把赋值的url直接打开，包含不了太多伪装信息，所以使用urllib.request.Request()\nheaders：告诉要访问的服务器，我们是什么类型的机器（浏览器），本质上是告诉浏览器我们可以接受什么水平的文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  import urllib.request import urllib.parse #解析器 import urllib.error # http://httpbin.org/post https://www.douban.com url = \u0026#34;http://httpbin.org/post\u0026#34; headers = {\u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} data = bytes(urllib.parse.urlencode({\u0026#34;name\u0026#34;:\u0026#34;hanson\u0026#34;}),encoding = \u0026#34;UTF-8\u0026#34;) #封装了一个请求对象 req = urllib.request.Request(url=url, data=data, headers=headers) response = urllib.request.urlopen(req)\t#响应对象 print(response.read().decode(\u0026#34;UTF-8\u0026#34;))   简单的爬取网页 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  import urllib.request import urllib.error def main(): #基础的URL # 1.爬取网页 baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xlsx\u0026#34; # datalist = getData(baseurl) getData(baseurl) #2.解析数据 #3.保存数据 # saveData(savepath) def getData(baseurl): datalist = [] for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) return datalist # 3.保存数据,在本列子中暂时不用 def saveData(savepath): pass # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main()   BeautifulSoup库的基本使用(爬虫的解析) BeautifulSoup4将HTML文档转换成树形结构，每个节点都是Python对象，归类成四种：\n Tag\t可以获取标签及其标签内容 NavigableString BeautifulSoup comment  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) # Tag 获取标签及内容 print(bs.title) #output: \u0026lt;title\u0026gt;百度一下，你就知道 \u0026lt;/title\u0026gt; print(bs.a) #output: \u0026lt;a class=\u0026#34;mnav\u0026#34; href=\u0026#34;http://news.baidu.com\u0026#34; name=\u0026#34;tj_trnews\u0026#34;\u0026gt;\u0026lt;!--新闻--\u0026gt;\u0026lt;/a\u0026gt; # 获取标签里面的内容 print(bs.title.string) #output: 百度一下，你就知道 print(bs.a.string) #output: 新闻 # NavigableString 获取标签里的所有属性，并返回一个字典 print(bs.a.attrs) #attrs是attribute属性的缩写 # print(bs) 打印整个html文件 # 文件的遍历 contents print(\u0026#34;-----文档的遍历-----\u0026#34;) print(bs.head.contents) #返回一个列表   BeautifulSoup2.0文档搜索 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  import re import bs4 with open(\u0026#34;../baidu.html\u0026#34;, \u0026#34;rb\u0026#34;) as file: htmlread = file.read() bs = bs4.BeautifulSoup(htmlread, \u0026#34;html.parser\u0026#34;) #文档搜索 # 1. find_all()方法:字符串过滤会查找与字符串完全匹配的内容 list = bs.find_all(\u0026#34;a\u0026#34;) # list = bs.find_all(\u0026#34;a\u0026#34;, limit = 3) limit限制多少 print(list) #正则表达式搜索 print(\u0026#34;---正则表达式搜索---\u0026#34;) re_list = bs.find_all(re.compile(\u0026#34;a\u0026#34;)) print(re_list) print(len(re_list)) #传入一个函数作为参数进行搜索,按照函数进行搜索 print(\u0026#34;---函数搜索---\u0026#34;) def name_is_exists(tag): return tag.has_attr(\u0026#34;name\u0026#34;) funcres = bs.find_all(name_is_exists) print(funcres) #kwargs关键字搜索 print(\u0026#34;----kwargs关键字搜索----\u0026#34;) kwlist = bs.find_all(id = \u0026#34;head\u0026#34;) kwlist2 = bs.find_all(href = \u0026#34;https://www.hao123.com\u0026#34;) kwlist3 = bs.find_all(class_ = True) #这里class加下划线是因为class是Python的关键字 print(kwlist3) #text文本参数,查找标签里的字符串 print(\u0026#34;----text文本参数-----\u0026#34;) ts_list = bs.find_all(text=\u0026#34;hao123\u0026#34;) ts_list2 = bs.find_all(text=[\u0026#34;hao123\u0026#34;, \u0026#34;地图\u0026#34;]) print(ts_list2) #css选择器,和css选择器的语法一样 print(\u0026#34;------css选择器-------\u0026#34;) css_list = bs.select(\u0026#34;title\u0026#34;) css_list2 = bs.select(\u0026#34;.mnav\u0026#34;) css_list3 = bs.select(\u0026#34;#u1\u0026#34;) css_list4 = bs.select(\u0026#34;a[class = \u0026#39;bri\u0026#39;]\u0026#34;) print(css_list3)   liwei爬虫学习的最终 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  # coding:utf-8 # @Time : 2022/5/7 9:57 # @Author : 软件1194温铭军 # @file : ts.py # $software : PyCharm import bs4 import re import xlwt import urllib.request import urllib.error def main(): baseurl = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; #待爬取的URL savepath = r\u0026#34;D:\\project\\python pycharm\\datasave\\douban_data.xls\u0026#34; #保存位置 datalist = getData(baseurl) #爬取到的数据列表 # getData(baseurl) # print(datalist) #3.保存数据 saveData(datalist,savepath) # 提取影片的提取规则 recom_link = re.compile(r\u0026#39;\u0026lt;a href=\u0026#34;(.*?)\u0026#34;\u0026gt;\u0026#39;) recom_img = re.compile(r\u0026#39;\u0026lt;img alt=\u0026#34;.*? src=\u0026#34;(.*?)\u0026#34;.*?/\u0026gt;\u0026#39;, re.S) #re.S让换行符也包含在内 recom_title = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_rating = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_judge = re.compile(r\u0026#39;\u0026lt;span\u0026gt;(\\d*)人评价\u0026lt;/span\u0026gt;\u0026#39;) recom_inq = re.compile(r\u0026#39;\u0026lt;span class=\u0026#34;inq\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026#39;) recom_db = re.compile(r\u0026#39;\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;(.*?)\u0026lt;/p\u0026gt;\u0026#39;,re.S) def getData(baseurl): datalist = [] #生成要爬取的网址 for i in range(0, 10): url = baseurl + str(i*25) html = askurl(url) #进行数据解析 soup = bs4.BeautifulSoup(html, \u0026#34;html.parser\u0026#34;) for item in soup.find_all(\u0026#34;div\u0026#34;, class_ = \u0026#34;item\u0026#34;):\t#在\u0026lt;class \u0026#39;bs4.BeautifulSoup\u0026#39;\u0026gt;中查找是div并且class=\u0026#34;item\u0026#34;的内容 # print(item) data = [] #保存每一部电影的信息 item = str(item) link = re.findall(recom_link, item)[0] data.append(link) #添加链接 img = re.findall(recom_img, item)[0] data.append(img) title = re.findall(recom_title, item) if (len(title) ==2): ctitle = title[0] data.append(ctitle) otitle = title[1].replace(\u0026#34;/\u0026#34;,\u0026#34;\u0026#34;) #去掉无关符号 data.append(otitle) else: data.append(title[0]) data.append(\u0026#34; \u0026#34;) #留空 rating = re.findall(recom_rating, item)[0] data.append(rating) judge = re.findall(recom_judge, item)[0] data.append(judge) inq = re.findall(recom_inq, item) if len(inq) !=0: inq = inq[0].replace(\u0026#34;。\u0026#34;, \u0026#34; \u0026#34;) data.append(inq) else: data.append(\u0026#34; \u0026#34;) db = re.findall(recom_db, item)[0] db = re.sub(\u0026#39;\u0026lt;br(\\s+)?/\u0026gt;(\\s+)?\u0026#39;, \u0026#34;\u0026#34;, db) db = re.sub(\u0026#39;/\u0026#39;, \u0026#34; \u0026#34;, db) data.append(db.strip()) datalist.append(data) # print(datalist) return datalist # 3.保存数据 def saveData(datalist,savepath): wb = xlwt.Workbook(encoding=\u0026#34;UTF-8\u0026#34;) ws = wb.add_sheet(\u0026#34;douban_dataTOP250\u0026#34;) col = (\u0026#34;电影详情链接\u0026#34;, \u0026#34;图片链接\u0026#34;, \u0026#34;影片中文名\u0026#34;, \u0026#34;影片外国名\u0026#34;, \u0026#34;评分\u0026#34;, \u0026#34;评价数\u0026#34;, \u0026#34;概况\u0026#34;, \u0026#34;相关信息\u0026#34;) for i in range(0,8): ws.write(0,i,col[i]) #列名 for i in range(0,250): print(\u0026#34;第%d条\u0026#34; %(i+1)) data = datalist[i] for j in range(0,8): ws.write(i+1,j,data[j]) #写入数据 wb.save(savepath) #保存 # 得到一个指定的URL的网页内容 def askurl(url): head = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } html = \u0026#34;\u0026#34; try: req = urllib.request.Request(url=url, headers=head) response = urllib.request.urlopen(req) html= response.read().decode(\u0026#34;UTF-8\u0026#34;) # print(html) except Exception as e: print(e) return html if __name__ == \u0026#34;__main__\u0026#34;: main() print(\u0026#34;爬取完毕\u0026#34;)   爬虫所用到的库 数据解析：\n bs4(beautifulsoup)解析【较慢】 re解析 xpath解析  re正则表达式  re.L 表示特殊字符集 \\w, \\W, \\b, \\B, \\s, \\S 依赖于当前环境 re.M 多行模式 re.S 即为' . \u0026lsquo;并且包括换行符在内的任意字符（\u0026rsquo; . \u0026lsquo;不包括换行符） re.U 表示特殊字符集 \\w, \\W, \\b, \\B, \\d, \\D, \\s, \\S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和\u0026rsquo; # \u0026lsquo;后面的注释   注意:\n 除了txt纯文本文件使用r来读取，其他类型的文件都是二进制文件用rb读取 乱码问题：Python内存层面使用的是Unicode编码，而Unicode不能存储和传输的，必须把Unicode编码进行转码(写文件时要编码，读文件时要解码)，转换成UTF-8或者GBK 得到的b\u0026rsquo;字符串\u0026rsquo;是字节，需要转码 在已经爬取得到网页后进行数据解析时，如果测试正则表达式没有问题但仍无数据时，可以加上re.S之类的re.compile(pattern[, flags])的flag参数 当使用BeautifulSoup的find(参数)参数中出现Python关键字时可以在关键字后面加__(PS: class_ = \u0026quot;值\u0026quot;)  1 2 3  #如下面两种写法一样 find(classs_=\u0026#34;table\u0026#34;) find(attrs={\u0026#34;class\u0026#34;:\u0026#34;table\u0026#34;})   BeautifulSoup中如果想拿到某个属性的值，可以使用get(\u0026ldquo;要获取的属性名\u0026rdquo;)方法  ","permalink":"https://hanson00.github.io/posts/technology/python/%E6%9D%8E%E5%B7%8D%E7%88%AC%E8%99%AB/","summary":"爬虫基础 爬虫基本流程 1.初始化URL，并将URL放入待爬取的队列 2.将URl通过DNS解析IP，将对应IP的站点下载到HTML页面，并保存到","title":"李巍爬虫"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Person struct { age int name string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板文件 \troute.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) //配置路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到首页\u0026#34;) }) route.POST(\u0026#34;/post\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;欢迎来到post页面\u0026#34;) }) route.GET(\u0026#34;json1\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, map[string]interface{}{ \u0026#34;json1\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第一个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json2\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;json2\u0026#34;: \u0026#34;666\u0026#34;, \u0026#34;第2个json页面\u0026#34;: \u0026#34;666\u0026#34;, }) }) route.GET(\u0026#34;json3\u0026#34;, func(c *gin.Context) { tom := \u0026amp;Person{ 18, \u0026#34;tom\u0026#34;, } c.JSON(http.StatusOK, tom) }) route.GET(\u0026#34;xml\u0026#34;, func(c *gin.Context) { c.XML(http.StatusOK, map[string]interface{}{ \u0026#34;xml\u0026#34;: \u0026#34;6666\u0026#34;, }) }) route.GET(\u0026#34;index\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;index.html\u0026#34;, map[string]interface{}{ \u0026#34;title\u0026#34;: \u0026#34;后台的标题\u0026#34;, }) }) route.Run(\u0026#34;:8821\u0026#34;) }   模板渲染 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) type Aticle struct { Title string Content string } func main() { //创建一个默认路由 \troute := gin.Default() //配置模板路径 \t//route.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) \t//如果有多层模板目录/**代表一层目录 \troute.LoadHTMLGlob(\u0026#34;templates/**/*\u0026#34;) //前台路由 \troute.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { //这里的html名称要和渲染的htmldefine后面的名称一致 \tc.HTML(http.StatusOK, \u0026#34;templates/default/beindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/new\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/default/benew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) //后台路由 \troute.GET(\u0026#34;/admin\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;templates/admin/adindex.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, }) }) route.GET(\u0026#34;/adminnew\u0026#34;, func(c *gin.Context) { news := \u0026amp;Aticle{ \u0026#34;新闻标题\u0026#34;, \u0026#34;内容在这里\u0026#34;, } c.HTML(http.StatusOK, \u0026#34;templates/admin/adnew.html\u0026#34;, gin.H{ \u0026#34;title\u0026#34;: \u0026#34;首页\u0026#34;, \u0026#34;new\u0026#34;: news, }) }) route.Run(\u0026#34;:8848\u0026#34;) }   ","permalink":"https://hanson00.github.io/posts/technology/golang/01gin/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34;","title":"01gin"},{"content":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看请求头参数如：UA(User-Agent)\nPOST请求注意发送数据，如果是POST请求则要在请求中加入From Data\n数据解析：尽量使用re解析和xpath解析数据\n网页的渲染方式： 使用浏览器开发者工具和查看网页源代码可以看出为什么渲染方式\n 服务器渲染：在服务器那边就直接把数据和html结合在一起，统一返回给客户端(浏览器) 客户端渲染：客户端浏览器第一次请求时只要一个html骨架，待浏览器检查html时再第二次向服务器请求数据，并进行渲染【在网页源代码中看不到数据】|如果遇到客户端渲染的网页则要在抓包工具中(浏览器开发者工具)捕获需要的数据源在对该数据源的URL进行请求抓取  关于网页编码与解码的一些总结   在一般网页有给出自己给出编码字符集时可以观察编码后自行赋值或者自己编写正则表达式自己提取\n电影天堂项目体现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests #1.从电影天堂的首页爬取相关首页的内容 # 2.从相关内容中提取相关下载信息 url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } # response = requests.get(url, headers=header, verify=False) response = requests.get(url, headers=header) #本次实验不需verify也可以获得信息 # 获取网页的编码方式,自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) # print(code) #或者在爬取乱码网页后进行观察后自行赋值（直接自己赋值） response.encoding = code html = response.text #下面两行代码也可解决编码问题 # code = response.encoding # print(response.text.encode(code).decode(\u0026#34;gb2312\u0026#34;)) # print(html) #提取首页中所需内容 redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(html) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) #在提取的首页内容中进一步提取子页所需内容 child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_response = requests.get(child_url, headers=header) child_response.encoding = code # print(child_response.text) child_html = child_response.text child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_html) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) response.close() print(\u0026#34;爬取结束\u0026#34;)     案例2\n1 2 3 4 5 6 7 8 9  import requests heads = {\u0026#34;User_Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34;} url1 = \u0026#34;https://www.baidu.com\u0026#34; request_data = requests.get(url=url1, headers=heads) code = request_data.encoding print(code) response = request_data.text.encode(code).decode(\u0026#34;utf-8\u0026#34;) print(response) #response = request_data.text 有时可以直接无需转码或者有时候可以使用decode(\u0026#34;gbk\u0026#34;)   requests get方式请求：\n1 2 3 4 5 6 7 8 9  import requests url = \u0026#34;https://www.baidu.com/s?ie=UTF-8\u0026amp;wd=%E5%91%A8%E6%9D%B0%E4%BC%A6\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(url, headers=headers) print(response.text)   post方式请求： post请求时需要查看网页的From data所需要的参数然后在填充后传入\n1 2 3 4 5 6 7 8 9 10 11  # post请求：百度翻译 import requests url = \u0026#34;https://fanyi.baidu.com/sug\u0026#34; ins = input(\u0026#34;请输入你要翻译的词\u0026#34;) data = { \u0026#34;kw\u0026#34;: ins } response = requests.post(url, data=data) print(response.json())   数据解析 三种解析方式：\n re解析(最快) bs4解析(便捷)在李巍爬虫学习中 xpath解析  re正则表达式 1 2 3 4 5 6 7 8 9 10 11 12  import re str = \u0026#39;\u0026#39;\u0026#39; \u0026lt;div class=\u0026#34;jay\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;1\u0026#34;\u0026gt;周杰伦\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay2\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;2\u0026#34;\u0026gt;周杰伦2\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;jay3\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;3\u0026#34;\u0026gt;周杰伦3\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026#39;\u0026#39;\u0026#39; # 使用 (?P\u0026lt;分组名\u0026gt;正则) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;(?P\u0026lt;id\u0026gt;.*?)\u0026#34;\u0026gt;\u0026lt;span id = \u0026#34;.*\u0026#34;\u0026gt;(.*?)\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;) result = res.finditer(str) for i in result: print(i.group(\u0026#39;id\u0026#39;))   xPath解析   首先导入from lxml import etree etree才包括了xPath解析的功能\nxPath解析XML\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  from lxml import etree xml = \u0026#34;\u0026#34;\u0026#34; \u0026lt;book\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;野花遍地香\u0026lt;/name\u0026gt; \u0026lt;price\u0026gt;1.23\u0026lt;/price\u0026gt; \u0026lt;nick\u0026gt;臭豆腐\u0026lt;/nick\u0026gt; \u0026lt;author\u0026gt; \u0026lt;nick id=\u0026#34;10086\u0026#34;\u0026gt;周大枪\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;10010\u0026#34;\u0026gt;周芷若\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;joy\u0026#34;\u0026gt;周杰伦\u0026lt;/nick\u0026gt; \u0026lt;nick class=\u0026#34;jolin\u0026#34;\u0026gt;蔡依林\u0026lt;/nick\u0026gt; \u0026lt;div\u0026gt; \u0026lt;nick\u0026gt;nick惹了\u0026lt;/nick\u0026gt; \u0026lt;nic\u0026gt;nic惹了\u0026lt;/nic\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span\u0026gt; \u0026lt;nick\u0026gt;22惹了\u0026lt;/nick\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;partner\u0026gt; \u0026lt;nick id=\u0026#34;ppc\u0026#34;\u0026gt;胖胖陈\u0026lt;/nick\u0026gt; \u0026lt;nick id=\u0026#34;ppbc\u0026#34;\u0026gt;胖胖不陈\u0026lt;/nick\u0026gt; \u0026lt;/partner\u0026gt; \u0026lt;/book\u0026gt; \u0026#34;\u0026#34;\u0026#34; tree = etree.XML(xml) #生成一个xPath解析的XML对象 # result = tree.xpath(\u0026#34;/book\u0026#34;) #output:[\u0026lt;Element book at 0x183918b2f00\u0026gt;] result = tree.xpath(\u0026#34;/book/name\u0026#34;) #output:[\u0026lt;Element name at 0x21065d142c0\u0026gt;] result = tree.xpath(\u0026#34;/book/name/text()\u0026#34;) #output:[\u0026#39;野花遍地香\u0026#39;] # 把后代(子孙节点的内容拿出来) result = tree.xpath(\u0026#34;/book/author//nick/text()\u0026#34;) #output:[\u0026#39;周大枪\u0026#39;, \u0026#39;周芷若\u0026#39;, \u0026#39;周杰伦\u0026#39;, \u0026#39;蔡依林\u0026#39;, \u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] #*通配符 result = tree.xpath(\u0026#34;/book/author/*/nick/text()\u0026#34;) #output:[\u0026#39;nick惹了\u0026#39;, \u0026#39;22惹了\u0026#39;] print(result)     xPath解析html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  from lxml import etree tree = etree.parse(\u0026#34;ts.html\u0026#34;)\t# result = tree.xpath(\u0026#34;/html\u0026#34;) #output:[\u0026lt;Element html at 0x1e6e3568880\u0026gt;] result = tree.xpath(\u0026#34;/html/body/ul/li/a\u0026#34;) #output:[\u0026lt;Element a at 0x1f354c57300\u0026gt;, \u0026lt;Element a at 0x1f354c57400\u0026gt;, \u0026lt;Element a at 0x1f354c573c0\u0026gt;] #xPath的索引是从第一个开始 result = tree.xpath(\u0026#34;/html/body/ul/li[1]/a/text()\u0026#34;) #output:[\u0026#39;百度\u0026#39;] #两种提取大炮的方法 result = tree.xpath(\u0026#34;/html/body/ol/li[2]/a/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] #@表示a里面的属性href是dapao的 #[@XXX=XXX] : 属性的筛选 result = tree.xpath(\u0026#34;/html/body/ol/li/a[@href=\u0026#39;dapao\u0026#39;]/text()\u0026#34;) #output:[\u0026#39;大炮\u0026#39;] result = tree.xpath(\u0026#34;/html/body/ol/li\u0026#34;) #output: [\u0026#39;飞机\u0026#39;] #[\u0026#39;大炮\u0026#39;] #[\u0026#39;火车\u0026#39;] for i in result: a = i.xpath(\u0026#34;./a/text()\u0026#34;) print(a) #拿到a里面的属性的值 result = tree.xpath(\u0026#34;/html/body/ol/li/a/@href\u0026#34;) print(result) \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34;\u0026gt;百度\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.google.com\u0026#34;\u0026gt;谷歌\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;http://www.sogou.com\u0026#34;\u0026gt;搜狗\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;feiji\u0026#34;\u0026gt;飞机\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;dapao\u0026#34;\u0026gt;大炮\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;huoche\u0026#34;\u0026gt;火车\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;div class=\u0026#34;job\u0026#34;\u0026gt;李嘉诚\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;common\u0026#34;\u0026gt;胡辣汤\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34;   xPath解析猪八戒网 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests from lxml import etree import csv baseurl = \u0026#34;https://zhanjiang.zbj.com/search/f/?kw=saas\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } response = requests.get(baseurl, headers=headers) # print(response.text) html = etree.HTML(response.text) #相对解析 #拿到整个大的div的列表 divs = html.xpath(\u0026#34;/html/body/div[6]/div/div/div[2]/div[5]/div[1]/div\u0026#34;) namelist = [] datalist = [] #在整个大的div里面选取要提取的数据 for i in divs: price = i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[1]/span[1]/text()\u0026#34;)[0].strip(\u0026#34;¥\u0026#34;) title = \u0026#34;saas\u0026#34;.join(i.xpath(\u0026#34;./div/div/a[2]/div[2]/div[2]/p/text()\u0026#34;)) com_name = i.xpath(\u0026#34;./div/div/a[1]/div[1]/p/text()\u0026#34;)[1].strip() place = i.xpath(\u0026#34;./div/div/a[1]/div[1]/div/span/text()\u0026#34;)[0] datalist = [title, price, com_name, place] # print(com_name) with open(\u0026#34;猪八戒.csv\u0026#34;, \u0026#34;a+\u0026#34;) as csvfile: zwriter = csv.writer(csvfile, ) zwriter.writerow(datalist) response.close() print(\u0026#34;爬取结束\u0026#34;)   防盗链-梨视频 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  import requests #原始页面的链接，有需要替换的信息 url = \u0026#34;https://www.pearvideo.com/video_1761797\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34;, #防盗链：溯源1-》2-》3一样 \u0026#39;Referer\u0026#39;: \u0026#39;https://www.pearvideo.com/video_1761797\u0026#39; } #获取后面要用于替换的信息 conid = url.split(\u0026#34;_\u0026#34;)[1] #从抓包工具中获取的url里面获取json，目的是要获得假的URL visit_url = f\u0026#34;https://www.pearvideo.com/videoStatus.jsp?contId={conid}\u0026amp;mrd=0.561999819571843\u0026#34; response = requests.get(url=visit_url, headers=header) # print(response.json()) #在抓包工具中的链接里面获得假视频URL和假视频URL的信息用于替换 #\u0026#39;https://video.pearvideo.com/mp4/adshort/20220511/1652607901500-15877599_adpkg-ad_hd.mp4\u0026#39; flase_url = response.json()[\u0026#34;videoInfo\u0026#34;][\u0026#34;videos\u0026#34;][\u0026#34;srcUrl\u0026#34;] systemtime = response.json()[\u0026#39;systemTime\u0026#39;] # print(systemtime) # print(flase_url) realurl = flase_url.replace(systemtime, f\u0026#34;cont-{conid}\u0026#34;) print(realurl) #下载视频 with open(\u0026#34;梨视频.mp4\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(realurl).content) response.close() print(\u0026#34;爬取结束\u0026#34;)   豆瓣小抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import requests import re import csv url = \u0026#34;https://movie.douban.com/top250?start=\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } req = requests.get(url, headers=header) response = req.text # print(response) res = re.compile(R\u0026#39;\u0026lt;div class=\u0026#34;hd\u0026#34;\u0026gt;.*?\u0026lt;span class=\u0026#34;title\u0026#34;\u0026gt;(?P\u0026lt;name\u0026gt;.*?)\u0026#39; R\u0026#39;\u0026lt;/span\u0026gt;.*?\u0026lt;p class=\u0026#34;\u0026#34;\u0026gt;.*?\u0026lt;br\u0026gt;(?P\u0026lt;year\u0026gt;.*?)\u0026amp;nbsp.*?\u0026#39; R\u0026#39;\u0026lt;span class=\u0026#34;rating_num\u0026#34; property=\u0026#34;v:average\u0026#34;\u0026gt;(?P\u0026lt;scroe\u0026gt;.*?)\u0026lt;/span\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;span\u0026gt;(?P\u0026lt;people\u0026gt;\\d+)人评价\u0026lt;/span\u0026gt;\u0026#39;, re.S) result = res.finditer(response) with open(\u0026#34;lufeidouban.csv\u0026#34;, \u0026#34;a\u0026#34;) as f: file_writer = csv.writer(f) for i in result: # print(i.group(\u0026#34;name\u0026#34;)) # print(i.group(\u0026#34;year\u0026#34;).strip()) # print(i.group(\u0026#34;scroe\u0026#34;)) # print(i.group(\u0026#34;people\u0026#34;)) #将数据保存成csv文件 dic = i.groupdict() dic[\u0026#34;year\u0026#34;] = dic[\u0026#34;year\u0026#34;].strip() file_writer.writerow(dic.values()) req.close() print(\u0026#34;爬取结束\u0026#34;)    北京新发地抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # coding:utf-8 # @Time : 2022/5/13 15:42 # @Author : 软件1194温铭军 # @file : beijing_xinfadi.py # $software : PyCharm import requests import csv #使用post请求时所要添加data请求参数,可以在浏览器开发者工具里面的From Data里面看到 url = \u0026#34;http://www.xinfadi.com.cn/getPriceData.html\u0026#34; headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } data= { \u0026#39;limit\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;current\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateStartTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;pubDateEndTime\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodPcatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodCatid\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;prodName\u0026#39;: \u0026#39;\u0026#39; } response = requests.post(url, headers=headers, data=data) result = response.json() ts = result[\u0026#34;list\u0026#34;] with open(\u0026#34;北京新发地农产品数据.csv\u0026#34;, \u0026#34;w\u0026#34;, newline=\u0026#39;\u0026#39;) as f: csvwriter = csv.DictWriter(f, fieldnames=[\u0026#39;prodName\u0026#39;, \u0026#39;lowPrice\u0026#39;, \u0026#39;highPrice\u0026#39;, \u0026#39;avgPrice\u0026#39;, \u0026#39;place\u0026#39;, \u0026#39;unitInfo\u0026#39;,\u0026#39;pubDate\u0026#39;], extrasaction=\u0026#39;ignore\u0026#39;) csvwriter.writeheader() for i in range(len(ts)): csvwriter.writerow(ts[i]) response.close() print(\u0026#34;爬取结束\u0026#34;)   电影天堂重复代码处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # coding:utf-8 # @Time : 2022/5/13 9:09 # @Author : 软件1194温铭军 # @file : dytt.py # $software : PyCharm # verify = false告诉网站不做校验忽略ssl，verify中文核实，校验 # request在发送请求时也会发送一个校验，发送一个证书 import re import requests url = \u0026#34;https://dytt89.com/\u0026#34; header = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 94.0.4606.71Safari / 537.36\u0026#34; } def htmldata(url): \u0026#34;\u0026#34;\u0026#34; 代替提取网页信息 :param url: :return: \u0026#34;\u0026#34;\u0026#34; response = requests.get(url, headers=header) code1 = charsetre.search(response.text) code = code1.group(\u0026#34;charset\u0026#34;) response.encoding = code html = response.text response.close() return html # 或者自己通过正则提取 charsetre = re.compile(\u0026#39;charset=(?P\u0026lt;charset\u0026gt;.*?)\u0026#34;\u0026gt;\u0026#39;, re.S) redata = re.compile(R\u0026#39;2022必看热片.*?\u0026lt;ul\u0026gt;(?P\u0026lt;data1\u0026gt;.*?)\u0026lt;/ul\u0026gt;\u0026#39;, re.S) child_re = re.compile(R\u0026#34;\u0026lt;a href=\u0026#39;(?P\u0026lt;href\u0026gt;.*?)\u0026#39;\u0026#34;, re.S) data = redata.finditer(htmldata(url)) for i in data: link = i.group(\u0026#34;data1\u0026#34;) # print(link) child_href = child_re.finditer(link) child_url = \u0026#39;\u0026#39; for i in child_href: # print(i.group(\u0026#34;href\u0026#34;)) child_url = url + i.group(\u0026#34;href\u0026#34;).strip(\u0026#34;/\u0026#34;) # print(child_url) child_data = htmldata(child_url) child_rename = re.compile(R\u0026#39;◎译　名　(?P\u0026lt;mname\u0026gt;.*?)\u0026lt;br /\u0026gt;.*?\u0026#39; R\u0026#39;\u0026lt;td style=\u0026#34;WORD-WRAP: break-word\u0026#34; bgcolor=\u0026#34;#fdfddf\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;(?P\u0026lt;load\u0026gt;.*?)\u0026#34;\u0026#39;, re.S) a =child_rename.finditer(child_data) for i in a: print(i.group(\u0026#34;mname\u0026#34;)) print(i.group(\u0026#34;load\u0026#34;)) print(\u0026#34;爬取结束\u0026#34;)   代理 代理：通过第三方的一个机器去发送请求（去网上找免费代理，或者找）\n综合训练-网易云音乐评论抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132  # coding:utf-8 # @Time : 2022/5/16 10:07 # @file : 网易云音乐.py # $software : PyCharm #爬取网易云音乐评论 # 步骤： # 1.找到未加密的数据 # 2.想办法把参数按照网易云音乐本身的逻辑加密 # 3.请求到网易，拿到评论信息 # 加密参数 # params:就是encText # encSecKey:就是encSecKey from Crypto.Cipher import AES import requests import re import json from base64 import b64encode url = \u0026#34;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=\u0026#34; #POST请求 headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#34; \u0026#34; (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\u0026#34; } #找到了真正的参数，接下来还需要解密过程 data = { \u0026#39;csrf_token\u0026#39;: \u0026#34;\u0026#34;, \u0026#39;cursor\u0026#39;: \u0026#34;-1\u0026#34;, \u0026#39;offset\u0026#39;: \u0026#34;0\u0026#34;, \u0026#39;orderType\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageNo\u0026#39;: \u0026#34;1\u0026#34;, \u0026#39;pageSize\u0026#39;: \u0026#34;20\u0026#34;, \u0026#39;rid\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34;, \u0026#39;threadId\u0026#39;: \u0026#34;R_SO_4_441491828\u0026#34; } #处理加密过程 #服务于windows.arXXX的加密过程 e = \u0026#39;010001\u0026#39; f = \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; g = \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; i = \u0026#39;LvjKmGgecBJv6kkF\u0026#39; #手动固定 #如果把i固定那么下面这个函数一定是固定的，由下面的c函数解出 def get_encSecKey(): return \u0026#34;7c435414bc686e49dc26a57ffedbb80b320fd755755c5da8e72f416a61370039aa2d3fa333e54a6e7c9abe7f26faffa69d1721db76cb2e6f17d0393d4cfbc6176590909d027022c4e458aee2123c329b60e1e422beef0f8e39efad5014cbeab022199bc7e6c47a5d0bca5528f6c7946305ae019674309d562c69b39dde9ea429\u0026#34; #字典不能加密，所以下面这个函数默认收到的是字符串 def get_params(data): \u0026#34;\u0026#34;\u0026#34; 就是去还原下面的b的两次加密 h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i) :param data: :return: \u0026#34;\u0026#34;\u0026#34; first = enc_params(data, g) second = enc_params(first, i) return second #返回的就是params #转换成16的长度为加密算法服务 def to_16(data): pad = 16 - len(data) % 16 data += chr(pad) *pad return data #第二个参数的加密算法 def enc_params(data, key): iv = \u0026#39;0102030405060708\u0026#39; data = to_16(data) aes = AES.new(key=key.encode(\u0026#34;utf-8\u0026#34;), IV=iv, mode=AES.MODE_CBC) #创建了一个加密工具 bs = aes.encrypt(data.encode(\u0026#34;utf-8\u0026#34;)) #如果想要返回字符串还需要base64转码 return str(b64encode(bs), \u0026#34;utf-8\u0026#34;) pass \u0026#34;\u0026#34;\u0026#34; function a(a) { var d, e, b = \u0026#34;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\u0026#34;, c = \u0026#34;\u0026#34;; for (d = 0; a \u0026gt; d; d += 1) #循环16次 e = Math.random() * b.length, #产生随机数 e = Math.floor(e), #取整 c += b.charAt(e); return c #返回一个16个字母 } function b(a, b) { a是要加密的内容 var c = CryptoJS.enc.Utf8.parse(b) c和b是一回事 , d = CryptoJS.enc.Utf8.parse(\u0026#34;0102030405060708\u0026#34;) 偏移量 , e = CryptoJS.enc.Utf8.parse(a) e和a相同都是数据 , f = CryptoJS.AES.encrypt(e, c, { e是数据，c是 AES是加密算法自己查的话可以知道下面的东西 iv: d, 偏移量 mode: CryptoJS.mode.CBC 加密模式CBC 看完之后发现少了个密钥所以c(b)是密钥 }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,\u0026#34;\u0026#34;,c), #RSA加密 e = encryptedString(d, a) } function d(d, e, f, g) { d：就是真实数据data,除了数据d是变的之外，其他都是不变的 var h = {}产生一个空对象 e：固定值 \u0026#39;010001\u0026#39; , i = a(16); i是一个16位数的随机字符串 f:超长定值 \u0026#39;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\u0026#39; h.encText = b(d, g), d是数据 g是密钥 g:固定值 \u0026#39;0CoJUm6Qyw8W8jud\u0026#39; h.encText = b(h.encText, i), i是密钥 #返回的就是加密数据param #如果i固定那么encSeckey就固定 h.encSecKey = c(i, e, f), i是一个16位数的随机字符串，ef是固定的值 #返回的就是加密参数encSecKey return h }windows.a = d加密参数的入口是d encText是通过两次加密得出第二个加密参数 \u0026#34;\u0026#34;\u0026#34; response = requests.post(url, headers=headers, data={ \u0026#34;params\u0026#34;:get_params(json.dumps(data)), \u0026#34;encSecKey\u0026#34;:get_encSecKey() }) print(response.text) # list = response.json()[\u0026#39;data\u0026#39;][\u0026#39;hotComments\u0026#39;] # res = re.compile(r\u0026#34;\u0026#39;content\u0026#39;: \u0026#39;(.*?)\u0026#39;,\u0026#34;) # # print(list[0]) # print(res.findall(str(list))) response.close() print(\u0026#34;OVER!\u0026#34;)   [后续练习] 熟练使用浏览器开发者工具\n [附录] 使用.*?代替变化的内容，使用(.*?)代表需要提取的内容\n记得关闭请求\n","permalink":"https://hanson00.github.io/posts/technology/python/%E8%B7%AF%E9%A3%9E%E7%88%AC%E8%99%AB/","summary":"爬虫—路飞视频学习 爬虫如果出问题第一件事情查看请求头参数如：UA(User-Agent) POST请求注意发送数据，如果是POST请求则要在请","title":"路飞爬虫"},{"content":"Git Learning HEAD是指当前所在分支\ngit branch 显示所在分支\ngit check cat 切换到cat分支\n  git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表示文件已经被安置到暂存区，稍后会与其他文件一起放到存储库) git add \u0026ndash;all全部文件添加到暂存区 git commit -m**\u0026ldquo;init commit\u0026rdquo;** commit只会把暂存区里面的文件提交到存储库，加粗字体段为提交文件的说明描述一定要添加否则会打开vim编辑器进行再次补充 额外点：二段式：git commit -a -m \u0026ldquo;描述补充信息\u0026rdquo;（一次直接add 和commit） git log 查看日志信息   使用git查询历史记录时的常见问题  git log git log welcome.html 访问特定文件的日志信息 git log \u0026ndash;oneline 显示一行日志 git reflog 可以移动指针次数有所记录方便回调版本 想要查找某个人或者某些人的commit \u0026ndash;git log \u0026ndash;oneline \u0026ndash;author=\u0026ldquo;作者名称\u0026rdquo; 使用\u0026quot;\\|\u0026ldquo;表示或者 git log \u0026ndash;oneline \u0026ndash;grep=\u0026ldquo;关键字信息\u0026rdquo; 在关键字的内容中进行查询 git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; 找到早上九点到十二点的所有commit git log \u0026ndash;oneline \u0026ndash;since=\u0026ldquo;9am\u0026rdquo; \u0026ndash;untill=\u0026ldquo;12am\u0026rdquo; \u0026ndash;after=\u0026ldquo;2022-01\u0026rdquo; 从2022-01之后的每天的早上九点到十二点的所有commit   在git中删除或者修改文件  直接删除，使用系统直接删除或者使用rm  删除后还要使用git add \u0026ldquo;要删除的文件名称\u0026rdquo; git add 1.txt 移入暂存区 add后还要进行commit操作 git commit -m \u0026ldquo;描述信息\u0026rdquo; 提交到存储库   使用git进行删除 git rm 2.txt 直接完成add操作后面只需使用commit操作即可完成   如果不想git在对文件进行控制可以使用git rm \u0026ldquo;2.txt文件名\u0026rdquo; \u0026ndash;cached 变更文件名  使用系统直接改变名称后 使用git add \u0026ndash;all git commit -m \u0026ldquo;描述信息\u0026rdquo;   或者直接使用git进行改名 git mv filename1 filename2 修改commit描述记录 git commit \u0026ndash;amend -m \u0026ldquo;描述信息\u0026rdquo; 使用amend只能修改最后一条记录 git blame 找出哪段代码谁是修改的 git checkout welcome.html 挽救误删的文件   版本前进后退  git reflog git reset \u0026ndash;hard hash值 前进后退到某一个文档  git diff 文件名 将工作区文件和暂存区文件作比较\ngit diff HEAD 文件名 与本地库做对比\ngit diff 全部文件做比较\n 分支  git branch 获取当前分支 git branch cat 创建cat分支 git branch -d cat 删除cat分支 -D强制删除 git checkout cat 切换分支 git branch -v 查看有什么分支  分支合并 1.先切换到要合并的分支 git merge     git与GitHub   如果是全新的项目创建的话git init\n  git add 1.txt\n  git commit -m \u0026ldquo;git one file\u0026rdquo;\n  接下来要把内容推送到远端的git服务器上\n git remote add origin GitHub服务器地址 origin只是默认的代名词，代指后面的那个服务器地址，可以改名 （起别名，以后可以用这个别名代替远程库地址） git push -u origin master push指令： 例如 远端节点为wmj，选择cat分支推送上去 git push wmj cat  把master分支的内容推送到origin位置 如果远端服务器不存在master分支，会自动创建 如果服务器上存在master分支，则会把master分支的位置指向最新的状态 -u其实就是upstream（上游），其实就是另一个分支的名称而已，在git里，每个分支都可以设置一个上游，它会自动跟踪某个分支 git push -u origin master 会把origin/master设置为本地master的分支的upstream，当下次执行git push时不添加任何指令，git会猜测推送给远端节点origin，并把master分支推上去 git push origin master=git push origin master:master 如果想把分支推上去后进行改名则git push origin master:cat      把推送的内容拉回来更新  git fetch git merge origin/master    推送命令 git pull =git fetch+git merge\n  git pull -rebase 可以使得产生的额外commit取消\n  有时候push推送失败是因为在线版本的内容比本地内容新此时需要 先拉再推 git pull \u0026ndash;rebase\n  看到有趣的项目时单击clone or download 或者使用 git clone git@github.com:\u0026hellip;. 复制下来地址后面还可以加目录名\n与其他开发者互动pull request（待续） ","permalink":"https://hanson00.github.io/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/git%E5%AD%A6%E4%B9%A0/","summary":"Git Learning HEAD是指当前所在分支 git branch 显示所在分支 git check cat 切换到cat分支 git init 初始化库，并让git对其控制 git status 检查目录状态\u0026ndash;(new file 表","title":"Git学习"},{"content":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪 正在学习:  爬虫  🧠 计划学习:\n 数据分析 机器学习 Python量化   ","permalink":"https://hanson00.github.io/about/","summary":"👋HI 大家好，我是一个热爱计算机的同学，励志成为一名资深程序员! 喜欢学习各种有趣好玩的知识 虽然开始的太晚，但我相信只要努力就一定会有收获，希望和大家一起努力 希望可以慢下心来好好学，慢慢来\n💪 正在学习:  爬虫  🧠 计划学习:\n 数据分析 机器学习 Python量化   ","title":""}]